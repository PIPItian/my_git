{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.1235955056179776,
  "eval_steps": 300,
  "global_step": 900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0,
      "grad_norm": 0.013946587219834328,
      "learning_rate": 0.01999583853516438,
      "loss": 1.1357,
      "step": 1
    },
    {
      "epoch": 0.0,
      "grad_norm": 0.03152783587574959,
      "learning_rate": 0.019991677070328756,
      "loss": 1.3057,
      "step": 2
    },
    {
      "epoch": 0.0,
      "grad_norm": 0.02430560812354088,
      "learning_rate": 0.019987515605493136,
      "loss": 1.5762,
      "step": 3
    },
    {
      "epoch": 0.0,
      "grad_norm": 0.03687096759676933,
      "learning_rate": 0.01998335414065751,
      "loss": 0.9321,
      "step": 4
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.026205476373434067,
      "learning_rate": 0.019979192675821888,
      "loss": 0.9751,
      "step": 5
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.04537992179393768,
      "learning_rate": 0.019975031210986267,
      "loss": 0.7012,
      "step": 6
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.02319790981709957,
      "learning_rate": 0.019970869746150647,
      "loss": 1.1514,
      "step": 7
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.027198510244488716,
      "learning_rate": 0.019966708281315023,
      "loss": 1.1768,
      "step": 8
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.026688028126955032,
      "learning_rate": 0.019962546816479403,
      "loss": 1.0107,
      "step": 9
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.027742978185415268,
      "learning_rate": 0.01995838535164378,
      "loss": 0.52,
      "step": 10
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.017290690913796425,
      "learning_rate": 0.019954223886808155,
      "loss": 0.5259,
      "step": 11
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.024143831804394722,
      "learning_rate": 0.019950062421972534,
      "loss": 0.895,
      "step": 12
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.03602828457951546,
      "learning_rate": 0.019945900957136914,
      "loss": 1.042,
      "step": 13
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.03868493810296059,
      "learning_rate": 0.01994173949230129,
      "loss": 0.9253,
      "step": 14
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.019538432359695435,
      "learning_rate": 0.01993757802746567,
      "loss": 0.8286,
      "step": 15
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.028750460594892502,
      "learning_rate": 0.019933416562630046,
      "loss": 0.5039,
      "step": 16
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.022473318502306938,
      "learning_rate": 0.019929255097794422,
      "loss": 0.8252,
      "step": 17
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.01876644417643547,
      "learning_rate": 0.0199250936329588,
      "loss": 0.5903,
      "step": 18
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.029065033420920372,
      "learning_rate": 0.01992093216812318,
      "loss": 0.6187,
      "step": 19
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.014426208101212978,
      "learning_rate": 0.019916770703287557,
      "loss": 0.2732,
      "step": 20
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.01835629716515541,
      "learning_rate": 0.019912609238451937,
      "loss": 0.5063,
      "step": 21
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.021680880337953568,
      "learning_rate": 0.019908447773616313,
      "loss": 0.5586,
      "step": 22
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.018381770700216293,
      "learning_rate": 0.01990428630878069,
      "loss": 0.4036,
      "step": 23
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.0139843188226223,
      "learning_rate": 0.01990012484394507,
      "loss": 0.2484,
      "step": 24
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.022173702716827393,
      "learning_rate": 0.019895963379109448,
      "loss": 0.7319,
      "step": 25
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.019779937341809273,
      "learning_rate": 0.019891801914273824,
      "loss": 0.25,
      "step": 26
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.015408563427627087,
      "learning_rate": 0.019887640449438203,
      "loss": 0.6279,
      "step": 27
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.017980150878429413,
      "learning_rate": 0.01988347898460258,
      "loss": 0.7153,
      "step": 28
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.01553407683968544,
      "learning_rate": 0.019879317519766956,
      "loss": 0.6553,
      "step": 29
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.017622757703065872,
      "learning_rate": 0.019875156054931335,
      "loss": 0.54,
      "step": 30
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.019046414643526077,
      "learning_rate": 0.019870994590095715,
      "loss": 0.7466,
      "step": 31
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.018101511523127556,
      "learning_rate": 0.01986683312526009,
      "loss": 0.6934,
      "step": 32
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.01025469321757555,
      "learning_rate": 0.01986267166042447,
      "loss": 0.5674,
      "step": 33
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.02104567363858223,
      "learning_rate": 0.019858510195588847,
      "loss": 0.8809,
      "step": 34
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.011890831403434277,
      "learning_rate": 0.019854348730753226,
      "loss": 0.2898,
      "step": 35
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.022102177143096924,
      "learning_rate": 0.019850187265917602,
      "loss": 0.48,
      "step": 36
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.014674047939479351,
      "learning_rate": 0.019846025801081982,
      "loss": 0.3022,
      "step": 37
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.019139140844345093,
      "learning_rate": 0.01984186433624636,
      "loss": 0.3066,
      "step": 38
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.016503287479281425,
      "learning_rate": 0.019837702871410737,
      "loss": 0.5918,
      "step": 39
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.019555386155843735,
      "learning_rate": 0.019833541406575114,
      "loss": 0.4966,
      "step": 40
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.015926335006952286,
      "learning_rate": 0.019829379941739493,
      "loss": 0.4143,
      "step": 41
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.016720635816454887,
      "learning_rate": 0.01982521847690387,
      "loss": 0.6763,
      "step": 42
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.010436618700623512,
      "learning_rate": 0.01982105701206825,
      "loss": 0.7246,
      "step": 43
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.016686901450157166,
      "learning_rate": 0.01981689554723263,
      "loss": 0.321,
      "step": 44
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.013563442043960094,
      "learning_rate": 0.019812734082397004,
      "loss": 0.5669,
      "step": 45
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.01583418808877468,
      "learning_rate": 0.019808572617561384,
      "loss": 0.4619,
      "step": 46
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.012616423889994621,
      "learning_rate": 0.01980441115272576,
      "loss": 0.6016,
      "step": 47
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.019653743132948875,
      "learning_rate": 0.019800249687890136,
      "loss": 0.262,
      "step": 48
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.01714310795068741,
      "learning_rate": 0.019796088223054516,
      "loss": 0.4636,
      "step": 49
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.012529288418591022,
      "learning_rate": 0.019791926758218895,
      "loss": 0.0888,
      "step": 50
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.025462329387664795,
      "learning_rate": 0.01978776529338327,
      "loss": 1.1182,
      "step": 51
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.013185207732021809,
      "learning_rate": 0.01978360382854765,
      "loss": 0.6724,
      "step": 52
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.025705890730023384,
      "learning_rate": 0.019779442363712027,
      "loss": 1.2998,
      "step": 53
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.018231689929962158,
      "learning_rate": 0.019775280898876403,
      "loss": 0.4458,
      "step": 54
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.013859216123819351,
      "learning_rate": 0.019771119434040783,
      "loss": 0.623,
      "step": 55
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.020070232450962067,
      "learning_rate": 0.019766957969205162,
      "loss": 0.73,
      "step": 56
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.012890408746898174,
      "learning_rate": 0.01976279650436954,
      "loss": 0.4128,
      "step": 57
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.014063001610338688,
      "learning_rate": 0.019758635039533918,
      "loss": 0.4192,
      "step": 58
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.016345465555787086,
      "learning_rate": 0.019754473574698294,
      "loss": 0.5889,
      "step": 59
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.0205059964209795,
      "learning_rate": 0.01975031210986267,
      "loss": 0.4709,
      "step": 60
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.014671096578240395,
      "learning_rate": 0.01974615064502705,
      "loss": 0.3325,
      "step": 61
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.011892193928360939,
      "learning_rate": 0.01974198918019143,
      "loss": 0.4302,
      "step": 62
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.015515914186835289,
      "learning_rate": 0.019737827715355805,
      "loss": 0.9307,
      "step": 63
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.01472906768321991,
      "learning_rate": 0.019733666250520185,
      "loss": 0.3135,
      "step": 64
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.013207866810262203,
      "learning_rate": 0.01972950478568456,
      "loss": 0.4983,
      "step": 65
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.006755692884325981,
      "learning_rate": 0.019725343320848937,
      "loss": 0.4126,
      "step": 66
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.013154924847185612,
      "learning_rate": 0.019721181856013317,
      "loss": 0.5415,
      "step": 67
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.012332231737673283,
      "learning_rate": 0.019717020391177696,
      "loss": 0.5249,
      "step": 68
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.01766934059560299,
      "learning_rate": 0.019712858926342072,
      "loss": 0.4358,
      "step": 69
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.018197795376181602,
      "learning_rate": 0.019708697461506452,
      "loss": 0.6753,
      "step": 70
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.016164090484380722,
      "learning_rate": 0.019704535996670828,
      "loss": 0.4712,
      "step": 71
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.029932130128145218,
      "learning_rate": 0.019700374531835204,
      "loss": 0.5342,
      "step": 72
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.01345884520560503,
      "learning_rate": 0.019696213066999584,
      "loss": 0.5029,
      "step": 73
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.01129131205379963,
      "learning_rate": 0.019692051602163963,
      "loss": 0.5376,
      "step": 74
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.016407852992415428,
      "learning_rate": 0.01968789013732834,
      "loss": 0.791,
      "step": 75
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.019868925213813782,
      "learning_rate": 0.01968372867249272,
      "loss": 0.5015,
      "step": 76
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.0146525539457798,
      "learning_rate": 0.019679567207657095,
      "loss": 0.541,
      "step": 77
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.010244067758321762,
      "learning_rate": 0.019675405742821474,
      "loss": 0.2642,
      "step": 78
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.015067693777382374,
      "learning_rate": 0.01967124427798585,
      "loss": 0.2817,
      "step": 79
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.010389878414571285,
      "learning_rate": 0.01966708281315023,
      "loss": 0.6113,
      "step": 80
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.011934892274439335,
      "learning_rate": 0.019662921348314606,
      "loss": 0.0456,
      "step": 81
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.01124374195933342,
      "learning_rate": 0.019658759883478986,
      "loss": 0.2357,
      "step": 82
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.017671743407845497,
      "learning_rate": 0.019654598418643362,
      "loss": 0.5024,
      "step": 83
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.02609691210091114,
      "learning_rate": 0.01965043695380774,
      "loss": 0.5361,
      "step": 84
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.01812068186700344,
      "learning_rate": 0.019646275488972118,
      "loss": 0.4084,
      "step": 85
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.014044742099940777,
      "learning_rate": 0.019642114024136497,
      "loss": 0.2783,
      "step": 86
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.013514760881662369,
      "learning_rate": 0.019637952559300873,
      "loss": 0.5366,
      "step": 87
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.01608353666961193,
      "learning_rate": 0.019633791094465253,
      "loss": 0.4722,
      "step": 88
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.014681047759950161,
      "learning_rate": 0.019629629629629632,
      "loss": 0.3625,
      "step": 89
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.011236661113798618,
      "learning_rate": 0.01962546816479401,
      "loss": 0.2866,
      "step": 90
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.014246581122279167,
      "learning_rate": 0.019621306699958384,
      "loss": 0.4597,
      "step": 91
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.015461236238479614,
      "learning_rate": 0.019617145235122764,
      "loss": 0.251,
      "step": 92
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.013209463097155094,
      "learning_rate": 0.01961298377028714,
      "loss": 0.4397,
      "step": 93
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.017247267067432404,
      "learning_rate": 0.01960882230545152,
      "loss": 0.1571,
      "step": 94
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.0060082110576331615,
      "learning_rate": 0.0196046608406159,
      "loss": 0.0955,
      "step": 95
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.04800603538751602,
      "learning_rate": 0.019600499375780275,
      "loss": 0.5498,
      "step": 96
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.010818454436957836,
      "learning_rate": 0.01959633791094465,
      "loss": 0.3552,
      "step": 97
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.013875529170036316,
      "learning_rate": 0.01959217644610903,
      "loss": 0.2754,
      "step": 98
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.009726175107061863,
      "learning_rate": 0.019588014981273407,
      "loss": 0.5693,
      "step": 99
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.010561893694102764,
      "learning_rate": 0.019583853516437787,
      "loss": 0.269,
      "step": 100
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.018042325973510742,
      "learning_rate": 0.019579692051602166,
      "loss": 0.2888,
      "step": 101
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.011434352956712246,
      "learning_rate": 0.019575530586766542,
      "loss": 0.3032,
      "step": 102
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.01294125895947218,
      "learning_rate": 0.01957136912193092,
      "loss": 0.3904,
      "step": 103
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.00973493605852127,
      "learning_rate": 0.019567207657095298,
      "loss": 0.1273,
      "step": 104
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.011769156903028488,
      "learning_rate": 0.019563046192259674,
      "loss": 0.3728,
      "step": 105
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.013946526683866978,
      "learning_rate": 0.019558884727424054,
      "loss": 0.4209,
      "step": 106
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.024769112467765808,
      "learning_rate": 0.019554723262588433,
      "loss": 0.5957,
      "step": 107
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.013953709043562412,
      "learning_rate": 0.01955056179775281,
      "loss": 0.896,
      "step": 108
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.016479069367051125,
      "learning_rate": 0.019546400332917185,
      "loss": 0.2915,
      "step": 109
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.012262118980288506,
      "learning_rate": 0.019542238868081565,
      "loss": 0.3962,
      "step": 110
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.009211440570652485,
      "learning_rate": 0.019538077403245944,
      "loss": 0.2974,
      "step": 111
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.01734967529773712,
      "learning_rate": 0.01953391593841032,
      "loss": 0.583,
      "step": 112
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.02681916393339634,
      "learning_rate": 0.0195297544735747,
      "loss": 0.5518,
      "step": 113
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.019412348046898842,
      "learning_rate": 0.019525593008739076,
      "loss": 0.1301,
      "step": 114
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.0728633925318718,
      "learning_rate": 0.019521431543903452,
      "loss": 0.3469,
      "step": 115
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.010789559222757816,
      "learning_rate": 0.019517270079067832,
      "loss": 0.2566,
      "step": 116
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.01410316675901413,
      "learning_rate": 0.01951310861423221,
      "loss": 0.6997,
      "step": 117
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.02386266365647316,
      "learning_rate": 0.019508947149396588,
      "loss": 0.7905,
      "step": 118
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.01500307023525238,
      "learning_rate": 0.019504785684560967,
      "loss": 0.311,
      "step": 119
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.01638227328658104,
      "learning_rate": 0.019500624219725343,
      "loss": 0.377,
      "step": 120
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.018486998975276947,
      "learning_rate": 0.019496462754889723,
      "loss": 0.4883,
      "step": 121
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.004287502728402615,
      "learning_rate": 0.0194923012900541,
      "loss": 0.0809,
      "step": 122
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.012090643867850304,
      "learning_rate": 0.01948813982521848,
      "loss": 0.3147,
      "step": 123
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.012809454463422298,
      "learning_rate": 0.019483978360382855,
      "loss": 0.2346,
      "step": 124
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.03159857168793678,
      "learning_rate": 0.019479816895547234,
      "loss": 0.0633,
      "step": 125
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.03378335013985634,
      "learning_rate": 0.01947565543071161,
      "loss": 0.4414,
      "step": 126
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.00972415879368782,
      "learning_rate": 0.01947149396587599,
      "loss": 0.1332,
      "step": 127
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.014316284097731113,
      "learning_rate": 0.019467332501040366,
      "loss": 0.2642,
      "step": 128
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.01945355348289013,
      "learning_rate": 0.019463171036204745,
      "loss": 0.2568,
      "step": 129
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.014685138128697872,
      "learning_rate": 0.01945900957136912,
      "loss": 0.2495,
      "step": 130
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.010893230326473713,
      "learning_rate": 0.0194548481065335,
      "loss": 0.1844,
      "step": 131
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.015432030893862247,
      "learning_rate": 0.01945068664169788,
      "loss": 0.2192,
      "step": 132
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.023381555452942848,
      "learning_rate": 0.019446525176862257,
      "loss": 0.1019,
      "step": 133
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.025069458410143852,
      "learning_rate": 0.019442363712026633,
      "loss": 0.6982,
      "step": 134
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.013284072279930115,
      "learning_rate": 0.019438202247191012,
      "loss": 0.2725,
      "step": 135
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.01459498330950737,
      "learning_rate": 0.01943404078235539,
      "loss": 0.342,
      "step": 136
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.010649852454662323,
      "learning_rate": 0.019429879317519768,
      "loss": 0.1597,
      "step": 137
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.00572684733197093,
      "learning_rate": 0.019425717852684148,
      "loss": 0.0163,
      "step": 138
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.010500311851501465,
      "learning_rate": 0.019421556387848524,
      "loss": 0.3718,
      "step": 139
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.02403194084763527,
      "learning_rate": 0.0194173949230129,
      "loss": 0.3464,
      "step": 140
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.015808913856744766,
      "learning_rate": 0.01941323345817728,
      "loss": 0.0919,
      "step": 141
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.017955824732780457,
      "learning_rate": 0.019409071993341655,
      "loss": 0.3323,
      "step": 142
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.015312157571315765,
      "learning_rate": 0.019404910528506035,
      "loss": 0.4712,
      "step": 143
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.013958172872662544,
      "learning_rate": 0.019400749063670415,
      "loss": 0.1681,
      "step": 144
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.01720457337796688,
      "learning_rate": 0.01939658759883479,
      "loss": 0.541,
      "step": 145
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.017649512737989426,
      "learning_rate": 0.019392426133999167,
      "loss": 0.6196,
      "step": 146
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.016069641336798668,
      "learning_rate": 0.019388264669163546,
      "loss": 0.3416,
      "step": 147
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.015727760270237923,
      "learning_rate": 0.019384103204327922,
      "loss": 0.2766,
      "step": 148
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.011026602238416672,
      "learning_rate": 0.019379941739492302,
      "loss": 0.2603,
      "step": 149
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.011536615900695324,
      "learning_rate": 0.01937578027465668,
      "loss": 0.0615,
      "step": 150
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.018519138917326927,
      "learning_rate": 0.019371618809821058,
      "loss": 0.2013,
      "step": 151
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.019297972321510315,
      "learning_rate": 0.019367457344985434,
      "loss": 0.7739,
      "step": 152
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.035224251449108124,
      "learning_rate": 0.019363295880149813,
      "loss": 0.1853,
      "step": 153
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.011118430644273758,
      "learning_rate": 0.01935913441531419,
      "loss": 0.585,
      "step": 154
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.01876828819513321,
      "learning_rate": 0.01935497295047857,
      "loss": 0.439,
      "step": 155
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.013631806708872318,
      "learning_rate": 0.01935081148564295,
      "loss": 0.2147,
      "step": 156
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.013377000577747822,
      "learning_rate": 0.019346650020807325,
      "loss": 0.0768,
      "step": 157
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.009209630079567432,
      "learning_rate": 0.0193424885559717,
      "loss": 0.12,
      "step": 158
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.010687937960028648,
      "learning_rate": 0.01933832709113608,
      "loss": 0.3562,
      "step": 159
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.01562159787863493,
      "learning_rate": 0.019334165626300456,
      "loss": 0.2192,
      "step": 160
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.005969279445707798,
      "learning_rate": 0.019330004161464836,
      "loss": 0.0499,
      "step": 161
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.01860101707279682,
      "learning_rate": 0.019325842696629215,
      "loss": 0.4082,
      "step": 162
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.013926600106060505,
      "learning_rate": 0.01932168123179359,
      "loss": 0.27,
      "step": 163
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.007220026105642319,
      "learning_rate": 0.01931751976695797,
      "loss": 0.139,
      "step": 164
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.016826435923576355,
      "learning_rate": 0.019313358302122347,
      "loss": 0.4153,
      "step": 165
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.008984547108411789,
      "learning_rate": 0.019309196837286723,
      "loss": 0.2205,
      "step": 166
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.014899051748216152,
      "learning_rate": 0.019305035372451103,
      "loss": 0.4397,
      "step": 167
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.008856400847434998,
      "learning_rate": 0.019300873907615482,
      "loss": 0.0795,
      "step": 168
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.015470389276742935,
      "learning_rate": 0.01929671244277986,
      "loss": 0.6055,
      "step": 169
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.01177644170820713,
      "learning_rate": 0.019292550977944238,
      "loss": 0.3594,
      "step": 170
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.018051499500870705,
      "learning_rate": 0.019288389513108614,
      "loss": 0.7207,
      "step": 171
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.01194706466048956,
      "learning_rate": 0.01928422804827299,
      "loss": 0.4624,
      "step": 172
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.021377161145210266,
      "learning_rate": 0.01928006658343737,
      "loss": 0.4302,
      "step": 173
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.011635289527475834,
      "learning_rate": 0.01927590511860175,
      "loss": 0.2068,
      "step": 174
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.012977547012269497,
      "learning_rate": 0.019271743653766125,
      "loss": 0.3076,
      "step": 175
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.011483551934361458,
      "learning_rate": 0.019267582188930505,
      "loss": 0.2173,
      "step": 176
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.018402379006147385,
      "learning_rate": 0.01926342072409488,
      "loss": 0.3831,
      "step": 177
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.008669276721775532,
      "learning_rate": 0.019259259259259257,
      "loss": 0.1942,
      "step": 178
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.009989948943257332,
      "learning_rate": 0.019255097794423637,
      "loss": 0.4062,
      "step": 179
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.007285960018634796,
      "learning_rate": 0.019250936329588016,
      "loss": 0.0542,
      "step": 180
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.022931115701794624,
      "learning_rate": 0.019246774864752392,
      "loss": 0.3311,
      "step": 181
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.015280869789421558,
      "learning_rate": 0.019242613399916772,
      "loss": 0.1851,
      "step": 182
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.016446208581328392,
      "learning_rate": 0.019238451935081148,
      "loss": 0.3059,
      "step": 183
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.009221578016877174,
      "learning_rate": 0.019234290470245528,
      "loss": 0.1036,
      "step": 184
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.01323636807501316,
      "learning_rate": 0.019230129005409904,
      "loss": 0.3828,
      "step": 185
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.011723767034709454,
      "learning_rate": 0.019225967540574283,
      "loss": 0.6211,
      "step": 186
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.00718157272785902,
      "learning_rate": 0.019221806075738663,
      "loss": 0.1534,
      "step": 187
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.009665646590292454,
      "learning_rate": 0.01921764461090304,
      "loss": 0.1459,
      "step": 188
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.01421988382935524,
      "learning_rate": 0.019213483146067415,
      "loss": 0.3362,
      "step": 189
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.014052601531147957,
      "learning_rate": 0.019209321681231795,
      "loss": 0.4727,
      "step": 190
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.007880319841206074,
      "learning_rate": 0.01920516021639617,
      "loss": 0.0958,
      "step": 191
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.013098879717290401,
      "learning_rate": 0.01920099875156055,
      "loss": 0.2483,
      "step": 192
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.010881153866648674,
      "learning_rate": 0.01919683728672493,
      "loss": 0.0822,
      "step": 193
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.011607570573687553,
      "learning_rate": 0.019192675821889306,
      "loss": 0.4939,
      "step": 194
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.015284847468137741,
      "learning_rate": 0.019188514357053682,
      "loss": 0.4248,
      "step": 195
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.006532551255077124,
      "learning_rate": 0.01918435289221806,
      "loss": 0.0183,
      "step": 196
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.00698343338444829,
      "learning_rate": 0.019180191427382438,
      "loss": 0.0674,
      "step": 197
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.01601043902337551,
      "learning_rate": 0.019176029962546817,
      "loss": 1.0,
      "step": 198
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.018778465688228607,
      "learning_rate": 0.019171868497711197,
      "loss": 0.5615,
      "step": 199
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.017414990812540054,
      "learning_rate": 0.019167707032875573,
      "loss": 0.4424,
      "step": 200
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.009499352425336838,
      "learning_rate": 0.01916354556803995,
      "loss": 0.0865,
      "step": 201
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.011209850199520588,
      "learning_rate": 0.01915938410320433,
      "loss": 0.3103,
      "step": 202
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.011232535354793072,
      "learning_rate": 0.019155222638368705,
      "loss": 0.3169,
      "step": 203
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.016806548461318016,
      "learning_rate": 0.019151061173533084,
      "loss": 0.6245,
      "step": 204
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.01189274899661541,
      "learning_rate": 0.019146899708697464,
      "loss": 0.1663,
      "step": 205
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.015232094563543797,
      "learning_rate": 0.01914273824386184,
      "loss": 0.6851,
      "step": 206
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.02008361555635929,
      "learning_rate": 0.01913857677902622,
      "loss": 0.4065,
      "step": 207
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.007860634475946426,
      "learning_rate": 0.019134415314190596,
      "loss": 0.114,
      "step": 208
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.009345010854303837,
      "learning_rate": 0.01913025384935497,
      "loss": 0.3083,
      "step": 209
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.01482164952903986,
      "learning_rate": 0.01912609238451935,
      "loss": 0.5288,
      "step": 210
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.013445671647787094,
      "learning_rate": 0.01912193091968373,
      "loss": 0.0577,
      "step": 211
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.01909550465643406,
      "learning_rate": 0.019117769454848107,
      "loss": 0.3801,
      "step": 212
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.013162663206458092,
      "learning_rate": 0.019113607990012486,
      "loss": 0.2639,
      "step": 213
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.01083003357052803,
      "learning_rate": 0.019109446525176862,
      "loss": 0.1499,
      "step": 214
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.00959020759910345,
      "learning_rate": 0.01910528506034124,
      "loss": 0.1101,
      "step": 215
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.010005058720707893,
      "learning_rate": 0.019101123595505618,
      "loss": 0.2026,
      "step": 216
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.02039876952767372,
      "learning_rate": 0.019096962130669998,
      "loss": 0.282,
      "step": 217
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.014765151776373386,
      "learning_rate": 0.019092800665834374,
      "loss": 0.3125,
      "step": 218
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.009571024216711521,
      "learning_rate": 0.019088639200998753,
      "loss": 0.183,
      "step": 219
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.010038415901362896,
      "learning_rate": 0.01908447773616313,
      "loss": 0.1028,
      "step": 220
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.012818839401006699,
      "learning_rate": 0.019080316271327506,
      "loss": 0.5225,
      "step": 221
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.014451993629336357,
      "learning_rate": 0.019076154806491885,
      "loss": 0.168,
      "step": 222
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.006471678148955107,
      "learning_rate": 0.019071993341656265,
      "loss": 0.0197,
      "step": 223
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.004482632502913475,
      "learning_rate": 0.01906783187682064,
      "loss": 0.0293,
      "step": 224
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.016593338921666145,
      "learning_rate": 0.01906367041198502,
      "loss": 0.5806,
      "step": 225
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.00906510278582573,
      "learning_rate": 0.019059508947149396,
      "loss": 0.1466,
      "step": 226
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.010454477742314339,
      "learning_rate": 0.019055347482313773,
      "loss": 0.1746,
      "step": 227
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.0206717811524868,
      "learning_rate": 0.019051186017478152,
      "loss": 0.6543,
      "step": 228
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.009957603178918362,
      "learning_rate": 0.01904702455264253,
      "loss": 0.0541,
      "step": 229
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.02599034458398819,
      "learning_rate": 0.019042863087806908,
      "loss": 0.4255,
      "step": 230
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.017418760806322098,
      "learning_rate": 0.019038701622971287,
      "loss": 0.4253,
      "step": 231
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.013599650003015995,
      "learning_rate": 0.019034540158135663,
      "loss": 0.3621,
      "step": 232
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.012645847164094448,
      "learning_rate": 0.01903037869330004,
      "loss": 0.1808,
      "step": 233
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.011502467095851898,
      "learning_rate": 0.01902621722846442,
      "loss": 0.4675,
      "step": 234
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.009859512560069561,
      "learning_rate": 0.0190220557636288,
      "loss": 0.0282,
      "step": 235
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.009849142283201218,
      "learning_rate": 0.019017894298793175,
      "loss": 0.0837,
      "step": 236
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.015944460406899452,
      "learning_rate": 0.019013732833957554,
      "loss": 0.2374,
      "step": 237
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.015828672796487808,
      "learning_rate": 0.01900957136912193,
      "loss": 0.5649,
      "step": 238
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.011765114963054657,
      "learning_rate": 0.01900540990428631,
      "loss": 0.4055,
      "step": 239
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.012066415511071682,
      "learning_rate": 0.019001248439450686,
      "loss": 0.3311,
      "step": 240
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.01208413578569889,
      "learning_rate": 0.018997086974615066,
      "loss": 0.3303,
      "step": 241
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.020172521471977234,
      "learning_rate": 0.01899292550977944,
      "loss": 0.6533,
      "step": 242
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.013071781024336815,
      "learning_rate": 0.01898876404494382,
      "loss": 0.2922,
      "step": 243
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.014257648959755898,
      "learning_rate": 0.018984602580108197,
      "loss": 0.5166,
      "step": 244
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.008450665511190891,
      "learning_rate": 0.018980441115272577,
      "loss": 0.1483,
      "step": 245
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01024819165468216,
      "learning_rate": 0.018976279650436953,
      "loss": 0.3325,
      "step": 246
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.012138486839830875,
      "learning_rate": 0.018972118185601333,
      "loss": 0.3181,
      "step": 247
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01706678234040737,
      "learning_rate": 0.01896795672076571,
      "loss": 0.5381,
      "step": 248
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.0165503341704607,
      "learning_rate": 0.018963795255930088,
      "loss": 0.3474,
      "step": 249
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.017817780375480652,
      "learning_rate": 0.018959633791094468,
      "loss": 0.1743,
      "step": 250
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01351422630250454,
      "learning_rate": 0.018955472326258844,
      "loss": 0.1572,
      "step": 251
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01826195791363716,
      "learning_rate": 0.01895131086142322,
      "loss": 0.3682,
      "step": 252
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.01721639558672905,
      "learning_rate": 0.0189471493965876,
      "loss": 0.4712,
      "step": 253
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.0058861286379396915,
      "learning_rate": 0.018942987931751976,
      "loss": 0.0426,
      "step": 254
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.0133828679099679,
      "learning_rate": 0.018938826466916355,
      "loss": 0.3984,
      "step": 255
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.015871714800596237,
      "learning_rate": 0.018934665002080735,
      "loss": 0.4358,
      "step": 256
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.012308573350310326,
      "learning_rate": 0.01893050353724511,
      "loss": 0.303,
      "step": 257
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.010923982597887516,
      "learning_rate": 0.018926342072409487,
      "loss": 0.1774,
      "step": 258
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.013635121285915375,
      "learning_rate": 0.018922180607573866,
      "loss": 0.4199,
      "step": 259
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.013736642897129059,
      "learning_rate": 0.018918019142738246,
      "loss": 0.3162,
      "step": 260
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.018210401758551598,
      "learning_rate": 0.018913857677902622,
      "loss": 0.3315,
      "step": 261
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.01903851330280304,
      "learning_rate": 0.018909696213067,
      "loss": 0.6294,
      "step": 262
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.01758592203259468,
      "learning_rate": 0.018905534748231378,
      "loss": 0.853,
      "step": 263
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.014722253195941448,
      "learning_rate": 0.018901373283395754,
      "loss": 0.2148,
      "step": 264
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.010228004306554794,
      "learning_rate": 0.018897211818560133,
      "loss": 0.07,
      "step": 265
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.017041178420186043,
      "learning_rate": 0.018893050353724513,
      "loss": 0.3865,
      "step": 266
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.024548158049583435,
      "learning_rate": 0.01888888888888889,
      "loss": 0.408,
      "step": 267
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.021002428606152534,
      "learning_rate": 0.01888472742405327,
      "loss": 0.4521,
      "step": 268
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.023030998185276985,
      "learning_rate": 0.018880565959217645,
      "loss": 0.6274,
      "step": 269
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.01349740382283926,
      "learning_rate": 0.01887640449438202,
      "loss": 0.3054,
      "step": 270
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.01190619170665741,
      "learning_rate": 0.0188722430295464,
      "loss": 0.1816,
      "step": 271
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.01658334955573082,
      "learning_rate": 0.01886808156471078,
      "loss": 0.491,
      "step": 272
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.008173597976565361,
      "learning_rate": 0.018863920099875156,
      "loss": 0.0178,
      "step": 273
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.017877396196126938,
      "learning_rate": 0.018859758635039536,
      "loss": 0.4114,
      "step": 274
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.012737761251628399,
      "learning_rate": 0.01885559717020391,
      "loss": 0.2288,
      "step": 275
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.02273883856832981,
      "learning_rate": 0.018851435705368288,
      "loss": 0.4519,
      "step": 276
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.006959815509617329,
      "learning_rate": 0.018847274240532667,
      "loss": 0.0447,
      "step": 277
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.012611573562026024,
      "learning_rate": 0.018843112775697047,
      "loss": 0.1226,
      "step": 278
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.014003496617078781,
      "learning_rate": 0.018838951310861423,
      "loss": 0.4014,
      "step": 279
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.01877623423933983,
      "learning_rate": 0.018834789846025803,
      "loss": 0.2646,
      "step": 280
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.018713688477873802,
      "learning_rate": 0.01883062838119018,
      "loss": 0.5034,
      "step": 281
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.02422862872481346,
      "learning_rate": 0.018826466916354558,
      "loss": 0.8271,
      "step": 282
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.016888445243239403,
      "learning_rate": 0.018822305451518934,
      "loss": 0.541,
      "step": 283
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.025673290714621544,
      "learning_rate": 0.018818143986683314,
      "loss": 0.5269,
      "step": 284
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.013471826910972595,
      "learning_rate": 0.01881398252184769,
      "loss": 0.1483,
      "step": 285
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.016805335879325867,
      "learning_rate": 0.01880982105701207,
      "loss": 0.4895,
      "step": 286
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.02230897918343544,
      "learning_rate": 0.018805659592176446,
      "loss": 0.2627,
      "step": 287
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.017700405791401863,
      "learning_rate": 0.018801498127340825,
      "loss": 0.2869,
      "step": 288
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.026683760806918144,
      "learning_rate": 0.0187973366625052,
      "loss": 0.4714,
      "step": 289
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.017930233851075172,
      "learning_rate": 0.01879317519766958,
      "loss": 0.124,
      "step": 290
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.014429387636482716,
      "learning_rate": 0.018789013732833957,
      "loss": 0.4141,
      "step": 291
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.012393507175147533,
      "learning_rate": 0.018784852267998337,
      "loss": 0.1812,
      "step": 292
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.013746536336839199,
      "learning_rate": 0.018780690803162716,
      "loss": 0.2296,
      "step": 293
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.017592409625649452,
      "learning_rate": 0.018776529338327092,
      "loss": 0.4626,
      "step": 294
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.02273542992770672,
      "learning_rate": 0.01877236787349147,
      "loss": 0.2491,
      "step": 295
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.04013910889625549,
      "learning_rate": 0.018768206408655848,
      "loss": 0.5811,
      "step": 296
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.012396056205034256,
      "learning_rate": 0.018764044943820224,
      "loss": 0.5254,
      "step": 297
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.01785169169306755,
      "learning_rate": 0.018759883478984603,
      "loss": 0.397,
      "step": 298
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.01874547079205513,
      "learning_rate": 0.018755722014148983,
      "loss": 0.2937,
      "step": 299
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.01092853955924511,
      "learning_rate": 0.01875156054931336,
      "loss": 0.4192,
      "step": 300
    },
    {
      "epoch": 0.37,
      "eval_loss": 0.3251953125,
      "eval_runtime": 183.3307,
      "eval_samples_per_second": 1.096,
      "eval_steps_per_second": 0.551,
      "step": 300
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.012453447096049786,
      "learning_rate": 0.018747399084477735,
      "loss": 0.2705,
      "step": 301
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.020620588213205338,
      "learning_rate": 0.018743237619642115,
      "loss": 0.6685,
      "step": 302
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.018006984144449234,
      "learning_rate": 0.01873907615480649,
      "loss": 0.4458,
      "step": 303
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.012783350422978401,
      "learning_rate": 0.01873491468997087,
      "loss": 0.345,
      "step": 304
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.019229650497436523,
      "learning_rate": 0.01873075322513525,
      "loss": 0.3953,
      "step": 305
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.017787378281354904,
      "learning_rate": 0.018726591760299626,
      "loss": 0.624,
      "step": 306
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.01644415408372879,
      "learning_rate": 0.018722430295464002,
      "loss": 0.2329,
      "step": 307
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.012315182946622372,
      "learning_rate": 0.018718268830628382,
      "loss": 0.1984,
      "step": 308
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.012109145522117615,
      "learning_rate": 0.018714107365792758,
      "loss": 0.2041,
      "step": 309
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.013556061312556267,
      "learning_rate": 0.018709945900957137,
      "loss": 0.4844,
      "step": 310
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.016224991530179977,
      "learning_rate": 0.018705784436121517,
      "loss": 0.5752,
      "step": 311
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.021627897396683693,
      "learning_rate": 0.018701622971285893,
      "loss": 0.2301,
      "step": 312
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.022303931415081024,
      "learning_rate": 0.01869746150645027,
      "loss": 0.4536,
      "step": 313
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.014758111909031868,
      "learning_rate": 0.01869330004161465,
      "loss": 0.3098,
      "step": 314
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.013408167287707329,
      "learning_rate": 0.018689138576779025,
      "loss": 0.0468,
      "step": 315
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.017526132985949516,
      "learning_rate": 0.018684977111943404,
      "loss": 0.1798,
      "step": 316
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.025102650746703148,
      "learning_rate": 0.018680815647107784,
      "loss": 0.5762,
      "step": 317
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.014943995513021946,
      "learning_rate": 0.01867665418227216,
      "loss": 0.4561,
      "step": 318
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.016996843740344048,
      "learning_rate": 0.018672492717436536,
      "loss": 0.2896,
      "step": 319
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.011097901500761509,
      "learning_rate": 0.018668331252600916,
      "loss": 0.1134,
      "step": 320
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.013591892085969448,
      "learning_rate": 0.018664169787765292,
      "loss": 0.1129,
      "step": 321
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.015822574496269226,
      "learning_rate": 0.01866000832292967,
      "loss": 0.2207,
      "step": 322
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.015380003489553928,
      "learning_rate": 0.01865584685809405,
      "loss": 0.16,
      "step": 323
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.008600973524153233,
      "learning_rate": 0.018651685393258427,
      "loss": 0.0347,
      "step": 324
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.014758262783288956,
      "learning_rate": 0.018647523928422807,
      "loss": 0.2202,
      "step": 325
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.016456298530101776,
      "learning_rate": 0.018643362463587183,
      "loss": 0.2969,
      "step": 326
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.014062338508665562,
      "learning_rate": 0.01863920099875156,
      "loss": 0.2715,
      "step": 327
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.014881882816553116,
      "learning_rate": 0.01863503953391594,
      "loss": 0.022,
      "step": 328
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.0236114040017128,
      "learning_rate": 0.018630878069080318,
      "loss": 0.2834,
      "step": 329
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.023272477090358734,
      "learning_rate": 0.018626716604244694,
      "loss": 0.5132,
      "step": 330
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.012915327213704586,
      "learning_rate": 0.018622555139409074,
      "loss": 0.1646,
      "step": 331
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.027405433356761932,
      "learning_rate": 0.01861839367457345,
      "loss": 0.4375,
      "step": 332
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.019116155803203583,
      "learning_rate": 0.01861423220973783,
      "loss": 0.2905,
      "step": 333
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.015979696065187454,
      "learning_rate": 0.018610070744902205,
      "loss": 0.2188,
      "step": 334
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.016301333904266357,
      "learning_rate": 0.018605909280066585,
      "loss": 0.2358,
      "step": 335
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.011888011358678341,
      "learning_rate": 0.018601747815230964,
      "loss": 0.246,
      "step": 336
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.023288020864129066,
      "learning_rate": 0.01859758635039534,
      "loss": 0.541,
      "step": 337
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.005208710674196482,
      "learning_rate": 0.018593424885559717,
      "loss": 0.0097,
      "step": 338
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.011656765826046467,
      "learning_rate": 0.018589263420724096,
      "loss": 0.0943,
      "step": 339
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.026298977434635162,
      "learning_rate": 0.018585101955888472,
      "loss": 1.1572,
      "step": 340
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.015261673368513584,
      "learning_rate": 0.018580940491052852,
      "loss": 0.0914,
      "step": 341
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.018320735543966293,
      "learning_rate": 0.01857677902621723,
      "loss": 0.4023,
      "step": 342
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.020216815173625946,
      "learning_rate": 0.018572617561381607,
      "loss": 0.26,
      "step": 343
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.016558609902858734,
      "learning_rate": 0.018568456096545984,
      "loss": 0.301,
      "step": 344
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.013275112956762314,
      "learning_rate": 0.018564294631710363,
      "loss": 0.1666,
      "step": 345
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.023631185293197632,
      "learning_rate": 0.01856013316687474,
      "loss": 0.365,
      "step": 346
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.023109402507543564,
      "learning_rate": 0.01855597170203912,
      "loss": 0.4705,
      "step": 347
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.024755585938692093,
      "learning_rate": 0.0185518102372035,
      "loss": 0.6177,
      "step": 348
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.038343414664268494,
      "learning_rate": 0.018547648772367874,
      "loss": 0.4514,
      "step": 349
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.026825403794646263,
      "learning_rate": 0.01854348730753225,
      "loss": 0.5698,
      "step": 350
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.01578792929649353,
      "learning_rate": 0.01853932584269663,
      "loss": 0.2073,
      "step": 351
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.010524454526603222,
      "learning_rate": 0.018535164377861006,
      "loss": 0.2559,
      "step": 352
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.010751763358712196,
      "learning_rate": 0.018531002913025386,
      "loss": 0.1526,
      "step": 353
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.015715204179286957,
      "learning_rate": 0.018526841448189765,
      "loss": 0.4263,
      "step": 354
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.01730133220553398,
      "learning_rate": 0.01852267998335414,
      "loss": 0.3289,
      "step": 355
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.016789477318525314,
      "learning_rate": 0.018518518518518517,
      "loss": 0.209,
      "step": 356
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.01814727671444416,
      "learning_rate": 0.018514357053682897,
      "loss": 0.2617,
      "step": 357
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.02244481071829796,
      "learning_rate": 0.018510195588847273,
      "loss": 0.5991,
      "step": 358
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.01240987703204155,
      "learning_rate": 0.018506034124011653,
      "loss": 0.187,
      "step": 359
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.01305888220667839,
      "learning_rate": 0.018501872659176032,
      "loss": 0.1776,
      "step": 360
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.015885083004832268,
      "learning_rate": 0.01849771119434041,
      "loss": 0.2406,
      "step": 361
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.018674463033676147,
      "learning_rate": 0.018493549729504784,
      "loss": 0.4717,
      "step": 362
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.011706807650625706,
      "learning_rate": 0.018489388264669164,
      "loss": 0.1733,
      "step": 363
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.012210343964397907,
      "learning_rate": 0.01848522679983354,
      "loss": 0.0668,
      "step": 364
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.023025935515761375,
      "learning_rate": 0.01848106533499792,
      "loss": 0.4929,
      "step": 365
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.019617287442088127,
      "learning_rate": 0.0184769038701623,
      "loss": 0.3542,
      "step": 366
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.019019676372408867,
      "learning_rate": 0.018472742405326675,
      "loss": 0.1937,
      "step": 367
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.019696107134222984,
      "learning_rate": 0.018468580940491055,
      "loss": 0.5415,
      "step": 368
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.01248240377753973,
      "learning_rate": 0.01846441947565543,
      "loss": 0.3943,
      "step": 369
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.01081449817866087,
      "learning_rate": 0.018460258010819807,
      "loss": 0.1473,
      "step": 370
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.02130720019340515,
      "learning_rate": 0.018456096545984187,
      "loss": 0.2279,
      "step": 371
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.023912722244858742,
      "learning_rate": 0.018451935081148566,
      "loss": 0.623,
      "step": 372
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.013592472299933434,
      "learning_rate": 0.018447773616312942,
      "loss": 0.1094,
      "step": 373
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.01946699246764183,
      "learning_rate": 0.018443612151477322,
      "loss": 0.3538,
      "step": 374
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.01677008904516697,
      "learning_rate": 0.018439450686641698,
      "loss": 0.5879,
      "step": 375
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.014828789979219437,
      "learning_rate": 0.018435289221806074,
      "loss": 0.4316,
      "step": 376
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.020289145410060883,
      "learning_rate": 0.018431127756970454,
      "loss": 0.4058,
      "step": 377
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.02077432908117771,
      "learning_rate": 0.018426966292134833,
      "loss": 0.1469,
      "step": 378
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.010249602608382702,
      "learning_rate": 0.01842280482729921,
      "loss": 0.015,
      "step": 379
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.016949528828263283,
      "learning_rate": 0.01841864336246359,
      "loss": 0.2949,
      "step": 380
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.01481599546968937,
      "learning_rate": 0.018414481897627965,
      "loss": 0.1646,
      "step": 381
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.030077632516622543,
      "learning_rate": 0.01841032043279234,
      "loss": 0.4893,
      "step": 382
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.013509195297956467,
      "learning_rate": 0.01840615896795672,
      "loss": 0.1575,
      "step": 383
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.015224408358335495,
      "learning_rate": 0.0184019975031211,
      "loss": 0.4365,
      "step": 384
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.019919302314519882,
      "learning_rate": 0.018397836038285476,
      "loss": 0.2722,
      "step": 385
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.011082062497735023,
      "learning_rate": 0.018393674573449856,
      "loss": 0.259,
      "step": 386
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.028045201674103737,
      "learning_rate": 0.018389513108614232,
      "loss": 0.2676,
      "step": 387
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.027130700647830963,
      "learning_rate": 0.018385351643778608,
      "loss": 0.3445,
      "step": 388
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.013158266432583332,
      "learning_rate": 0.018381190178942988,
      "loss": 0.1779,
      "step": 389
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.021059006452560425,
      "learning_rate": 0.018377028714107367,
      "loss": 0.0562,
      "step": 390
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.009331466630101204,
      "learning_rate": 0.018372867249271743,
      "loss": 0.1804,
      "step": 391
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.012068395502865314,
      "learning_rate": 0.018368705784436123,
      "loss": 0.2776,
      "step": 392
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.026677824556827545,
      "learning_rate": 0.0183645443196005,
      "loss": 0.3213,
      "step": 393
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.029340950772166252,
      "learning_rate": 0.018360382854764875,
      "loss": 0.3765,
      "step": 394
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.015520433895289898,
      "learning_rate": 0.018356221389929255,
      "loss": 0.2681,
      "step": 395
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.02110999822616577,
      "learning_rate": 0.018352059925093634,
      "loss": 0.2029,
      "step": 396
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.01838153973221779,
      "learning_rate": 0.01834789846025801,
      "loss": 0.0622,
      "step": 397
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.01988046057522297,
      "learning_rate": 0.01834373699542239,
      "loss": 0.3376,
      "step": 398
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.020841067656874657,
      "learning_rate": 0.018339575530586766,
      "loss": 0.4333,
      "step": 399
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.009150462225079536,
      "learning_rate": 0.018335414065751145,
      "loss": 0.0217,
      "step": 400
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.028743427246809006,
      "learning_rate": 0.01833125260091552,
      "loss": 0.0804,
      "step": 401
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.013679815456271172,
      "learning_rate": 0.0183270911360799,
      "loss": 0.1171,
      "step": 402
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.019026050344109535,
      "learning_rate": 0.01832292967124428,
      "loss": 0.2389,
      "step": 403
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.025706058368086815,
      "learning_rate": 0.018318768206408657,
      "loss": 0.6992,
      "step": 404
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.0252694021910429,
      "learning_rate": 0.018314606741573033,
      "loss": 0.4209,
      "step": 405
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.024600857868790627,
      "learning_rate": 0.018310445276737412,
      "loss": 0.6948,
      "step": 406
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.022102979943156242,
      "learning_rate": 0.01830628381190179,
      "loss": 0.3425,
      "step": 407
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.0011589693604037166,
      "learning_rate": 0.018302122347066168,
      "loss": 0.0023,
      "step": 408
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.01646098494529724,
      "learning_rate": 0.018297960882230548,
      "loss": 0.2659,
      "step": 409
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.01534626167267561,
      "learning_rate": 0.018293799417394924,
      "loss": 0.3,
      "step": 410
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.022580618038773537,
      "learning_rate": 0.018289637952559303,
      "loss": 0.4175,
      "step": 411
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.017710313200950623,
      "learning_rate": 0.01828547648772368,
      "loss": 0.4795,
      "step": 412
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.015363357961177826,
      "learning_rate": 0.018281315022888055,
      "loss": 0.1886,
      "step": 413
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.021407626569271088,
      "learning_rate": 0.018277153558052435,
      "loss": 0.1099,
      "step": 414
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.01610243320465088,
      "learning_rate": 0.018272992093216815,
      "loss": 0.4968,
      "step": 415
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.03274524584412575,
      "learning_rate": 0.01826883062838119,
      "loss": 0.7466,
      "step": 416
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.028339523822069168,
      "learning_rate": 0.01826466916354557,
      "loss": 0.7749,
      "step": 417
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.013688563369214535,
      "learning_rate": 0.018260507698709946,
      "loss": 0.0635,
      "step": 418
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.021380651742219925,
      "learning_rate": 0.018256346233874322,
      "loss": 0.4131,
      "step": 419
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.026248306035995483,
      "learning_rate": 0.018252184769038702,
      "loss": 0.2974,
      "step": 420
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.02018003910779953,
      "learning_rate": 0.01824802330420308,
      "loss": 0.2115,
      "step": 421
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.024361971765756607,
      "learning_rate": 0.018243861839367458,
      "loss": 0.6206,
      "step": 422
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.015359626151621342,
      "learning_rate": 0.018239700374531837,
      "loss": 0.1203,
      "step": 423
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.016143817454576492,
      "learning_rate": 0.018235538909696213,
      "loss": 0.2698,
      "step": 424
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.015773026272654533,
      "learning_rate": 0.01823137744486059,
      "loss": 0.1071,
      "step": 425
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.011392384767532349,
      "learning_rate": 0.01822721598002497,
      "loss": 0.1879,
      "step": 426
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.012423066422343254,
      "learning_rate": 0.01822305451518935,
      "loss": 0.0672,
      "step": 427
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.029530595988035202,
      "learning_rate": 0.018218893050353725,
      "loss": 0.3613,
      "step": 428
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.024436235427856445,
      "learning_rate": 0.018214731585518104,
      "loss": 0.3896,
      "step": 429
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.016348615288734436,
      "learning_rate": 0.01821057012068248,
      "loss": 0.3174,
      "step": 430
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.015601023100316525,
      "learning_rate": 0.018206408655846856,
      "loss": 0.2079,
      "step": 431
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.014723938889801502,
      "learning_rate": 0.018202247191011236,
      "loss": 0.2329,
      "step": 432
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.024064524099230766,
      "learning_rate": 0.018198085726175615,
      "loss": 0.4507,
      "step": 433
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.025292137637734413,
      "learning_rate": 0.01819392426133999,
      "loss": 0.6035,
      "step": 434
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.022207777947187424,
      "learning_rate": 0.01818976279650437,
      "loss": 0.7056,
      "step": 435
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.022870400920510292,
      "learning_rate": 0.018185601331668747,
      "loss": 0.2622,
      "step": 436
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.020909421145915985,
      "learning_rate": 0.018181439866833123,
      "loss": 0.2377,
      "step": 437
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.021142246201634407,
      "learning_rate": 0.018177278401997503,
      "loss": 0.2996,
      "step": 438
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.01641058176755905,
      "learning_rate": 0.018173116937161882,
      "loss": 0.3174,
      "step": 439
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.021069413051009178,
      "learning_rate": 0.01816895547232626,
      "loss": 0.2654,
      "step": 440
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.025653362274169922,
      "learning_rate": 0.018164794007490638,
      "loss": 0.4221,
      "step": 441
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.01986721344292164,
      "learning_rate": 0.018160632542655014,
      "loss": 0.3657,
      "step": 442
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.021333815529942513,
      "learning_rate": 0.018156471077819394,
      "loss": 0.467,
      "step": 443
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.018084486946463585,
      "learning_rate": 0.01815230961298377,
      "loss": 0.2644,
      "step": 444
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.028349634259939194,
      "learning_rate": 0.01814814814814815,
      "loss": 0.5391,
      "step": 445
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.022226015105843544,
      "learning_rate": 0.018143986683312525,
      "loss": 0.5645,
      "step": 446
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.015429804101586342,
      "learning_rate": 0.018139825218476905,
      "loss": 0.0782,
      "step": 447
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.024509834125638008,
      "learning_rate": 0.01813566375364128,
      "loss": 0.4612,
      "step": 448
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.011498290114104748,
      "learning_rate": 0.01813150228880566,
      "loss": 0.1084,
      "step": 449
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.016364967450499535,
      "learning_rate": 0.018127340823970037,
      "loss": 0.3008,
      "step": 450
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.0184890478849411,
      "learning_rate": 0.018123179359134416,
      "loss": 0.2186,
      "step": 451
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.01742672361433506,
      "learning_rate": 0.018119017894298792,
      "loss": 0.243,
      "step": 452
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.01943567395210266,
      "learning_rate": 0.018114856429463172,
      "loss": 0.2913,
      "step": 453
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.024606555700302124,
      "learning_rate": 0.01811069496462755,
      "loss": 0.0767,
      "step": 454
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.008739195764064789,
      "learning_rate": 0.018106533499791928,
      "loss": 0.0287,
      "step": 455
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.01797635853290558,
      "learning_rate": 0.018102372034956304,
      "loss": 0.1814,
      "step": 456
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.015862181782722473,
      "learning_rate": 0.018098210570120683,
      "loss": 0.4111,
      "step": 457
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.022920627146959305,
      "learning_rate": 0.01809404910528506,
      "loss": 0.375,
      "step": 458
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.01702135242521763,
      "learning_rate": 0.01808988764044944,
      "loss": 0.3601,
      "step": 459
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.016336023807525635,
      "learning_rate": 0.01808572617561382,
      "loss": 0.1782,
      "step": 460
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.015450933948159218,
      "learning_rate": 0.018081564710778195,
      "loss": 0.2888,
      "step": 461
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.02359917387366295,
      "learning_rate": 0.01807740324594257,
      "loss": 0.313,
      "step": 462
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.011471382342278957,
      "learning_rate": 0.01807324178110695,
      "loss": 0.1418,
      "step": 463
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.019025051966309547,
      "learning_rate": 0.018069080316271326,
      "loss": 0.3232,
      "step": 464
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.0016236061928793788,
      "learning_rate": 0.018064918851435706,
      "loss": 0.0021,
      "step": 465
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.018086018040776253,
      "learning_rate": 0.018060757386600085,
      "loss": 0.3213,
      "step": 466
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.014555457048118114,
      "learning_rate": 0.01805659592176446,
      "loss": 0.144,
      "step": 467
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.01344548910856247,
      "learning_rate": 0.018052434456928838,
      "loss": 0.1176,
      "step": 468
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.014173297211527824,
      "learning_rate": 0.018048272992093217,
      "loss": 0.0638,
      "step": 469
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.021349281072616577,
      "learning_rate": 0.018044111527257593,
      "loss": 0.1447,
      "step": 470
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.02288207970559597,
      "learning_rate": 0.018039950062421973,
      "loss": 0.3296,
      "step": 471
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.0221555233001709,
      "learning_rate": 0.018035788597586352,
      "loss": 0.2883,
      "step": 472
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.020956367254257202,
      "learning_rate": 0.01803162713275073,
      "loss": 0.2776,
      "step": 473
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.03914099186658859,
      "learning_rate": 0.018027465667915105,
      "loss": 0.2311,
      "step": 474
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.016052138060331345,
      "learning_rate": 0.018023304203079484,
      "loss": 0.2822,
      "step": 475
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.02345450036227703,
      "learning_rate": 0.018019142738243864,
      "loss": 0.4175,
      "step": 476
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.015222625806927681,
      "learning_rate": 0.01801498127340824,
      "loss": 0.2379,
      "step": 477
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.01661856472492218,
      "learning_rate": 0.01801081980857262,
      "loss": 0.142,
      "step": 478
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.027985790744423866,
      "learning_rate": 0.018006658343736996,
      "loss": 0.2676,
      "step": 479
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.005155233666300774,
      "learning_rate": 0.01800249687890137,
      "loss": 0.0064,
      "step": 480
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.01854134164750576,
      "learning_rate": 0.01799833541406575,
      "loss": 0.3274,
      "step": 481
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.021399671211838722,
      "learning_rate": 0.01799417394923013,
      "loss": 0.3025,
      "step": 482
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.013370412401854992,
      "learning_rate": 0.017990012484394507,
      "loss": 0.22,
      "step": 483
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.02801896072924137,
      "learning_rate": 0.017985851019558886,
      "loss": 0.5527,
      "step": 484
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.016469566151499748,
      "learning_rate": 0.017981689554723262,
      "loss": 0.3855,
      "step": 485
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.018887661397457123,
      "learning_rate": 0.017977528089887642,
      "loss": 0.1444,
      "step": 486
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.016790995374321938,
      "learning_rate": 0.017973366625052018,
      "loss": 0.1682,
      "step": 487
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.01708918623626232,
      "learning_rate": 0.017969205160216398,
      "loss": 0.2532,
      "step": 488
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.013119114562869072,
      "learning_rate": 0.017965043695380774,
      "loss": 0.2593,
      "step": 489
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.01730186864733696,
      "learning_rate": 0.017960882230545153,
      "loss": 0.4368,
      "step": 490
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.0007973984465934336,
      "learning_rate": 0.01795672076570953,
      "loss": 0.0015,
      "step": 491
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.016564833000302315,
      "learning_rate": 0.01795255930087391,
      "loss": 0.1823,
      "step": 492
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.017495324835181236,
      "learning_rate": 0.017948397836038285,
      "loss": 0.2197,
      "step": 493
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.013706967234611511,
      "learning_rate": 0.017944236371202665,
      "loss": 0.0506,
      "step": 494
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.02236577868461609,
      "learning_rate": 0.01794007490636704,
      "loss": 0.0427,
      "step": 495
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.016671421006321907,
      "learning_rate": 0.01793591344153142,
      "loss": 0.4666,
      "step": 496
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.021448083221912384,
      "learning_rate": 0.0179317519766958,
      "loss": 0.2856,
      "step": 497
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.022695811465382576,
      "learning_rate": 0.017927590511860176,
      "loss": 0.4028,
      "step": 498
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.013902803882956505,
      "learning_rate": 0.017923429047024552,
      "loss": 0.386,
      "step": 499
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.013315980322659016,
      "learning_rate": 0.01791926758218893,
      "loss": 0.0199,
      "step": 500
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.01322450116276741,
      "learning_rate": 0.017915106117353308,
      "loss": 0.0723,
      "step": 501
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.026917418465018272,
      "learning_rate": 0.017910944652517687,
      "loss": 0.4121,
      "step": 502
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.017656033858656883,
      "learning_rate": 0.017906783187682067,
      "loss": 0.166,
      "step": 503
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.016564955934882164,
      "learning_rate": 0.017902621722846443,
      "loss": 0.4065,
      "step": 504
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.008763215504586697,
      "learning_rate": 0.01789846025801082,
      "loss": 0.0299,
      "step": 505
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.0215713270008564,
      "learning_rate": 0.0178942987931752,
      "loss": 0.4661,
      "step": 506
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.011343676596879959,
      "learning_rate": 0.017890137328339575,
      "loss": 0.1272,
      "step": 507
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.021843140944838524,
      "learning_rate": 0.017885975863503954,
      "loss": 0.3967,
      "step": 508
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.005304041784256697,
      "learning_rate": 0.017881814398668334,
      "loss": 0.0106,
      "step": 509
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.014229088090360165,
      "learning_rate": 0.01787765293383271,
      "loss": 0.1688,
      "step": 510
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.02197921648621559,
      "learning_rate": 0.017873491468997086,
      "loss": 0.3103,
      "step": 511
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.0247966218739748,
      "learning_rate": 0.017869330004161466,
      "loss": 0.2664,
      "step": 512
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.017200419679284096,
      "learning_rate": 0.01786516853932584,
      "loss": 0.1678,
      "step": 513
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.01293894462287426,
      "learning_rate": 0.01786100707449022,
      "loss": 0.136,
      "step": 514
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.015838829800486565,
      "learning_rate": 0.0178568456096546,
      "loss": 0.2961,
      "step": 515
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.014234489761292934,
      "learning_rate": 0.017852684144818977,
      "loss": 0.4495,
      "step": 516
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.022854570299386978,
      "learning_rate": 0.017848522679983353,
      "loss": 0.1954,
      "step": 517
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.03496082127094269,
      "learning_rate": 0.017844361215147733,
      "loss": 0.8726,
      "step": 518
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.007512242998927832,
      "learning_rate": 0.01784019975031211,
      "loss": 0.0353,
      "step": 519
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.012978347018361092,
      "learning_rate": 0.017836038285476488,
      "loss": 0.47,
      "step": 520
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.0132742365822196,
      "learning_rate": 0.017831876820640868,
      "loss": 0.0959,
      "step": 521
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.023542677983641624,
      "learning_rate": 0.017827715355805244,
      "loss": 0.217,
      "step": 522
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.01400215644389391,
      "learning_rate": 0.01782355389096962,
      "loss": 0.0919,
      "step": 523
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.010237505659461021,
      "learning_rate": 0.017819392426134,
      "loss": 0.0629,
      "step": 524
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.015069641172885895,
      "learning_rate": 0.017815230961298376,
      "loss": 0.1357,
      "step": 525
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.011609483510255814,
      "learning_rate": 0.017811069496462755,
      "loss": 0.0459,
      "step": 526
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.01928263157606125,
      "learning_rate": 0.017806908031627135,
      "loss": 0.3618,
      "step": 527
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.015118389390408993,
      "learning_rate": 0.01780274656679151,
      "loss": 0.1812,
      "step": 528
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.030556663870811462,
      "learning_rate": 0.01779858510195589,
      "loss": 0.4568,
      "step": 529
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.02257608063519001,
      "learning_rate": 0.017794423637120266,
      "loss": 0.2603,
      "step": 530
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.02450978383421898,
      "learning_rate": 0.017790262172284643,
      "loss": 0.3921,
      "step": 531
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.018064459785819054,
      "learning_rate": 0.017786100707449022,
      "loss": 0.1653,
      "step": 532
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.014579844661056995,
      "learning_rate": 0.0177819392426134,
      "loss": 0.5142,
      "step": 533
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.018851060420274734,
      "learning_rate": 0.017777777777777778,
      "loss": 0.2979,
      "step": 534
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.01819692738354206,
      "learning_rate": 0.017773616312942157,
      "loss": 0.4919,
      "step": 535
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.012211748398840427,
      "learning_rate": 0.017769454848106533,
      "loss": 0.3062,
      "step": 536
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.019866351038217545,
      "learning_rate": 0.01776529338327091,
      "loss": 0.5698,
      "step": 537
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.01991969533264637,
      "learning_rate": 0.01776113191843529,
      "loss": 0.4614,
      "step": 538
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.01758577860891819,
      "learning_rate": 0.01775697045359967,
      "loss": 0.1968,
      "step": 539
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.011264219880104065,
      "learning_rate": 0.017752808988764045,
      "loss": 0.0826,
      "step": 540
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.018839716911315918,
      "learning_rate": 0.017748647523928424,
      "loss": 0.4075,
      "step": 541
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.012899573892354965,
      "learning_rate": 0.0177444860590928,
      "loss": 0.2368,
      "step": 542
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.02240430936217308,
      "learning_rate": 0.017740324594257176,
      "loss": 0.2886,
      "step": 543
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.01477088499814272,
      "learning_rate": 0.017736163129421556,
      "loss": 0.0632,
      "step": 544
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.0271300058811903,
      "learning_rate": 0.017732001664585936,
      "loss": 0.98,
      "step": 545
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.03747643902897835,
      "learning_rate": 0.01772784019975031,
      "loss": 0.2114,
      "step": 546
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.013209293596446514,
      "learning_rate": 0.01772367873491469,
      "loss": 0.1293,
      "step": 547
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.023283572867512703,
      "learning_rate": 0.017719517270079067,
      "loss": 0.1272,
      "step": 548
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.02200518362224102,
      "learning_rate": 0.017715355805243447,
      "loss": 0.6167,
      "step": 549
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.018459653481841087,
      "learning_rate": 0.017711194340407823,
      "loss": 0.1938,
      "step": 550
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.01839401386678219,
      "learning_rate": 0.017707032875572203,
      "loss": 0.4368,
      "step": 551
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.031117385253310204,
      "learning_rate": 0.017702871410736582,
      "loss": 0.5078,
      "step": 552
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.016220442950725555,
      "learning_rate": 0.017698709945900958,
      "loss": 0.2343,
      "step": 553
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.028234465047717094,
      "learning_rate": 0.017694548481065334,
      "loss": 0.4602,
      "step": 554
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.021623658016324043,
      "learning_rate": 0.017690387016229714,
      "loss": 0.241,
      "step": 555
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.018278444185853004,
      "learning_rate": 0.01768622555139409,
      "loss": 0.2126,
      "step": 556
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.018106509000062943,
      "learning_rate": 0.01768206408655847,
      "loss": 0.4719,
      "step": 557
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.012071166187524796,
      "learning_rate": 0.01767790262172285,
      "loss": 0.1615,
      "step": 558
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.026161981746554375,
      "learning_rate": 0.017673741156887225,
      "loss": 0.6118,
      "step": 559
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.01915688067674637,
      "learning_rate": 0.0176695796920516,
      "loss": 0.0596,
      "step": 560
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.01590966060757637,
      "learning_rate": 0.01766541822721598,
      "loss": 0.1901,
      "step": 561
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.009362030774354935,
      "learning_rate": 0.017661256762380357,
      "loss": 0.0426,
      "step": 562
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.015719180926680565,
      "learning_rate": 0.017657095297544737,
      "loss": 0.2035,
      "step": 563
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.014808082953095436,
      "learning_rate": 0.017652933832709116,
      "loss": 0.1589,
      "step": 564
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.022726433351635933,
      "learning_rate": 0.017648772367873492,
      "loss": 0.2559,
      "step": 565
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.02343447506427765,
      "learning_rate": 0.017644610903037868,
      "loss": 0.4946,
      "step": 566
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.016504652798175812,
      "learning_rate": 0.017640449438202248,
      "loss": 0.1726,
      "step": 567
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.016712628304958344,
      "learning_rate": 0.017636287973366624,
      "loss": 0.2517,
      "step": 568
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.023331880569458008,
      "learning_rate": 0.017632126508531003,
      "loss": 0.3794,
      "step": 569
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.005479931831359863,
      "learning_rate": 0.017627965043695383,
      "loss": 0.0262,
      "step": 570
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.015947291627526283,
      "learning_rate": 0.01762380357885976,
      "loss": 0.2195,
      "step": 571
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.0175675917416811,
      "learning_rate": 0.01761964211402414,
      "loss": 0.1621,
      "step": 572
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.02074829861521721,
      "learning_rate": 0.017615480649188515,
      "loss": 0.27,
      "step": 573
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.0031139347702264786,
      "learning_rate": 0.01761131918435289,
      "loss": 0.005,
      "step": 574
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.013330874964594841,
      "learning_rate": 0.01760715771951727,
      "loss": 0.2313,
      "step": 575
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.018731823191046715,
      "learning_rate": 0.01760299625468165,
      "loss": 0.3057,
      "step": 576
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.03177861496806145,
      "learning_rate": 0.017598834789846026,
      "loss": 0.3982,
      "step": 577
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.022089118137955666,
      "learning_rate": 0.017594673325010406,
      "loss": 0.3254,
      "step": 578
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.010733268223702908,
      "learning_rate": 0.017590511860174782,
      "loss": 0.0645,
      "step": 579
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.022312788292765617,
      "learning_rate": 0.017586350395339158,
      "loss": 0.4197,
      "step": 580
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.020953146740794182,
      "learning_rate": 0.017582188930503537,
      "loss": 0.4355,
      "step": 581
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.014730071648955345,
      "learning_rate": 0.017578027465667917,
      "loss": 0.3032,
      "step": 582
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.008113733492791653,
      "learning_rate": 0.017573866000832293,
      "loss": 0.0445,
      "step": 583
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.015508532524108887,
      "learning_rate": 0.017569704535996673,
      "loss": 0.1904,
      "step": 584
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.028984874486923218,
      "learning_rate": 0.01756554307116105,
      "loss": 0.522,
      "step": 585
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.017001159489154816,
      "learning_rate": 0.017561381606325425,
      "loss": 0.5171,
      "step": 586
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.006894498132169247,
      "learning_rate": 0.017557220141489804,
      "loss": 0.0688,
      "step": 587
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.020242296159267426,
      "learning_rate": 0.017553058676654184,
      "loss": 0.3547,
      "step": 588
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.02047143317759037,
      "learning_rate": 0.01754889721181856,
      "loss": 0.1675,
      "step": 589
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.026067951694130898,
      "learning_rate": 0.01754473574698294,
      "loss": 0.1515,
      "step": 590
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.030283140018582344,
      "learning_rate": 0.017540574282147316,
      "loss": 0.5273,
      "step": 591
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.018824715167284012,
      "learning_rate": 0.017536412817311692,
      "loss": 0.4636,
      "step": 592
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.013927336782217026,
      "learning_rate": 0.01753225135247607,
      "loss": 0.332,
      "step": 593
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.014410550706088543,
      "learning_rate": 0.01752808988764045,
      "loss": 0.1453,
      "step": 594
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.0069348462857306,
      "learning_rate": 0.017523928422804827,
      "loss": 0.0132,
      "step": 595
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.014951486140489578,
      "learning_rate": 0.017519766957969207,
      "loss": 0.2013,
      "step": 596
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.023131584748625755,
      "learning_rate": 0.017515605493133583,
      "loss": 0.5049,
      "step": 597
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.016293618828058243,
      "learning_rate": 0.01751144402829796,
      "loss": 0.303,
      "step": 598
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.011231700889766216,
      "learning_rate": 0.01750728256346234,
      "loss": 0.1244,
      "step": 599
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.02238783799111843,
      "learning_rate": 0.017503121098626718,
      "loss": 0.4756,
      "step": 600
    },
    {
      "epoch": 0.75,
      "eval_loss": 0.2890625,
      "eval_runtime": 183.0178,
      "eval_samples_per_second": 1.098,
      "eval_steps_per_second": 0.552,
      "step": 600
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.007596211973577738,
      "learning_rate": 0.017498959633791094,
      "loss": 0.0335,
      "step": 601
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.023596379905939102,
      "learning_rate": 0.017494798168955474,
      "loss": 0.281,
      "step": 602
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.015985485166311264,
      "learning_rate": 0.01749063670411985,
      "loss": 0.1886,
      "step": 603
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.017524354159832,
      "learning_rate": 0.01748647523928423,
      "loss": 0.3987,
      "step": 604
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.03007538802921772,
      "learning_rate": 0.017482313774448605,
      "loss": 0.2864,
      "step": 605
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.019697928801178932,
      "learning_rate": 0.017478152309612985,
      "loss": 0.2177,
      "step": 606
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.029588017612695694,
      "learning_rate": 0.01747399084477736,
      "loss": 0.1004,
      "step": 607
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.010724203661084175,
      "learning_rate": 0.01746982937994174,
      "loss": 0.0515,
      "step": 608
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.011881942860782146,
      "learning_rate": 0.017465667915106117,
      "loss": 0.0547,
      "step": 609
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.03622346371412277,
      "learning_rate": 0.017461506450270496,
      "loss": 0.5923,
      "step": 610
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.019429568201303482,
      "learning_rate": 0.017457344985434872,
      "loss": 0.2776,
      "step": 611
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.028999803587794304,
      "learning_rate": 0.017453183520599252,
      "loss": 0.25,
      "step": 612
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.012266678735613823,
      "learning_rate": 0.017449022055763628,
      "loss": 0.0726,
      "step": 613
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.007757336366921663,
      "learning_rate": 0.017444860590928007,
      "loss": 0.0082,
      "step": 614
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.023186028003692627,
      "learning_rate": 0.017440699126092387,
      "loss": 0.4475,
      "step": 615
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.01969732716679573,
      "learning_rate": 0.017436537661256763,
      "loss": 0.0712,
      "step": 616
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.03147481009364128,
      "learning_rate": 0.01743237619642114,
      "loss": 0.3835,
      "step": 617
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.03012148104608059,
      "learning_rate": 0.01742821473158552,
      "loss": 0.4275,
      "step": 618
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.022663293406367302,
      "learning_rate": 0.0174240532667499,
      "loss": 0.4573,
      "step": 619
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.015420160256326199,
      "learning_rate": 0.017419891801914274,
      "loss": 0.068,
      "step": 620
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.014434471726417542,
      "learning_rate": 0.017415730337078654,
      "loss": 0.1478,
      "step": 621
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.013026262633502483,
      "learning_rate": 0.01741156887224303,
      "loss": 0.0424,
      "step": 622
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.02457118220627308,
      "learning_rate": 0.017407407407407406,
      "loss": 0.4966,
      "step": 623
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.017207419499754906,
      "learning_rate": 0.017403245942571786,
      "loss": 0.332,
      "step": 624
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.020753245800733566,
      "learning_rate": 0.017399084477736165,
      "loss": 0.2368,
      "step": 625
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.007681884802877903,
      "learning_rate": 0.01739492301290054,
      "loss": 0.0363,
      "step": 626
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.020835790783166885,
      "learning_rate": 0.01739076154806492,
      "loss": 0.4446,
      "step": 627
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.02038772776722908,
      "learning_rate": 0.017386600083229297,
      "loss": 0.3643,
      "step": 628
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.015672018751502037,
      "learning_rate": 0.017382438618393673,
      "loss": 0.1254,
      "step": 629
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.033103521913290024,
      "learning_rate": 0.017378277153558053,
      "loss": 0.4282,
      "step": 630
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.00956597737967968,
      "learning_rate": 0.017374115688722432,
      "loss": 0.0905,
      "step": 631
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.011758465319871902,
      "learning_rate": 0.01736995422388681,
      "loss": 0.0837,
      "step": 632
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.007047138176858425,
      "learning_rate": 0.017365792759051188,
      "loss": 0.0301,
      "step": 633
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.011444054543972015,
      "learning_rate": 0.017361631294215564,
      "loss": 0.2379,
      "step": 634
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.020829778164625168,
      "learning_rate": 0.01735746982937994,
      "loss": 0.2778,
      "step": 635
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.0010183991398662329,
      "learning_rate": 0.01735330836454432,
      "loss": 0.0013,
      "step": 636
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.023843087255954742,
      "learning_rate": 0.0173491468997087,
      "loss": 0.2979,
      "step": 637
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.015374841168522835,
      "learning_rate": 0.017344985434873075,
      "loss": 0.1357,
      "step": 638
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.01591675356030464,
      "learning_rate": 0.017340823970037455,
      "loss": 0.2201,
      "step": 639
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.009581562131643295,
      "learning_rate": 0.01733666250520183,
      "loss": 0.0497,
      "step": 640
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.04708545282483101,
      "learning_rate": 0.017332501040366207,
      "loss": 0.3418,
      "step": 641
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.02598581276834011,
      "learning_rate": 0.017328339575530587,
      "loss": 0.5991,
      "step": 642
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.021948708221316338,
      "learning_rate": 0.017324178110694966,
      "loss": 0.2468,
      "step": 643
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.0162848848849535,
      "learning_rate": 0.017320016645859342,
      "loss": 0.2642,
      "step": 644
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.01911388337612152,
      "learning_rate": 0.017315855181023722,
      "loss": 0.4583,
      "step": 645
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.012276459485292435,
      "learning_rate": 0.017311693716188098,
      "loss": 0.2476,
      "step": 646
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.020888086408376694,
      "learning_rate": 0.017307532251352477,
      "loss": 0.3142,
      "step": 647
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.01172264851629734,
      "learning_rate": 0.017303370786516854,
      "loss": 0.0508,
      "step": 648
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.016026314347982407,
      "learning_rate": 0.017299209321681233,
      "loss": 0.28,
      "step": 649
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.022636111825704575,
      "learning_rate": 0.01729504785684561,
      "loss": 0.4592,
      "step": 650
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.01944619044661522,
      "learning_rate": 0.01729088639200999,
      "loss": 0.4524,
      "step": 651
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.017689630389213562,
      "learning_rate": 0.017286724927174365,
      "loss": 0.2343,
      "step": 652
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.009402558207511902,
      "learning_rate": 0.017282563462338744,
      "loss": 0.1232,
      "step": 653
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.02710762619972229,
      "learning_rate": 0.01727840199750312,
      "loss": 0.2598,
      "step": 654
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.028154436498880386,
      "learning_rate": 0.0172742405326675,
      "loss": 0.3254,
      "step": 655
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.013529193587601185,
      "learning_rate": 0.017270079067831876,
      "loss": 0.2168,
      "step": 656
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.01863241009414196,
      "learning_rate": 0.017265917602996256,
      "loss": 0.4363,
      "step": 657
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.01757151633501053,
      "learning_rate": 0.017261756138160635,
      "loss": 0.3752,
      "step": 658
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.01853569597005844,
      "learning_rate": 0.01725759467332501,
      "loss": 0.1473,
      "step": 659
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.00975044071674347,
      "learning_rate": 0.017253433208489388,
      "loss": 0.0475,
      "step": 660
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.010218817740678787,
      "learning_rate": 0.017249271743653767,
      "loss": 0.0728,
      "step": 661
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.025808019563555717,
      "learning_rate": 0.017245110278818143,
      "loss": 0.5747,
      "step": 662
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.01887434720993042,
      "learning_rate": 0.017240948813982523,
      "loss": 0.2776,
      "step": 663
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.019917313009500504,
      "learning_rate": 0.017236787349146902,
      "loss": 0.3391,
      "step": 664
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.020668426528573036,
      "learning_rate": 0.01723262588431128,
      "loss": 0.1902,
      "step": 665
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.013723928481340408,
      "learning_rate": 0.017228464419475654,
      "loss": 0.103,
      "step": 666
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.002858448540791869,
      "learning_rate": 0.017224302954640034,
      "loss": 0.0057,
      "step": 667
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.010656685568392277,
      "learning_rate": 0.01722014148980441,
      "loss": 0.0876,
      "step": 668
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.02592632547020912,
      "learning_rate": 0.01721598002496879,
      "loss": 0.7422,
      "step": 669
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.015538211911916733,
      "learning_rate": 0.01721181856013317,
      "loss": 0.2396,
      "step": 670
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.02267232909798622,
      "learning_rate": 0.017207657095297545,
      "loss": 0.2925,
      "step": 671
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.018011275678873062,
      "learning_rate": 0.01720349563046192,
      "loss": 0.4634,
      "step": 672
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.01288380566984415,
      "learning_rate": 0.0171993341656263,
      "loss": 0.1312,
      "step": 673
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.024027708917856216,
      "learning_rate": 0.017195172700790677,
      "loss": 0.2993,
      "step": 674
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.007085581310093403,
      "learning_rate": 0.017191011235955057,
      "loss": 0.0382,
      "step": 675
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.02409108355641365,
      "learning_rate": 0.017186849771119436,
      "loss": 0.1493,
      "step": 676
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.018357884138822556,
      "learning_rate": 0.017182688306283812,
      "loss": 0.2805,
      "step": 677
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.017132099717855453,
      "learning_rate": 0.01717852684144819,
      "loss": 0.249,
      "step": 678
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.02037871442735195,
      "learning_rate": 0.017174365376612568,
      "loss": 0.5376,
      "step": 679
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.023976625874638557,
      "learning_rate": 0.017170203911776944,
      "loss": 0.1683,
      "step": 680
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.034002043306827545,
      "learning_rate": 0.017166042446941324,
      "loss": 0.2203,
      "step": 681
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.013092076405882835,
      "learning_rate": 0.017161880982105703,
      "loss": 0.0602,
      "step": 682
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.01644989661872387,
      "learning_rate": 0.01715771951727008,
      "loss": 0.2192,
      "step": 683
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.020339403301477432,
      "learning_rate": 0.017153558052434455,
      "loss": 0.4336,
      "step": 684
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.00023564025468658656,
      "learning_rate": 0.017149396587598835,
      "loss": 0.0004,
      "step": 685
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.018476612865924835,
      "learning_rate": 0.01714523512276321,
      "loss": 0.2593,
      "step": 686
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.020325573161244392,
      "learning_rate": 0.01714107365792759,
      "loss": 0.2556,
      "step": 687
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.024439135566353798,
      "learning_rate": 0.01713691219309197,
      "loss": 0.2673,
      "step": 688
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.024870112538337708,
      "learning_rate": 0.017132750728256346,
      "loss": 0.5039,
      "step": 689
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.02139340527355671,
      "learning_rate": 0.017128589263420726,
      "loss": 0.2881,
      "step": 690
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.019896239042282104,
      "learning_rate": 0.017124427798585102,
      "loss": 0.261,
      "step": 691
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.021802013739943504,
      "learning_rate": 0.01712026633374948,
      "loss": 0.3359,
      "step": 692
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.024331288412213326,
      "learning_rate": 0.017116104868913858,
      "loss": 0.5405,
      "step": 693
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.0015833770157769322,
      "learning_rate": 0.017111943404078237,
      "loss": 0.0021,
      "step": 694
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.016407884657382965,
      "learning_rate": 0.017107781939242613,
      "loss": 0.3918,
      "step": 695
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.016509268432855606,
      "learning_rate": 0.017103620474406993,
      "loss": 0.1449,
      "step": 696
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.013785818591713905,
      "learning_rate": 0.01709945900957137,
      "loss": 0.0516,
      "step": 697
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.023755798116326332,
      "learning_rate": 0.01709529754473575,
      "loss": 0.2493,
      "step": 698
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.01698322221636772,
      "learning_rate": 0.017091136079900125,
      "loss": 0.2065,
      "step": 699
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.015968430787324905,
      "learning_rate": 0.017086974615064504,
      "loss": 0.153,
      "step": 700
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.02535528875887394,
      "learning_rate": 0.017082813150228884,
      "loss": 0.4792,
      "step": 701
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.013174477964639664,
      "learning_rate": 0.01707865168539326,
      "loss": 0.1099,
      "step": 702
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.016354139894247055,
      "learning_rate": 0.017074490220557636,
      "loss": 0.16,
      "step": 703
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.025194713845849037,
      "learning_rate": 0.017070328755722015,
      "loss": 0.6577,
      "step": 704
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.017464514821767807,
      "learning_rate": 0.01706616729088639,
      "loss": 0.2629,
      "step": 705
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.026380835101008415,
      "learning_rate": 0.01706200582605077,
      "loss": 0.3901,
      "step": 706
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.00990260485559702,
      "learning_rate": 0.01705784436121515,
      "loss": 0.0551,
      "step": 707
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.01918584108352661,
      "learning_rate": 0.017053682896379527,
      "loss": 0.5674,
      "step": 708
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.020656105130910873,
      "learning_rate": 0.017049521431543903,
      "loss": 0.184,
      "step": 709
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.0175065565854311,
      "learning_rate": 0.017045359966708282,
      "loss": 0.224,
      "step": 710
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.01879909820854664,
      "learning_rate": 0.01704119850187266,
      "loss": 0.1205,
      "step": 711
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.017924316227436066,
      "learning_rate": 0.017037037037037038,
      "loss": 0.3433,
      "step": 712
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.027081698179244995,
      "learning_rate": 0.017032875572201418,
      "loss": 0.7124,
      "step": 713
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.010771729983389378,
      "learning_rate": 0.017028714107365794,
      "loss": 0.0534,
      "step": 714
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.0018802395788952708,
      "learning_rate": 0.01702455264253017,
      "loss": 0.0037,
      "step": 715
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.0325336679816246,
      "learning_rate": 0.01702039117769455,
      "loss": 0.3564,
      "step": 716
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.018861383199691772,
      "learning_rate": 0.017016229712858925,
      "loss": 0.3057,
      "step": 717
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.015037810429930687,
      "learning_rate": 0.017012068248023305,
      "loss": 0.1246,
      "step": 718
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.01766980066895485,
      "learning_rate": 0.017007906783187685,
      "loss": 0.4333,
      "step": 719
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.030631523579359055,
      "learning_rate": 0.01700374531835206,
      "loss": 0.9087,
      "step": 720
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.01510903611779213,
      "learning_rate": 0.016999583853516437,
      "loss": 0.2042,
      "step": 721
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.02484944276511669,
      "learning_rate": 0.016995422388680816,
      "loss": 0.1501,
      "step": 722
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.021651616320014,
      "learning_rate": 0.016991260923845192,
      "loss": 0.5435,
      "step": 723
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.02143322303891182,
      "learning_rate": 0.016987099459009572,
      "loss": 0.303,
      "step": 724
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.021336205303668976,
      "learning_rate": 0.01698293799417395,
      "loss": 0.2406,
      "step": 725
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.027396924793720245,
      "learning_rate": 0.016978776529338328,
      "loss": 0.201,
      "step": 726
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.021244604140520096,
      "learning_rate": 0.016974615064502704,
      "loss": 0.3933,
      "step": 727
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.01537284255027771,
      "learning_rate": 0.016970453599667083,
      "loss": 0.3054,
      "step": 728
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.010397098027169704,
      "learning_rate": 0.01696629213483146,
      "loss": 0.0867,
      "step": 729
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.019817698746919632,
      "learning_rate": 0.01696213066999584,
      "loss": 0.4824,
      "step": 730
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.00739717110991478,
      "learning_rate": 0.01695796920516022,
      "loss": 0.0374,
      "step": 731
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.024609167128801346,
      "learning_rate": 0.016953807740324595,
      "loss": 0.3767,
      "step": 732
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.014955861493945122,
      "learning_rate": 0.016949646275488974,
      "loss": 0.1862,
      "step": 733
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.021667569875717163,
      "learning_rate": 0.01694548481065335,
      "loss": 0.188,
      "step": 734
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.01052304357290268,
      "learning_rate": 0.016941323345817726,
      "loss": 0.0376,
      "step": 735
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.021990731358528137,
      "learning_rate": 0.016937161880982106,
      "loss": 0.1248,
      "step": 736
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.02426757477223873,
      "learning_rate": 0.016933000416146485,
      "loss": 0.3538,
      "step": 737
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.013726136647164822,
      "learning_rate": 0.01692883895131086,
      "loss": 0.1019,
      "step": 738
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.02444680966436863,
      "learning_rate": 0.01692467748647524,
      "loss": 0.3425,
      "step": 739
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.020823568105697632,
      "learning_rate": 0.016920516021639617,
      "loss": 0.3403,
      "step": 740
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.017308739945292473,
      "learning_rate": 0.016916354556803993,
      "loss": 0.2034,
      "step": 741
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.019694337621331215,
      "learning_rate": 0.016912193091968373,
      "loss": 0.2474,
      "step": 742
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.026313232257962227,
      "learning_rate": 0.016908031627132752,
      "loss": 0.334,
      "step": 743
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.011907829903066158,
      "learning_rate": 0.01690387016229713,
      "loss": 0.1154,
      "step": 744
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.02314845472574234,
      "learning_rate": 0.016899708697461508,
      "loss": 0.4189,
      "step": 745
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.017764708027243614,
      "learning_rate": 0.016895547232625884,
      "loss": 0.3669,
      "step": 746
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.02391207590699196,
      "learning_rate": 0.01689138576779026,
      "loss": 0.5166,
      "step": 747
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.01798044703900814,
      "learning_rate": 0.01688722430295464,
      "loss": 0.2668,
      "step": 748
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.016684608533978462,
      "learning_rate": 0.01688306283811902,
      "loss": 0.1847,
      "step": 749
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.02295888401567936,
      "learning_rate": 0.016878901373283395,
      "loss": 0.4199,
      "step": 750
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.01934513822197914,
      "learning_rate": 0.016874739908447775,
      "loss": 0.4692,
      "step": 751
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.020473016425967216,
      "learning_rate": 0.01687057844361215,
      "loss": 0.2822,
      "step": 752
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.013607135973870754,
      "learning_rate": 0.016866416978776527,
      "loss": 0.0914,
      "step": 753
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.015915434807538986,
      "learning_rate": 0.016862255513940907,
      "loss": 0.0856,
      "step": 754
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.015103944577276707,
      "learning_rate": 0.016858094049105286,
      "loss": 0.2494,
      "step": 755
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.0011906675063073635,
      "learning_rate": 0.016853932584269662,
      "loss": 0.0012,
      "step": 756
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.029274487867951393,
      "learning_rate": 0.016849771119434042,
      "loss": 0.4189,
      "step": 757
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.020976923406124115,
      "learning_rate": 0.016845609654598418,
      "loss": 0.3796,
      "step": 758
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.012252322398126125,
      "learning_rate": 0.016841448189762794,
      "loss": 0.1074,
      "step": 759
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.018603961914777756,
      "learning_rate": 0.016837286724927174,
      "loss": 0.1809,
      "step": 760
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.023209350183606148,
      "learning_rate": 0.016833125260091553,
      "loss": 0.4783,
      "step": 761
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.009813662618398666,
      "learning_rate": 0.01682896379525593,
      "loss": 0.064,
      "step": 762
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.012941231951117516,
      "learning_rate": 0.01682480233042031,
      "loss": 0.1528,
      "step": 763
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.03905802220106125,
      "learning_rate": 0.016820640865584685,
      "loss": 0.6968,
      "step": 764
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.0210886150598526,
      "learning_rate": 0.016816479400749065,
      "loss": 0.3132,
      "step": 765
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.02608836255967617,
      "learning_rate": 0.01681231793591344,
      "loss": 0.3147,
      "step": 766
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.017373185604810715,
      "learning_rate": 0.01680815647107782,
      "loss": 0.3047,
      "step": 767
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.02733996883034706,
      "learning_rate": 0.0168039950062422,
      "loss": 0.2959,
      "step": 768
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.013021211139857769,
      "learning_rate": 0.016799833541406576,
      "loss": 0.0486,
      "step": 769
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.03494340926408768,
      "learning_rate": 0.016795672076570952,
      "loss": 0.6533,
      "step": 770
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.00021946057677268982,
      "learning_rate": 0.01679151061173533,
      "loss": 0.0003,
      "step": 771
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.019399845972657204,
      "learning_rate": 0.016787349146899708,
      "loss": 0.2708,
      "step": 772
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.040895044803619385,
      "learning_rate": 0.016783187682064087,
      "loss": 0.4102,
      "step": 773
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.025216389447450638,
      "learning_rate": 0.016779026217228467,
      "loss": 0.3115,
      "step": 774
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.018137844279408455,
      "learning_rate": 0.016774864752392843,
      "loss": 0.2991,
      "step": 775
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.018322160467505455,
      "learning_rate": 0.016770703287557222,
      "loss": 0.1591,
      "step": 776
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.019225873053073883,
      "learning_rate": 0.0167665418227216,
      "loss": 0.283,
      "step": 777
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.01226904895156622,
      "learning_rate": 0.016762380357885975,
      "loss": 0.1577,
      "step": 778
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.003205365501344204,
      "learning_rate": 0.016758218893050354,
      "loss": 0.0049,
      "step": 779
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.022741660475730896,
      "learning_rate": 0.016754057428214734,
      "loss": 0.355,
      "step": 780
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.01638094149529934,
      "learning_rate": 0.01674989596337911,
      "loss": 0.2808,
      "step": 781
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.025196842849254608,
      "learning_rate": 0.01674573449854349,
      "loss": 0.6519,
      "step": 782
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.012154725380241871,
      "learning_rate": 0.016741573033707866,
      "loss": 0.1292,
      "step": 783
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.010786495171487331,
      "learning_rate": 0.01673741156887224,
      "loss": 0.0632,
      "step": 784
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.017939181998372078,
      "learning_rate": 0.01673325010403662,
      "loss": 0.2375,
      "step": 785
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.021562866866588593,
      "learning_rate": 0.016729088639201,
      "loss": 0.4019,
      "step": 786
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.01772688515484333,
      "learning_rate": 0.016724927174365377,
      "loss": 0.1752,
      "step": 787
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.017077792435884476,
      "learning_rate": 0.016720765709529756,
      "loss": 0.126,
      "step": 788
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.0011668333318084478,
      "learning_rate": 0.016716604244694133,
      "loss": 0.001,
      "step": 789
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.014091866090893745,
      "learning_rate": 0.01671244277985851,
      "loss": 0.1432,
      "step": 790
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.015959175303578377,
      "learning_rate": 0.016708281315022888,
      "loss": 0.3792,
      "step": 791
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.03273507580161095,
      "learning_rate": 0.016704119850187268,
      "loss": 0.4983,
      "step": 792
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.02138064242899418,
      "learning_rate": 0.016699958385351644,
      "loss": 0.5034,
      "step": 793
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.02423279918730259,
      "learning_rate": 0.016695796920516023,
      "loss": 0.1621,
      "step": 794
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.014763862825930119,
      "learning_rate": 0.0166916354556804,
      "loss": 0.127,
      "step": 795
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.021140828728675842,
      "learning_rate": 0.016687473990844776,
      "loss": 0.4648,
      "step": 796
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.016006823629140854,
      "learning_rate": 0.016683312526009155,
      "loss": 0.2067,
      "step": 797
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.01839813031256199,
      "learning_rate": 0.016679151061173535,
      "loss": 0.3167,
      "step": 798
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.017348794266581535,
      "learning_rate": 0.01667498959633791,
      "loss": 0.1687,
      "step": 799
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.0005906976875849068,
      "learning_rate": 0.01667082813150229,
      "loss": 0.0008,
      "step": 800
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.06840900331735611,
      "learning_rate": 0.016666666666666666,
      "loss": 0.4583,
      "step": 801
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.013588778674602509,
      "learning_rate": 0.016662505201831043,
      "loss": 0.4026,
      "step": 802
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.008593741804361343,
      "learning_rate": 0.016658343736995422,
      "loss": 0.0325,
      "step": 803
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.002787481527775526,
      "learning_rate": 0.0166541822721598,
      "loss": 0.0017,
      "step": 804
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.022920945659279823,
      "learning_rate": 0.016650020807324178,
      "loss": 0.4495,
      "step": 805
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.036856673657894135,
      "learning_rate": 0.016645859342488557,
      "loss": 1.2549,
      "step": 806
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.012767734006047249,
      "learning_rate": 0.016641697877652933,
      "loss": 0.3547,
      "step": 807
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.01914498209953308,
      "learning_rate": 0.01663753641281731,
      "loss": 0.2715,
      "step": 808
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.015170822851359844,
      "learning_rate": 0.01663337494798169,
      "loss": 0.261,
      "step": 809
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.020282737910747528,
      "learning_rate": 0.01662921348314607,
      "loss": 0.181,
      "step": 810
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.01731625199317932,
      "learning_rate": 0.016625052018310445,
      "loss": 0.2266,
      "step": 811
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.016341226175427437,
      "learning_rate": 0.016620890553474824,
      "loss": 0.1481,
      "step": 812
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.015706125646829605,
      "learning_rate": 0.0166167290886392,
      "loss": 0.3499,
      "step": 813
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.024310791864991188,
      "learning_rate": 0.01661256762380358,
      "loss": 0.2399,
      "step": 814
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.014865083619952202,
      "learning_rate": 0.016608406158967956,
      "loss": 0.2145,
      "step": 815
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.01781914383172989,
      "learning_rate": 0.016604244694132336,
      "loss": 0.3718,
      "step": 816
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.014720149338245392,
      "learning_rate": 0.01660008322929671,
      "loss": 0.3037,
      "step": 817
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.013470636680722237,
      "learning_rate": 0.01659592176446109,
      "loss": 0.0586,
      "step": 818
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.018091998994350433,
      "learning_rate": 0.016591760299625467,
      "loss": 0.3838,
      "step": 819
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.013013910502195358,
      "learning_rate": 0.016587598834789847,
      "loss": 0.1472,
      "step": 820
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.014777330681681633,
      "learning_rate": 0.016583437369954223,
      "loss": 0.1665,
      "step": 821
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.023757070302963257,
      "learning_rate": 0.016579275905118603,
      "loss": 0.6221,
      "step": 822
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.008868034929037094,
      "learning_rate": 0.01657511444028298,
      "loss": 0.0116,
      "step": 823
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.009676007553935051,
      "learning_rate": 0.016570952975447358,
      "loss": 0.0316,
      "step": 824
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.023485155776143074,
      "learning_rate": 0.016566791510611738,
      "loss": 0.3381,
      "step": 825
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.011961746960878372,
      "learning_rate": 0.016562630045776114,
      "loss": 0.039,
      "step": 826
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.07419464737176895,
      "learning_rate": 0.01655846858094049,
      "loss": 0.2639,
      "step": 827
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.026102904230356216,
      "learning_rate": 0.01655430711610487,
      "loss": 0.1685,
      "step": 828
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.01779910735785961,
      "learning_rate": 0.016550145651269246,
      "loss": 0.1917,
      "step": 829
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.01312586572021246,
      "learning_rate": 0.016545984186433625,
      "loss": 0.1495,
      "step": 830
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.023067975416779518,
      "learning_rate": 0.016541822721598005,
      "loss": 0.481,
      "step": 831
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.01735624298453331,
      "learning_rate": 0.01653766125676238,
      "loss": 0.235,
      "step": 832
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.022691868245601654,
      "learning_rate": 0.016533499791926757,
      "loss": 0.3594,
      "step": 833
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.02032083086669445,
      "learning_rate": 0.016529338327091136,
      "loss": 0.0811,
      "step": 834
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.011677929200232029,
      "learning_rate": 0.016525176862255513,
      "loss": 0.0311,
      "step": 835
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.0012912238016724586,
      "learning_rate": 0.016521015397419892,
      "loss": 0.0021,
      "step": 836
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.009877496398985386,
      "learning_rate": 0.01651685393258427,
      "loss": 0.0571,
      "step": 837
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.022816717624664307,
      "learning_rate": 0.016512692467748648,
      "loss": 0.2988,
      "step": 838
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.020957378670573235,
      "learning_rate": 0.016508531002913024,
      "loss": 0.3513,
      "step": 839
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.01921050064265728,
      "learning_rate": 0.016504369538077403,
      "loss": 0.3225,
      "step": 840
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.015538414008915424,
      "learning_rate": 0.016500208073241783,
      "loss": 0.2856,
      "step": 841
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.005771830677986145,
      "learning_rate": 0.01649604660840616,
      "loss": 0.0359,
      "step": 842
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.019735774025321007,
      "learning_rate": 0.01649188514357054,
      "loss": 0.2627,
      "step": 843
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.01537058874964714,
      "learning_rate": 0.016487723678734915,
      "loss": 0.1748,
      "step": 844
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.017002243548631668,
      "learning_rate": 0.01648356221389929,
      "loss": 0.208,
      "step": 845
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.011843948625028133,
      "learning_rate": 0.01647940074906367,
      "loss": 0.0623,
      "step": 846
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.013152760453522205,
      "learning_rate": 0.01647523928422805,
      "loss": 0.0202,
      "step": 847
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.01586482860147953,
      "learning_rate": 0.016471077819392426,
      "loss": 0.2474,
      "step": 848
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.021015800535678864,
      "learning_rate": 0.016466916354556806,
      "loss": 0.3752,
      "step": 849
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.01488250121474266,
      "learning_rate": 0.01646275488972118,
      "loss": 0.2264,
      "step": 850
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.01454408373683691,
      "learning_rate": 0.016458593424885558,
      "loss": 0.1473,
      "step": 851
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.021884668618440628,
      "learning_rate": 0.016454431960049937,
      "loss": 0.1755,
      "step": 852
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.024333517998456955,
      "learning_rate": 0.016450270495214317,
      "loss": 0.2444,
      "step": 853
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.025943482294678688,
      "learning_rate": 0.016446109030378693,
      "loss": 0.2568,
      "step": 854
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.02077079750597477,
      "learning_rate": 0.016441947565543073,
      "loss": 0.2686,
      "step": 855
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.009182083420455456,
      "learning_rate": 0.01643778610070745,
      "loss": 0.0546,
      "step": 856
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.027752196416258812,
      "learning_rate": 0.016433624635871828,
      "loss": 0.5513,
      "step": 857
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.028133325278759003,
      "learning_rate": 0.016429463171036204,
      "loss": 0.3528,
      "step": 858
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.01251843012869358,
      "learning_rate": 0.016425301706200584,
      "loss": 0.0686,
      "step": 859
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.020108910277485847,
      "learning_rate": 0.01642114024136496,
      "loss": 0.1724,
      "step": 860
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.02237214334309101,
      "learning_rate": 0.01641697877652934,
      "loss": 0.3689,
      "step": 861
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.020316746085882187,
      "learning_rate": 0.016412817311693716,
      "loss": 0.4507,
      "step": 862
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.0023943581618368626,
      "learning_rate": 0.016408655846858095,
      "loss": 0.0014,
      "step": 863
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.015778040513396263,
      "learning_rate": 0.01640449438202247,
      "loss": 0.1205,
      "step": 864
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.025118518620729446,
      "learning_rate": 0.01640033291718685,
      "loss": 0.4604,
      "step": 865
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.03616708144545555,
      "learning_rate": 0.016396171452351227,
      "loss": 0.2974,
      "step": 866
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.019496910274028778,
      "learning_rate": 0.016392009987515607,
      "loss": 0.3013,
      "step": 867
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.032156262546777725,
      "learning_rate": 0.016387848522679986,
      "loss": 0.4255,
      "step": 868
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.020332850515842438,
      "learning_rate": 0.016383687057844362,
      "loss": 0.1069,
      "step": 869
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.017447790130972862,
      "learning_rate": 0.01637952559300874,
      "loss": 0.3247,
      "step": 870
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.022485749796032906,
      "learning_rate": 0.016375364128173118,
      "loss": 0.335,
      "step": 871
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.016633182764053345,
      "learning_rate": 0.016371202663337494,
      "loss": 0.4094,
      "step": 872
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.022176867350935936,
      "learning_rate": 0.016367041198501874,
      "loss": 0.4968,
      "step": 873
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.023500703275203705,
      "learning_rate": 0.016362879733666253,
      "loss": 0.166,
      "step": 874
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.018345315009355545,
      "learning_rate": 0.01635871826883063,
      "loss": 0.134,
      "step": 875
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.020953403785824776,
      "learning_rate": 0.016354556803995005,
      "loss": 0.2262,
      "step": 876
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.019372910261154175,
      "learning_rate": 0.016350395339159385,
      "loss": 0.3599,
      "step": 877
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.01696900837123394,
      "learning_rate": 0.01634623387432376,
      "loss": 0.0999,
      "step": 878
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.02973988838493824,
      "learning_rate": 0.01634207240948814,
      "loss": 0.0488,
      "step": 879
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.01587802916765213,
      "learning_rate": 0.01633791094465252,
      "loss": 0.1545,
      "step": 880
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.013657576404511929,
      "learning_rate": 0.016333749479816896,
      "loss": 0.0683,
      "step": 881
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.01823948137462139,
      "learning_rate": 0.016329588014981272,
      "loss": 0.1791,
      "step": 882
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.020346296951174736,
      "learning_rate": 0.016325426550145652,
      "loss": 0.4141,
      "step": 883
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.011030556634068489,
      "learning_rate": 0.016321265085310028,
      "loss": 0.0443,
      "step": 884
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.019400158897042274,
      "learning_rate": 0.016317103620474407,
      "loss": 0.3291,
      "step": 885
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.019549163058400154,
      "learning_rate": 0.016312942155638787,
      "loss": 0.1863,
      "step": 886
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.018476836383342743,
      "learning_rate": 0.016308780690803163,
      "loss": 0.4438,
      "step": 887
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.025145960971713066,
      "learning_rate": 0.01630461922596754,
      "loss": 0.3721,
      "step": 888
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.01449675764888525,
      "learning_rate": 0.01630045776113192,
      "loss": 0.0373,
      "step": 889
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.025906091555953026,
      "learning_rate": 0.016296296296296295,
      "loss": 0.3435,
      "step": 890
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.02867051772773266,
      "learning_rate": 0.016292134831460674,
      "loss": 0.4929,
      "step": 891
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.004738725256174803,
      "learning_rate": 0.016287973366625054,
      "loss": 0.0032,
      "step": 892
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.017011573538184166,
      "learning_rate": 0.01628381190178943,
      "loss": 0.0698,
      "step": 893
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.02346462942659855,
      "learning_rate": 0.016279650436953806,
      "loss": 0.3755,
      "step": 894
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.01730034500360489,
      "learning_rate": 0.016275488972118186,
      "loss": 0.191,
      "step": 895
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.011661549098789692,
      "learning_rate": 0.016271327507282562,
      "loss": 0.0641,
      "step": 896
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.02210068143904209,
      "learning_rate": 0.01626716604244694,
      "loss": 0.2524,
      "step": 897
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.01413202378898859,
      "learning_rate": 0.01626300457761132,
      "loss": 0.1211,
      "step": 898
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.0009288507280871272,
      "learning_rate": 0.016258843112775697,
      "loss": 0.0004,
      "step": 899
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.7877373695373535,
      "learning_rate": 0.016254681647940077,
      "loss": 0.2069,
      "step": 900
    },
    {
      "epoch": 1.12,
      "eval_loss": 0.276611328125,
      "eval_runtime": 183.0948,
      "eval_samples_per_second": 1.098,
      "eval_steps_per_second": 0.552,
      "step": 900
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 4806,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 300,
  "total_flos": 1.0355935195378483e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
