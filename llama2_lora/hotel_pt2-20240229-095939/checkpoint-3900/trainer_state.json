{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.868913857677903,
  "eval_steps": 300,
  "global_step": 3900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0,
      "grad_norm": 0.013946587219834328,
      "learning_rate": 0.01999583853516438,
      "loss": 1.1357,
      "step": 1
    },
    {
      "epoch": 0.0,
      "grad_norm": 0.03152783587574959,
      "learning_rate": 0.019991677070328756,
      "loss": 1.3057,
      "step": 2
    },
    {
      "epoch": 0.0,
      "grad_norm": 0.02430560812354088,
      "learning_rate": 0.019987515605493136,
      "loss": 1.5762,
      "step": 3
    },
    {
      "epoch": 0.0,
      "grad_norm": 0.03687096759676933,
      "learning_rate": 0.01998335414065751,
      "loss": 0.9321,
      "step": 4
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.026205476373434067,
      "learning_rate": 0.019979192675821888,
      "loss": 0.9751,
      "step": 5
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.04537992179393768,
      "learning_rate": 0.019975031210986267,
      "loss": 0.7012,
      "step": 6
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.02319790981709957,
      "learning_rate": 0.019970869746150647,
      "loss": 1.1514,
      "step": 7
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.027198510244488716,
      "learning_rate": 0.019966708281315023,
      "loss": 1.1768,
      "step": 8
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.026688028126955032,
      "learning_rate": 0.019962546816479403,
      "loss": 1.0107,
      "step": 9
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.027742978185415268,
      "learning_rate": 0.01995838535164378,
      "loss": 0.52,
      "step": 10
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.017290690913796425,
      "learning_rate": 0.019954223886808155,
      "loss": 0.5259,
      "step": 11
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.024143831804394722,
      "learning_rate": 0.019950062421972534,
      "loss": 0.895,
      "step": 12
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.03602828457951546,
      "learning_rate": 0.019945900957136914,
      "loss": 1.042,
      "step": 13
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.03868493810296059,
      "learning_rate": 0.01994173949230129,
      "loss": 0.9253,
      "step": 14
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.019538432359695435,
      "learning_rate": 0.01993757802746567,
      "loss": 0.8286,
      "step": 15
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.028750460594892502,
      "learning_rate": 0.019933416562630046,
      "loss": 0.5039,
      "step": 16
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.022473318502306938,
      "learning_rate": 0.019929255097794422,
      "loss": 0.8252,
      "step": 17
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.01876644417643547,
      "learning_rate": 0.0199250936329588,
      "loss": 0.5903,
      "step": 18
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.029065033420920372,
      "learning_rate": 0.01992093216812318,
      "loss": 0.6187,
      "step": 19
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.014426208101212978,
      "learning_rate": 0.019916770703287557,
      "loss": 0.2732,
      "step": 20
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.01835629716515541,
      "learning_rate": 0.019912609238451937,
      "loss": 0.5063,
      "step": 21
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.021680880337953568,
      "learning_rate": 0.019908447773616313,
      "loss": 0.5586,
      "step": 22
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.018381770700216293,
      "learning_rate": 0.01990428630878069,
      "loss": 0.4036,
      "step": 23
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.0139843188226223,
      "learning_rate": 0.01990012484394507,
      "loss": 0.2484,
      "step": 24
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.022173702716827393,
      "learning_rate": 0.019895963379109448,
      "loss": 0.7319,
      "step": 25
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.019779937341809273,
      "learning_rate": 0.019891801914273824,
      "loss": 0.25,
      "step": 26
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.015408563427627087,
      "learning_rate": 0.019887640449438203,
      "loss": 0.6279,
      "step": 27
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.017980150878429413,
      "learning_rate": 0.01988347898460258,
      "loss": 0.7153,
      "step": 28
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.01553407683968544,
      "learning_rate": 0.019879317519766956,
      "loss": 0.6553,
      "step": 29
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.017622757703065872,
      "learning_rate": 0.019875156054931335,
      "loss": 0.54,
      "step": 30
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.019046414643526077,
      "learning_rate": 0.019870994590095715,
      "loss": 0.7466,
      "step": 31
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.018101511523127556,
      "learning_rate": 0.01986683312526009,
      "loss": 0.6934,
      "step": 32
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.01025469321757555,
      "learning_rate": 0.01986267166042447,
      "loss": 0.5674,
      "step": 33
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.02104567363858223,
      "learning_rate": 0.019858510195588847,
      "loss": 0.8809,
      "step": 34
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.011890831403434277,
      "learning_rate": 0.019854348730753226,
      "loss": 0.2898,
      "step": 35
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.022102177143096924,
      "learning_rate": 0.019850187265917602,
      "loss": 0.48,
      "step": 36
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.014674047939479351,
      "learning_rate": 0.019846025801081982,
      "loss": 0.3022,
      "step": 37
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.019139140844345093,
      "learning_rate": 0.01984186433624636,
      "loss": 0.3066,
      "step": 38
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.016503287479281425,
      "learning_rate": 0.019837702871410737,
      "loss": 0.5918,
      "step": 39
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.019555386155843735,
      "learning_rate": 0.019833541406575114,
      "loss": 0.4966,
      "step": 40
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.015926335006952286,
      "learning_rate": 0.019829379941739493,
      "loss": 0.4143,
      "step": 41
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.016720635816454887,
      "learning_rate": 0.01982521847690387,
      "loss": 0.6763,
      "step": 42
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.010436618700623512,
      "learning_rate": 0.01982105701206825,
      "loss": 0.7246,
      "step": 43
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.016686901450157166,
      "learning_rate": 0.01981689554723263,
      "loss": 0.321,
      "step": 44
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.013563442043960094,
      "learning_rate": 0.019812734082397004,
      "loss": 0.5669,
      "step": 45
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.01583418808877468,
      "learning_rate": 0.019808572617561384,
      "loss": 0.4619,
      "step": 46
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.012616423889994621,
      "learning_rate": 0.01980441115272576,
      "loss": 0.6016,
      "step": 47
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.019653743132948875,
      "learning_rate": 0.019800249687890136,
      "loss": 0.262,
      "step": 48
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.01714310795068741,
      "learning_rate": 0.019796088223054516,
      "loss": 0.4636,
      "step": 49
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.012529288418591022,
      "learning_rate": 0.019791926758218895,
      "loss": 0.0888,
      "step": 50
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.025462329387664795,
      "learning_rate": 0.01978776529338327,
      "loss": 1.1182,
      "step": 51
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.013185207732021809,
      "learning_rate": 0.01978360382854765,
      "loss": 0.6724,
      "step": 52
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.025705890730023384,
      "learning_rate": 0.019779442363712027,
      "loss": 1.2998,
      "step": 53
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.018231689929962158,
      "learning_rate": 0.019775280898876403,
      "loss": 0.4458,
      "step": 54
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.013859216123819351,
      "learning_rate": 0.019771119434040783,
      "loss": 0.623,
      "step": 55
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.020070232450962067,
      "learning_rate": 0.019766957969205162,
      "loss": 0.73,
      "step": 56
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.012890408746898174,
      "learning_rate": 0.01976279650436954,
      "loss": 0.4128,
      "step": 57
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.014063001610338688,
      "learning_rate": 0.019758635039533918,
      "loss": 0.4192,
      "step": 58
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.016345465555787086,
      "learning_rate": 0.019754473574698294,
      "loss": 0.5889,
      "step": 59
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.0205059964209795,
      "learning_rate": 0.01975031210986267,
      "loss": 0.4709,
      "step": 60
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.014671096578240395,
      "learning_rate": 0.01974615064502705,
      "loss": 0.3325,
      "step": 61
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.011892193928360939,
      "learning_rate": 0.01974198918019143,
      "loss": 0.4302,
      "step": 62
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.015515914186835289,
      "learning_rate": 0.019737827715355805,
      "loss": 0.9307,
      "step": 63
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.01472906768321991,
      "learning_rate": 0.019733666250520185,
      "loss": 0.3135,
      "step": 64
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.013207866810262203,
      "learning_rate": 0.01972950478568456,
      "loss": 0.4983,
      "step": 65
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.006755692884325981,
      "learning_rate": 0.019725343320848937,
      "loss": 0.4126,
      "step": 66
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.013154924847185612,
      "learning_rate": 0.019721181856013317,
      "loss": 0.5415,
      "step": 67
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.012332231737673283,
      "learning_rate": 0.019717020391177696,
      "loss": 0.5249,
      "step": 68
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.01766934059560299,
      "learning_rate": 0.019712858926342072,
      "loss": 0.4358,
      "step": 69
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.018197795376181602,
      "learning_rate": 0.019708697461506452,
      "loss": 0.6753,
      "step": 70
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.016164090484380722,
      "learning_rate": 0.019704535996670828,
      "loss": 0.4712,
      "step": 71
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.029932130128145218,
      "learning_rate": 0.019700374531835204,
      "loss": 0.5342,
      "step": 72
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.01345884520560503,
      "learning_rate": 0.019696213066999584,
      "loss": 0.5029,
      "step": 73
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.01129131205379963,
      "learning_rate": 0.019692051602163963,
      "loss": 0.5376,
      "step": 74
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.016407852992415428,
      "learning_rate": 0.01968789013732834,
      "loss": 0.791,
      "step": 75
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.019868925213813782,
      "learning_rate": 0.01968372867249272,
      "loss": 0.5015,
      "step": 76
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.0146525539457798,
      "learning_rate": 0.019679567207657095,
      "loss": 0.541,
      "step": 77
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.010244067758321762,
      "learning_rate": 0.019675405742821474,
      "loss": 0.2642,
      "step": 78
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.015067693777382374,
      "learning_rate": 0.01967124427798585,
      "loss": 0.2817,
      "step": 79
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.010389878414571285,
      "learning_rate": 0.01966708281315023,
      "loss": 0.6113,
      "step": 80
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.011934892274439335,
      "learning_rate": 0.019662921348314606,
      "loss": 0.0456,
      "step": 81
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.01124374195933342,
      "learning_rate": 0.019658759883478986,
      "loss": 0.2357,
      "step": 82
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.017671743407845497,
      "learning_rate": 0.019654598418643362,
      "loss": 0.5024,
      "step": 83
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.02609691210091114,
      "learning_rate": 0.01965043695380774,
      "loss": 0.5361,
      "step": 84
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.01812068186700344,
      "learning_rate": 0.019646275488972118,
      "loss": 0.4084,
      "step": 85
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.014044742099940777,
      "learning_rate": 0.019642114024136497,
      "loss": 0.2783,
      "step": 86
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.013514760881662369,
      "learning_rate": 0.019637952559300873,
      "loss": 0.5366,
      "step": 87
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.01608353666961193,
      "learning_rate": 0.019633791094465253,
      "loss": 0.4722,
      "step": 88
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.014681047759950161,
      "learning_rate": 0.019629629629629632,
      "loss": 0.3625,
      "step": 89
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.011236661113798618,
      "learning_rate": 0.01962546816479401,
      "loss": 0.2866,
      "step": 90
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.014246581122279167,
      "learning_rate": 0.019621306699958384,
      "loss": 0.4597,
      "step": 91
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.015461236238479614,
      "learning_rate": 0.019617145235122764,
      "loss": 0.251,
      "step": 92
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.013209463097155094,
      "learning_rate": 0.01961298377028714,
      "loss": 0.4397,
      "step": 93
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.017247267067432404,
      "learning_rate": 0.01960882230545152,
      "loss": 0.1571,
      "step": 94
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.0060082110576331615,
      "learning_rate": 0.0196046608406159,
      "loss": 0.0955,
      "step": 95
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.04800603538751602,
      "learning_rate": 0.019600499375780275,
      "loss": 0.5498,
      "step": 96
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.010818454436957836,
      "learning_rate": 0.01959633791094465,
      "loss": 0.3552,
      "step": 97
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.013875529170036316,
      "learning_rate": 0.01959217644610903,
      "loss": 0.2754,
      "step": 98
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.009726175107061863,
      "learning_rate": 0.019588014981273407,
      "loss": 0.5693,
      "step": 99
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.010561893694102764,
      "learning_rate": 0.019583853516437787,
      "loss": 0.269,
      "step": 100
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.018042325973510742,
      "learning_rate": 0.019579692051602166,
      "loss": 0.2888,
      "step": 101
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.011434352956712246,
      "learning_rate": 0.019575530586766542,
      "loss": 0.3032,
      "step": 102
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.01294125895947218,
      "learning_rate": 0.01957136912193092,
      "loss": 0.3904,
      "step": 103
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.00973493605852127,
      "learning_rate": 0.019567207657095298,
      "loss": 0.1273,
      "step": 104
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.011769156903028488,
      "learning_rate": 0.019563046192259674,
      "loss": 0.3728,
      "step": 105
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.013946526683866978,
      "learning_rate": 0.019558884727424054,
      "loss": 0.4209,
      "step": 106
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.024769112467765808,
      "learning_rate": 0.019554723262588433,
      "loss": 0.5957,
      "step": 107
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.013953709043562412,
      "learning_rate": 0.01955056179775281,
      "loss": 0.896,
      "step": 108
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.016479069367051125,
      "learning_rate": 0.019546400332917185,
      "loss": 0.2915,
      "step": 109
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.012262118980288506,
      "learning_rate": 0.019542238868081565,
      "loss": 0.3962,
      "step": 110
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.009211440570652485,
      "learning_rate": 0.019538077403245944,
      "loss": 0.2974,
      "step": 111
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.01734967529773712,
      "learning_rate": 0.01953391593841032,
      "loss": 0.583,
      "step": 112
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.02681916393339634,
      "learning_rate": 0.0195297544735747,
      "loss": 0.5518,
      "step": 113
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.019412348046898842,
      "learning_rate": 0.019525593008739076,
      "loss": 0.1301,
      "step": 114
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.0728633925318718,
      "learning_rate": 0.019521431543903452,
      "loss": 0.3469,
      "step": 115
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.010789559222757816,
      "learning_rate": 0.019517270079067832,
      "loss": 0.2566,
      "step": 116
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.01410316675901413,
      "learning_rate": 0.01951310861423221,
      "loss": 0.6997,
      "step": 117
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.02386266365647316,
      "learning_rate": 0.019508947149396588,
      "loss": 0.7905,
      "step": 118
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.01500307023525238,
      "learning_rate": 0.019504785684560967,
      "loss": 0.311,
      "step": 119
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.01638227328658104,
      "learning_rate": 0.019500624219725343,
      "loss": 0.377,
      "step": 120
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.018486998975276947,
      "learning_rate": 0.019496462754889723,
      "loss": 0.4883,
      "step": 121
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.004287502728402615,
      "learning_rate": 0.0194923012900541,
      "loss": 0.0809,
      "step": 122
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.012090643867850304,
      "learning_rate": 0.01948813982521848,
      "loss": 0.3147,
      "step": 123
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.012809454463422298,
      "learning_rate": 0.019483978360382855,
      "loss": 0.2346,
      "step": 124
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.03159857168793678,
      "learning_rate": 0.019479816895547234,
      "loss": 0.0633,
      "step": 125
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.03378335013985634,
      "learning_rate": 0.01947565543071161,
      "loss": 0.4414,
      "step": 126
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.00972415879368782,
      "learning_rate": 0.01947149396587599,
      "loss": 0.1332,
      "step": 127
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.014316284097731113,
      "learning_rate": 0.019467332501040366,
      "loss": 0.2642,
      "step": 128
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.01945355348289013,
      "learning_rate": 0.019463171036204745,
      "loss": 0.2568,
      "step": 129
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.014685138128697872,
      "learning_rate": 0.01945900957136912,
      "loss": 0.2495,
      "step": 130
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.010893230326473713,
      "learning_rate": 0.0194548481065335,
      "loss": 0.1844,
      "step": 131
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.015432030893862247,
      "learning_rate": 0.01945068664169788,
      "loss": 0.2192,
      "step": 132
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.023381555452942848,
      "learning_rate": 0.019446525176862257,
      "loss": 0.1019,
      "step": 133
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.025069458410143852,
      "learning_rate": 0.019442363712026633,
      "loss": 0.6982,
      "step": 134
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.013284072279930115,
      "learning_rate": 0.019438202247191012,
      "loss": 0.2725,
      "step": 135
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.01459498330950737,
      "learning_rate": 0.01943404078235539,
      "loss": 0.342,
      "step": 136
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.010649852454662323,
      "learning_rate": 0.019429879317519768,
      "loss": 0.1597,
      "step": 137
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.00572684733197093,
      "learning_rate": 0.019425717852684148,
      "loss": 0.0163,
      "step": 138
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.010500311851501465,
      "learning_rate": 0.019421556387848524,
      "loss": 0.3718,
      "step": 139
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.02403194084763527,
      "learning_rate": 0.0194173949230129,
      "loss": 0.3464,
      "step": 140
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.015808913856744766,
      "learning_rate": 0.01941323345817728,
      "loss": 0.0919,
      "step": 141
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.017955824732780457,
      "learning_rate": 0.019409071993341655,
      "loss": 0.3323,
      "step": 142
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.015312157571315765,
      "learning_rate": 0.019404910528506035,
      "loss": 0.4712,
      "step": 143
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.013958172872662544,
      "learning_rate": 0.019400749063670415,
      "loss": 0.1681,
      "step": 144
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.01720457337796688,
      "learning_rate": 0.01939658759883479,
      "loss": 0.541,
      "step": 145
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.017649512737989426,
      "learning_rate": 0.019392426133999167,
      "loss": 0.6196,
      "step": 146
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.016069641336798668,
      "learning_rate": 0.019388264669163546,
      "loss": 0.3416,
      "step": 147
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.015727760270237923,
      "learning_rate": 0.019384103204327922,
      "loss": 0.2766,
      "step": 148
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.011026602238416672,
      "learning_rate": 0.019379941739492302,
      "loss": 0.2603,
      "step": 149
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.011536615900695324,
      "learning_rate": 0.01937578027465668,
      "loss": 0.0615,
      "step": 150
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.018519138917326927,
      "learning_rate": 0.019371618809821058,
      "loss": 0.2013,
      "step": 151
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.019297972321510315,
      "learning_rate": 0.019367457344985434,
      "loss": 0.7739,
      "step": 152
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.035224251449108124,
      "learning_rate": 0.019363295880149813,
      "loss": 0.1853,
      "step": 153
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.011118430644273758,
      "learning_rate": 0.01935913441531419,
      "loss": 0.585,
      "step": 154
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.01876828819513321,
      "learning_rate": 0.01935497295047857,
      "loss": 0.439,
      "step": 155
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.013631806708872318,
      "learning_rate": 0.01935081148564295,
      "loss": 0.2147,
      "step": 156
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.013377000577747822,
      "learning_rate": 0.019346650020807325,
      "loss": 0.0768,
      "step": 157
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.009209630079567432,
      "learning_rate": 0.0193424885559717,
      "loss": 0.12,
      "step": 158
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.010687937960028648,
      "learning_rate": 0.01933832709113608,
      "loss": 0.3562,
      "step": 159
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.01562159787863493,
      "learning_rate": 0.019334165626300456,
      "loss": 0.2192,
      "step": 160
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.005969279445707798,
      "learning_rate": 0.019330004161464836,
      "loss": 0.0499,
      "step": 161
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.01860101707279682,
      "learning_rate": 0.019325842696629215,
      "loss": 0.4082,
      "step": 162
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.013926600106060505,
      "learning_rate": 0.01932168123179359,
      "loss": 0.27,
      "step": 163
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.007220026105642319,
      "learning_rate": 0.01931751976695797,
      "loss": 0.139,
      "step": 164
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.016826435923576355,
      "learning_rate": 0.019313358302122347,
      "loss": 0.4153,
      "step": 165
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.008984547108411789,
      "learning_rate": 0.019309196837286723,
      "loss": 0.2205,
      "step": 166
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.014899051748216152,
      "learning_rate": 0.019305035372451103,
      "loss": 0.4397,
      "step": 167
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.008856400847434998,
      "learning_rate": 0.019300873907615482,
      "loss": 0.0795,
      "step": 168
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.015470389276742935,
      "learning_rate": 0.01929671244277986,
      "loss": 0.6055,
      "step": 169
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.01177644170820713,
      "learning_rate": 0.019292550977944238,
      "loss": 0.3594,
      "step": 170
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.018051499500870705,
      "learning_rate": 0.019288389513108614,
      "loss": 0.7207,
      "step": 171
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.01194706466048956,
      "learning_rate": 0.01928422804827299,
      "loss": 0.4624,
      "step": 172
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.021377161145210266,
      "learning_rate": 0.01928006658343737,
      "loss": 0.4302,
      "step": 173
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.011635289527475834,
      "learning_rate": 0.01927590511860175,
      "loss": 0.2068,
      "step": 174
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.012977547012269497,
      "learning_rate": 0.019271743653766125,
      "loss": 0.3076,
      "step": 175
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.011483551934361458,
      "learning_rate": 0.019267582188930505,
      "loss": 0.2173,
      "step": 176
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.018402379006147385,
      "learning_rate": 0.01926342072409488,
      "loss": 0.3831,
      "step": 177
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.008669276721775532,
      "learning_rate": 0.019259259259259257,
      "loss": 0.1942,
      "step": 178
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.009989948943257332,
      "learning_rate": 0.019255097794423637,
      "loss": 0.4062,
      "step": 179
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.007285960018634796,
      "learning_rate": 0.019250936329588016,
      "loss": 0.0542,
      "step": 180
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.022931115701794624,
      "learning_rate": 0.019246774864752392,
      "loss": 0.3311,
      "step": 181
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.015280869789421558,
      "learning_rate": 0.019242613399916772,
      "loss": 0.1851,
      "step": 182
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.016446208581328392,
      "learning_rate": 0.019238451935081148,
      "loss": 0.3059,
      "step": 183
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.009221578016877174,
      "learning_rate": 0.019234290470245528,
      "loss": 0.1036,
      "step": 184
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.01323636807501316,
      "learning_rate": 0.019230129005409904,
      "loss": 0.3828,
      "step": 185
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.011723767034709454,
      "learning_rate": 0.019225967540574283,
      "loss": 0.6211,
      "step": 186
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.00718157272785902,
      "learning_rate": 0.019221806075738663,
      "loss": 0.1534,
      "step": 187
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.009665646590292454,
      "learning_rate": 0.01921764461090304,
      "loss": 0.1459,
      "step": 188
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.01421988382935524,
      "learning_rate": 0.019213483146067415,
      "loss": 0.3362,
      "step": 189
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.014052601531147957,
      "learning_rate": 0.019209321681231795,
      "loss": 0.4727,
      "step": 190
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.007880319841206074,
      "learning_rate": 0.01920516021639617,
      "loss": 0.0958,
      "step": 191
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.013098879717290401,
      "learning_rate": 0.01920099875156055,
      "loss": 0.2483,
      "step": 192
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.010881153866648674,
      "learning_rate": 0.01919683728672493,
      "loss": 0.0822,
      "step": 193
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.011607570573687553,
      "learning_rate": 0.019192675821889306,
      "loss": 0.4939,
      "step": 194
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.015284847468137741,
      "learning_rate": 0.019188514357053682,
      "loss": 0.4248,
      "step": 195
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.006532551255077124,
      "learning_rate": 0.01918435289221806,
      "loss": 0.0183,
      "step": 196
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.00698343338444829,
      "learning_rate": 0.019180191427382438,
      "loss": 0.0674,
      "step": 197
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.01601043902337551,
      "learning_rate": 0.019176029962546817,
      "loss": 1.0,
      "step": 198
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.018778465688228607,
      "learning_rate": 0.019171868497711197,
      "loss": 0.5615,
      "step": 199
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.017414990812540054,
      "learning_rate": 0.019167707032875573,
      "loss": 0.4424,
      "step": 200
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.009499352425336838,
      "learning_rate": 0.01916354556803995,
      "loss": 0.0865,
      "step": 201
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.011209850199520588,
      "learning_rate": 0.01915938410320433,
      "loss": 0.3103,
      "step": 202
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.011232535354793072,
      "learning_rate": 0.019155222638368705,
      "loss": 0.3169,
      "step": 203
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.016806548461318016,
      "learning_rate": 0.019151061173533084,
      "loss": 0.6245,
      "step": 204
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.01189274899661541,
      "learning_rate": 0.019146899708697464,
      "loss": 0.1663,
      "step": 205
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.015232094563543797,
      "learning_rate": 0.01914273824386184,
      "loss": 0.6851,
      "step": 206
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.02008361555635929,
      "learning_rate": 0.01913857677902622,
      "loss": 0.4065,
      "step": 207
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.007860634475946426,
      "learning_rate": 0.019134415314190596,
      "loss": 0.114,
      "step": 208
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.009345010854303837,
      "learning_rate": 0.01913025384935497,
      "loss": 0.3083,
      "step": 209
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.01482164952903986,
      "learning_rate": 0.01912609238451935,
      "loss": 0.5288,
      "step": 210
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.013445671647787094,
      "learning_rate": 0.01912193091968373,
      "loss": 0.0577,
      "step": 211
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.01909550465643406,
      "learning_rate": 0.019117769454848107,
      "loss": 0.3801,
      "step": 212
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.013162663206458092,
      "learning_rate": 0.019113607990012486,
      "loss": 0.2639,
      "step": 213
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.01083003357052803,
      "learning_rate": 0.019109446525176862,
      "loss": 0.1499,
      "step": 214
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.00959020759910345,
      "learning_rate": 0.01910528506034124,
      "loss": 0.1101,
      "step": 215
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.010005058720707893,
      "learning_rate": 0.019101123595505618,
      "loss": 0.2026,
      "step": 216
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.02039876952767372,
      "learning_rate": 0.019096962130669998,
      "loss": 0.282,
      "step": 217
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.014765151776373386,
      "learning_rate": 0.019092800665834374,
      "loss": 0.3125,
      "step": 218
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.009571024216711521,
      "learning_rate": 0.019088639200998753,
      "loss": 0.183,
      "step": 219
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.010038415901362896,
      "learning_rate": 0.01908447773616313,
      "loss": 0.1028,
      "step": 220
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.012818839401006699,
      "learning_rate": 0.019080316271327506,
      "loss": 0.5225,
      "step": 221
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.014451993629336357,
      "learning_rate": 0.019076154806491885,
      "loss": 0.168,
      "step": 222
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.006471678148955107,
      "learning_rate": 0.019071993341656265,
      "loss": 0.0197,
      "step": 223
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.004482632502913475,
      "learning_rate": 0.01906783187682064,
      "loss": 0.0293,
      "step": 224
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.016593338921666145,
      "learning_rate": 0.01906367041198502,
      "loss": 0.5806,
      "step": 225
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.00906510278582573,
      "learning_rate": 0.019059508947149396,
      "loss": 0.1466,
      "step": 226
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.010454477742314339,
      "learning_rate": 0.019055347482313773,
      "loss": 0.1746,
      "step": 227
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.0206717811524868,
      "learning_rate": 0.019051186017478152,
      "loss": 0.6543,
      "step": 228
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.009957603178918362,
      "learning_rate": 0.01904702455264253,
      "loss": 0.0541,
      "step": 229
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.02599034458398819,
      "learning_rate": 0.019042863087806908,
      "loss": 0.4255,
      "step": 230
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.017418760806322098,
      "learning_rate": 0.019038701622971287,
      "loss": 0.4253,
      "step": 231
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.013599650003015995,
      "learning_rate": 0.019034540158135663,
      "loss": 0.3621,
      "step": 232
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.012645847164094448,
      "learning_rate": 0.01903037869330004,
      "loss": 0.1808,
      "step": 233
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.011502467095851898,
      "learning_rate": 0.01902621722846442,
      "loss": 0.4675,
      "step": 234
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.009859512560069561,
      "learning_rate": 0.0190220557636288,
      "loss": 0.0282,
      "step": 235
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.009849142283201218,
      "learning_rate": 0.019017894298793175,
      "loss": 0.0837,
      "step": 236
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.015944460406899452,
      "learning_rate": 0.019013732833957554,
      "loss": 0.2374,
      "step": 237
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.015828672796487808,
      "learning_rate": 0.01900957136912193,
      "loss": 0.5649,
      "step": 238
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.011765114963054657,
      "learning_rate": 0.01900540990428631,
      "loss": 0.4055,
      "step": 239
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.012066415511071682,
      "learning_rate": 0.019001248439450686,
      "loss": 0.3311,
      "step": 240
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.01208413578569889,
      "learning_rate": 0.018997086974615066,
      "loss": 0.3303,
      "step": 241
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.020172521471977234,
      "learning_rate": 0.01899292550977944,
      "loss": 0.6533,
      "step": 242
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.013071781024336815,
      "learning_rate": 0.01898876404494382,
      "loss": 0.2922,
      "step": 243
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.014257648959755898,
      "learning_rate": 0.018984602580108197,
      "loss": 0.5166,
      "step": 244
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.008450665511190891,
      "learning_rate": 0.018980441115272577,
      "loss": 0.1483,
      "step": 245
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01024819165468216,
      "learning_rate": 0.018976279650436953,
      "loss": 0.3325,
      "step": 246
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.012138486839830875,
      "learning_rate": 0.018972118185601333,
      "loss": 0.3181,
      "step": 247
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01706678234040737,
      "learning_rate": 0.01896795672076571,
      "loss": 0.5381,
      "step": 248
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.0165503341704607,
      "learning_rate": 0.018963795255930088,
      "loss": 0.3474,
      "step": 249
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.017817780375480652,
      "learning_rate": 0.018959633791094468,
      "loss": 0.1743,
      "step": 250
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01351422630250454,
      "learning_rate": 0.018955472326258844,
      "loss": 0.1572,
      "step": 251
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01826195791363716,
      "learning_rate": 0.01895131086142322,
      "loss": 0.3682,
      "step": 252
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.01721639558672905,
      "learning_rate": 0.0189471493965876,
      "loss": 0.4712,
      "step": 253
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.0058861286379396915,
      "learning_rate": 0.018942987931751976,
      "loss": 0.0426,
      "step": 254
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.0133828679099679,
      "learning_rate": 0.018938826466916355,
      "loss": 0.3984,
      "step": 255
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.015871714800596237,
      "learning_rate": 0.018934665002080735,
      "loss": 0.4358,
      "step": 256
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.012308573350310326,
      "learning_rate": 0.01893050353724511,
      "loss": 0.303,
      "step": 257
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.010923982597887516,
      "learning_rate": 0.018926342072409487,
      "loss": 0.1774,
      "step": 258
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.013635121285915375,
      "learning_rate": 0.018922180607573866,
      "loss": 0.4199,
      "step": 259
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.013736642897129059,
      "learning_rate": 0.018918019142738246,
      "loss": 0.3162,
      "step": 260
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.018210401758551598,
      "learning_rate": 0.018913857677902622,
      "loss": 0.3315,
      "step": 261
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.01903851330280304,
      "learning_rate": 0.018909696213067,
      "loss": 0.6294,
      "step": 262
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.01758592203259468,
      "learning_rate": 0.018905534748231378,
      "loss": 0.853,
      "step": 263
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.014722253195941448,
      "learning_rate": 0.018901373283395754,
      "loss": 0.2148,
      "step": 264
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.010228004306554794,
      "learning_rate": 0.018897211818560133,
      "loss": 0.07,
      "step": 265
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.017041178420186043,
      "learning_rate": 0.018893050353724513,
      "loss": 0.3865,
      "step": 266
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.024548158049583435,
      "learning_rate": 0.01888888888888889,
      "loss": 0.408,
      "step": 267
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.021002428606152534,
      "learning_rate": 0.01888472742405327,
      "loss": 0.4521,
      "step": 268
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.023030998185276985,
      "learning_rate": 0.018880565959217645,
      "loss": 0.6274,
      "step": 269
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.01349740382283926,
      "learning_rate": 0.01887640449438202,
      "loss": 0.3054,
      "step": 270
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.01190619170665741,
      "learning_rate": 0.0188722430295464,
      "loss": 0.1816,
      "step": 271
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.01658334955573082,
      "learning_rate": 0.01886808156471078,
      "loss": 0.491,
      "step": 272
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.008173597976565361,
      "learning_rate": 0.018863920099875156,
      "loss": 0.0178,
      "step": 273
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.017877396196126938,
      "learning_rate": 0.018859758635039536,
      "loss": 0.4114,
      "step": 274
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.012737761251628399,
      "learning_rate": 0.01885559717020391,
      "loss": 0.2288,
      "step": 275
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.02273883856832981,
      "learning_rate": 0.018851435705368288,
      "loss": 0.4519,
      "step": 276
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.006959815509617329,
      "learning_rate": 0.018847274240532667,
      "loss": 0.0447,
      "step": 277
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.012611573562026024,
      "learning_rate": 0.018843112775697047,
      "loss": 0.1226,
      "step": 278
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.014003496617078781,
      "learning_rate": 0.018838951310861423,
      "loss": 0.4014,
      "step": 279
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.01877623423933983,
      "learning_rate": 0.018834789846025803,
      "loss": 0.2646,
      "step": 280
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.018713688477873802,
      "learning_rate": 0.01883062838119018,
      "loss": 0.5034,
      "step": 281
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.02422862872481346,
      "learning_rate": 0.018826466916354558,
      "loss": 0.8271,
      "step": 282
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.016888445243239403,
      "learning_rate": 0.018822305451518934,
      "loss": 0.541,
      "step": 283
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.025673290714621544,
      "learning_rate": 0.018818143986683314,
      "loss": 0.5269,
      "step": 284
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.013471826910972595,
      "learning_rate": 0.01881398252184769,
      "loss": 0.1483,
      "step": 285
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.016805335879325867,
      "learning_rate": 0.01880982105701207,
      "loss": 0.4895,
      "step": 286
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.02230897918343544,
      "learning_rate": 0.018805659592176446,
      "loss": 0.2627,
      "step": 287
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.017700405791401863,
      "learning_rate": 0.018801498127340825,
      "loss": 0.2869,
      "step": 288
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.026683760806918144,
      "learning_rate": 0.0187973366625052,
      "loss": 0.4714,
      "step": 289
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.017930233851075172,
      "learning_rate": 0.01879317519766958,
      "loss": 0.124,
      "step": 290
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.014429387636482716,
      "learning_rate": 0.018789013732833957,
      "loss": 0.4141,
      "step": 291
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.012393507175147533,
      "learning_rate": 0.018784852267998337,
      "loss": 0.1812,
      "step": 292
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.013746536336839199,
      "learning_rate": 0.018780690803162716,
      "loss": 0.2296,
      "step": 293
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.017592409625649452,
      "learning_rate": 0.018776529338327092,
      "loss": 0.4626,
      "step": 294
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.02273542992770672,
      "learning_rate": 0.01877236787349147,
      "loss": 0.2491,
      "step": 295
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.04013910889625549,
      "learning_rate": 0.018768206408655848,
      "loss": 0.5811,
      "step": 296
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.012396056205034256,
      "learning_rate": 0.018764044943820224,
      "loss": 0.5254,
      "step": 297
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.01785169169306755,
      "learning_rate": 0.018759883478984603,
      "loss": 0.397,
      "step": 298
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.01874547079205513,
      "learning_rate": 0.018755722014148983,
      "loss": 0.2937,
      "step": 299
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.01092853955924511,
      "learning_rate": 0.01875156054931336,
      "loss": 0.4192,
      "step": 300
    },
    {
      "epoch": 0.37,
      "eval_loss": 0.3251953125,
      "eval_runtime": 183.3307,
      "eval_samples_per_second": 1.096,
      "eval_steps_per_second": 0.551,
      "step": 300
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.012453447096049786,
      "learning_rate": 0.018747399084477735,
      "loss": 0.2705,
      "step": 301
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.020620588213205338,
      "learning_rate": 0.018743237619642115,
      "loss": 0.6685,
      "step": 302
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.018006984144449234,
      "learning_rate": 0.01873907615480649,
      "loss": 0.4458,
      "step": 303
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.012783350422978401,
      "learning_rate": 0.01873491468997087,
      "loss": 0.345,
      "step": 304
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.019229650497436523,
      "learning_rate": 0.01873075322513525,
      "loss": 0.3953,
      "step": 305
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.017787378281354904,
      "learning_rate": 0.018726591760299626,
      "loss": 0.624,
      "step": 306
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.01644415408372879,
      "learning_rate": 0.018722430295464002,
      "loss": 0.2329,
      "step": 307
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.012315182946622372,
      "learning_rate": 0.018718268830628382,
      "loss": 0.1984,
      "step": 308
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.012109145522117615,
      "learning_rate": 0.018714107365792758,
      "loss": 0.2041,
      "step": 309
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.013556061312556267,
      "learning_rate": 0.018709945900957137,
      "loss": 0.4844,
      "step": 310
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.016224991530179977,
      "learning_rate": 0.018705784436121517,
      "loss": 0.5752,
      "step": 311
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.021627897396683693,
      "learning_rate": 0.018701622971285893,
      "loss": 0.2301,
      "step": 312
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.022303931415081024,
      "learning_rate": 0.01869746150645027,
      "loss": 0.4536,
      "step": 313
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.014758111909031868,
      "learning_rate": 0.01869330004161465,
      "loss": 0.3098,
      "step": 314
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.013408167287707329,
      "learning_rate": 0.018689138576779025,
      "loss": 0.0468,
      "step": 315
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.017526132985949516,
      "learning_rate": 0.018684977111943404,
      "loss": 0.1798,
      "step": 316
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.025102650746703148,
      "learning_rate": 0.018680815647107784,
      "loss": 0.5762,
      "step": 317
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.014943995513021946,
      "learning_rate": 0.01867665418227216,
      "loss": 0.4561,
      "step": 318
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.016996843740344048,
      "learning_rate": 0.018672492717436536,
      "loss": 0.2896,
      "step": 319
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.011097901500761509,
      "learning_rate": 0.018668331252600916,
      "loss": 0.1134,
      "step": 320
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.013591892085969448,
      "learning_rate": 0.018664169787765292,
      "loss": 0.1129,
      "step": 321
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.015822574496269226,
      "learning_rate": 0.01866000832292967,
      "loss": 0.2207,
      "step": 322
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.015380003489553928,
      "learning_rate": 0.01865584685809405,
      "loss": 0.16,
      "step": 323
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.008600973524153233,
      "learning_rate": 0.018651685393258427,
      "loss": 0.0347,
      "step": 324
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.014758262783288956,
      "learning_rate": 0.018647523928422807,
      "loss": 0.2202,
      "step": 325
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.016456298530101776,
      "learning_rate": 0.018643362463587183,
      "loss": 0.2969,
      "step": 326
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.014062338508665562,
      "learning_rate": 0.01863920099875156,
      "loss": 0.2715,
      "step": 327
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.014881882816553116,
      "learning_rate": 0.01863503953391594,
      "loss": 0.022,
      "step": 328
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.0236114040017128,
      "learning_rate": 0.018630878069080318,
      "loss": 0.2834,
      "step": 329
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.023272477090358734,
      "learning_rate": 0.018626716604244694,
      "loss": 0.5132,
      "step": 330
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.012915327213704586,
      "learning_rate": 0.018622555139409074,
      "loss": 0.1646,
      "step": 331
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.027405433356761932,
      "learning_rate": 0.01861839367457345,
      "loss": 0.4375,
      "step": 332
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.019116155803203583,
      "learning_rate": 0.01861423220973783,
      "loss": 0.2905,
      "step": 333
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.015979696065187454,
      "learning_rate": 0.018610070744902205,
      "loss": 0.2188,
      "step": 334
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.016301333904266357,
      "learning_rate": 0.018605909280066585,
      "loss": 0.2358,
      "step": 335
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.011888011358678341,
      "learning_rate": 0.018601747815230964,
      "loss": 0.246,
      "step": 336
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.023288020864129066,
      "learning_rate": 0.01859758635039534,
      "loss": 0.541,
      "step": 337
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.005208710674196482,
      "learning_rate": 0.018593424885559717,
      "loss": 0.0097,
      "step": 338
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.011656765826046467,
      "learning_rate": 0.018589263420724096,
      "loss": 0.0943,
      "step": 339
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.026298977434635162,
      "learning_rate": 0.018585101955888472,
      "loss": 1.1572,
      "step": 340
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.015261673368513584,
      "learning_rate": 0.018580940491052852,
      "loss": 0.0914,
      "step": 341
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.018320735543966293,
      "learning_rate": 0.01857677902621723,
      "loss": 0.4023,
      "step": 342
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.020216815173625946,
      "learning_rate": 0.018572617561381607,
      "loss": 0.26,
      "step": 343
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.016558609902858734,
      "learning_rate": 0.018568456096545984,
      "loss": 0.301,
      "step": 344
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.013275112956762314,
      "learning_rate": 0.018564294631710363,
      "loss": 0.1666,
      "step": 345
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.023631185293197632,
      "learning_rate": 0.01856013316687474,
      "loss": 0.365,
      "step": 346
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.023109402507543564,
      "learning_rate": 0.01855597170203912,
      "loss": 0.4705,
      "step": 347
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.024755585938692093,
      "learning_rate": 0.0185518102372035,
      "loss": 0.6177,
      "step": 348
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.038343414664268494,
      "learning_rate": 0.018547648772367874,
      "loss": 0.4514,
      "step": 349
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.026825403794646263,
      "learning_rate": 0.01854348730753225,
      "loss": 0.5698,
      "step": 350
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.01578792929649353,
      "learning_rate": 0.01853932584269663,
      "loss": 0.2073,
      "step": 351
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.010524454526603222,
      "learning_rate": 0.018535164377861006,
      "loss": 0.2559,
      "step": 352
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.010751763358712196,
      "learning_rate": 0.018531002913025386,
      "loss": 0.1526,
      "step": 353
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.015715204179286957,
      "learning_rate": 0.018526841448189765,
      "loss": 0.4263,
      "step": 354
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.01730133220553398,
      "learning_rate": 0.01852267998335414,
      "loss": 0.3289,
      "step": 355
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.016789477318525314,
      "learning_rate": 0.018518518518518517,
      "loss": 0.209,
      "step": 356
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.01814727671444416,
      "learning_rate": 0.018514357053682897,
      "loss": 0.2617,
      "step": 357
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.02244481071829796,
      "learning_rate": 0.018510195588847273,
      "loss": 0.5991,
      "step": 358
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.01240987703204155,
      "learning_rate": 0.018506034124011653,
      "loss": 0.187,
      "step": 359
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.01305888220667839,
      "learning_rate": 0.018501872659176032,
      "loss": 0.1776,
      "step": 360
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.015885083004832268,
      "learning_rate": 0.01849771119434041,
      "loss": 0.2406,
      "step": 361
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.018674463033676147,
      "learning_rate": 0.018493549729504784,
      "loss": 0.4717,
      "step": 362
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.011706807650625706,
      "learning_rate": 0.018489388264669164,
      "loss": 0.1733,
      "step": 363
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.012210343964397907,
      "learning_rate": 0.01848522679983354,
      "loss": 0.0668,
      "step": 364
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.023025935515761375,
      "learning_rate": 0.01848106533499792,
      "loss": 0.4929,
      "step": 365
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.019617287442088127,
      "learning_rate": 0.0184769038701623,
      "loss": 0.3542,
      "step": 366
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.019019676372408867,
      "learning_rate": 0.018472742405326675,
      "loss": 0.1937,
      "step": 367
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.019696107134222984,
      "learning_rate": 0.018468580940491055,
      "loss": 0.5415,
      "step": 368
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.01248240377753973,
      "learning_rate": 0.01846441947565543,
      "loss": 0.3943,
      "step": 369
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.01081449817866087,
      "learning_rate": 0.018460258010819807,
      "loss": 0.1473,
      "step": 370
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.02130720019340515,
      "learning_rate": 0.018456096545984187,
      "loss": 0.2279,
      "step": 371
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.023912722244858742,
      "learning_rate": 0.018451935081148566,
      "loss": 0.623,
      "step": 372
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.013592472299933434,
      "learning_rate": 0.018447773616312942,
      "loss": 0.1094,
      "step": 373
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.01946699246764183,
      "learning_rate": 0.018443612151477322,
      "loss": 0.3538,
      "step": 374
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.01677008904516697,
      "learning_rate": 0.018439450686641698,
      "loss": 0.5879,
      "step": 375
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.014828789979219437,
      "learning_rate": 0.018435289221806074,
      "loss": 0.4316,
      "step": 376
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.020289145410060883,
      "learning_rate": 0.018431127756970454,
      "loss": 0.4058,
      "step": 377
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.02077432908117771,
      "learning_rate": 0.018426966292134833,
      "loss": 0.1469,
      "step": 378
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.010249602608382702,
      "learning_rate": 0.01842280482729921,
      "loss": 0.015,
      "step": 379
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.016949528828263283,
      "learning_rate": 0.01841864336246359,
      "loss": 0.2949,
      "step": 380
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.01481599546968937,
      "learning_rate": 0.018414481897627965,
      "loss": 0.1646,
      "step": 381
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.030077632516622543,
      "learning_rate": 0.01841032043279234,
      "loss": 0.4893,
      "step": 382
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.013509195297956467,
      "learning_rate": 0.01840615896795672,
      "loss": 0.1575,
      "step": 383
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.015224408358335495,
      "learning_rate": 0.0184019975031211,
      "loss": 0.4365,
      "step": 384
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.019919302314519882,
      "learning_rate": 0.018397836038285476,
      "loss": 0.2722,
      "step": 385
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.011082062497735023,
      "learning_rate": 0.018393674573449856,
      "loss": 0.259,
      "step": 386
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.028045201674103737,
      "learning_rate": 0.018389513108614232,
      "loss": 0.2676,
      "step": 387
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.027130700647830963,
      "learning_rate": 0.018385351643778608,
      "loss": 0.3445,
      "step": 388
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.013158266432583332,
      "learning_rate": 0.018381190178942988,
      "loss": 0.1779,
      "step": 389
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.021059006452560425,
      "learning_rate": 0.018377028714107367,
      "loss": 0.0562,
      "step": 390
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.009331466630101204,
      "learning_rate": 0.018372867249271743,
      "loss": 0.1804,
      "step": 391
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.012068395502865314,
      "learning_rate": 0.018368705784436123,
      "loss": 0.2776,
      "step": 392
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.026677824556827545,
      "learning_rate": 0.0183645443196005,
      "loss": 0.3213,
      "step": 393
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.029340950772166252,
      "learning_rate": 0.018360382854764875,
      "loss": 0.3765,
      "step": 394
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.015520433895289898,
      "learning_rate": 0.018356221389929255,
      "loss": 0.2681,
      "step": 395
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.02110999822616577,
      "learning_rate": 0.018352059925093634,
      "loss": 0.2029,
      "step": 396
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.01838153973221779,
      "learning_rate": 0.01834789846025801,
      "loss": 0.0622,
      "step": 397
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.01988046057522297,
      "learning_rate": 0.01834373699542239,
      "loss": 0.3376,
      "step": 398
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.020841067656874657,
      "learning_rate": 0.018339575530586766,
      "loss": 0.4333,
      "step": 399
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.009150462225079536,
      "learning_rate": 0.018335414065751145,
      "loss": 0.0217,
      "step": 400
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.028743427246809006,
      "learning_rate": 0.01833125260091552,
      "loss": 0.0804,
      "step": 401
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.013679815456271172,
      "learning_rate": 0.0183270911360799,
      "loss": 0.1171,
      "step": 402
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.019026050344109535,
      "learning_rate": 0.01832292967124428,
      "loss": 0.2389,
      "step": 403
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.025706058368086815,
      "learning_rate": 0.018318768206408657,
      "loss": 0.6992,
      "step": 404
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.0252694021910429,
      "learning_rate": 0.018314606741573033,
      "loss": 0.4209,
      "step": 405
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.024600857868790627,
      "learning_rate": 0.018310445276737412,
      "loss": 0.6948,
      "step": 406
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.022102979943156242,
      "learning_rate": 0.01830628381190179,
      "loss": 0.3425,
      "step": 407
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.0011589693604037166,
      "learning_rate": 0.018302122347066168,
      "loss": 0.0023,
      "step": 408
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.01646098494529724,
      "learning_rate": 0.018297960882230548,
      "loss": 0.2659,
      "step": 409
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.01534626167267561,
      "learning_rate": 0.018293799417394924,
      "loss": 0.3,
      "step": 410
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.022580618038773537,
      "learning_rate": 0.018289637952559303,
      "loss": 0.4175,
      "step": 411
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.017710313200950623,
      "learning_rate": 0.01828547648772368,
      "loss": 0.4795,
      "step": 412
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.015363357961177826,
      "learning_rate": 0.018281315022888055,
      "loss": 0.1886,
      "step": 413
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.021407626569271088,
      "learning_rate": 0.018277153558052435,
      "loss": 0.1099,
      "step": 414
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.01610243320465088,
      "learning_rate": 0.018272992093216815,
      "loss": 0.4968,
      "step": 415
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.03274524584412575,
      "learning_rate": 0.01826883062838119,
      "loss": 0.7466,
      "step": 416
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.028339523822069168,
      "learning_rate": 0.01826466916354557,
      "loss": 0.7749,
      "step": 417
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.013688563369214535,
      "learning_rate": 0.018260507698709946,
      "loss": 0.0635,
      "step": 418
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.021380651742219925,
      "learning_rate": 0.018256346233874322,
      "loss": 0.4131,
      "step": 419
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.026248306035995483,
      "learning_rate": 0.018252184769038702,
      "loss": 0.2974,
      "step": 420
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.02018003910779953,
      "learning_rate": 0.01824802330420308,
      "loss": 0.2115,
      "step": 421
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.024361971765756607,
      "learning_rate": 0.018243861839367458,
      "loss": 0.6206,
      "step": 422
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.015359626151621342,
      "learning_rate": 0.018239700374531837,
      "loss": 0.1203,
      "step": 423
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.016143817454576492,
      "learning_rate": 0.018235538909696213,
      "loss": 0.2698,
      "step": 424
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.015773026272654533,
      "learning_rate": 0.01823137744486059,
      "loss": 0.1071,
      "step": 425
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.011392384767532349,
      "learning_rate": 0.01822721598002497,
      "loss": 0.1879,
      "step": 426
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.012423066422343254,
      "learning_rate": 0.01822305451518935,
      "loss": 0.0672,
      "step": 427
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.029530595988035202,
      "learning_rate": 0.018218893050353725,
      "loss": 0.3613,
      "step": 428
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.024436235427856445,
      "learning_rate": 0.018214731585518104,
      "loss": 0.3896,
      "step": 429
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.016348615288734436,
      "learning_rate": 0.01821057012068248,
      "loss": 0.3174,
      "step": 430
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.015601023100316525,
      "learning_rate": 0.018206408655846856,
      "loss": 0.2079,
      "step": 431
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.014723938889801502,
      "learning_rate": 0.018202247191011236,
      "loss": 0.2329,
      "step": 432
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.024064524099230766,
      "learning_rate": 0.018198085726175615,
      "loss": 0.4507,
      "step": 433
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.025292137637734413,
      "learning_rate": 0.01819392426133999,
      "loss": 0.6035,
      "step": 434
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.022207777947187424,
      "learning_rate": 0.01818976279650437,
      "loss": 0.7056,
      "step": 435
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.022870400920510292,
      "learning_rate": 0.018185601331668747,
      "loss": 0.2622,
      "step": 436
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.020909421145915985,
      "learning_rate": 0.018181439866833123,
      "loss": 0.2377,
      "step": 437
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.021142246201634407,
      "learning_rate": 0.018177278401997503,
      "loss": 0.2996,
      "step": 438
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.01641058176755905,
      "learning_rate": 0.018173116937161882,
      "loss": 0.3174,
      "step": 439
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.021069413051009178,
      "learning_rate": 0.01816895547232626,
      "loss": 0.2654,
      "step": 440
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.025653362274169922,
      "learning_rate": 0.018164794007490638,
      "loss": 0.4221,
      "step": 441
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.01986721344292164,
      "learning_rate": 0.018160632542655014,
      "loss": 0.3657,
      "step": 442
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.021333815529942513,
      "learning_rate": 0.018156471077819394,
      "loss": 0.467,
      "step": 443
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.018084486946463585,
      "learning_rate": 0.01815230961298377,
      "loss": 0.2644,
      "step": 444
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.028349634259939194,
      "learning_rate": 0.01814814814814815,
      "loss": 0.5391,
      "step": 445
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.022226015105843544,
      "learning_rate": 0.018143986683312525,
      "loss": 0.5645,
      "step": 446
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.015429804101586342,
      "learning_rate": 0.018139825218476905,
      "loss": 0.0782,
      "step": 447
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.024509834125638008,
      "learning_rate": 0.01813566375364128,
      "loss": 0.4612,
      "step": 448
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.011498290114104748,
      "learning_rate": 0.01813150228880566,
      "loss": 0.1084,
      "step": 449
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.016364967450499535,
      "learning_rate": 0.018127340823970037,
      "loss": 0.3008,
      "step": 450
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.0184890478849411,
      "learning_rate": 0.018123179359134416,
      "loss": 0.2186,
      "step": 451
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.01742672361433506,
      "learning_rate": 0.018119017894298792,
      "loss": 0.243,
      "step": 452
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.01943567395210266,
      "learning_rate": 0.018114856429463172,
      "loss": 0.2913,
      "step": 453
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.024606555700302124,
      "learning_rate": 0.01811069496462755,
      "loss": 0.0767,
      "step": 454
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.008739195764064789,
      "learning_rate": 0.018106533499791928,
      "loss": 0.0287,
      "step": 455
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.01797635853290558,
      "learning_rate": 0.018102372034956304,
      "loss": 0.1814,
      "step": 456
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.015862181782722473,
      "learning_rate": 0.018098210570120683,
      "loss": 0.4111,
      "step": 457
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.022920627146959305,
      "learning_rate": 0.01809404910528506,
      "loss": 0.375,
      "step": 458
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.01702135242521763,
      "learning_rate": 0.01808988764044944,
      "loss": 0.3601,
      "step": 459
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.016336023807525635,
      "learning_rate": 0.01808572617561382,
      "loss": 0.1782,
      "step": 460
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.015450933948159218,
      "learning_rate": 0.018081564710778195,
      "loss": 0.2888,
      "step": 461
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.02359917387366295,
      "learning_rate": 0.01807740324594257,
      "loss": 0.313,
      "step": 462
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.011471382342278957,
      "learning_rate": 0.01807324178110695,
      "loss": 0.1418,
      "step": 463
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.019025051966309547,
      "learning_rate": 0.018069080316271326,
      "loss": 0.3232,
      "step": 464
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.0016236061928793788,
      "learning_rate": 0.018064918851435706,
      "loss": 0.0021,
      "step": 465
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.018086018040776253,
      "learning_rate": 0.018060757386600085,
      "loss": 0.3213,
      "step": 466
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.014555457048118114,
      "learning_rate": 0.01805659592176446,
      "loss": 0.144,
      "step": 467
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.01344548910856247,
      "learning_rate": 0.018052434456928838,
      "loss": 0.1176,
      "step": 468
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.014173297211527824,
      "learning_rate": 0.018048272992093217,
      "loss": 0.0638,
      "step": 469
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.021349281072616577,
      "learning_rate": 0.018044111527257593,
      "loss": 0.1447,
      "step": 470
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.02288207970559597,
      "learning_rate": 0.018039950062421973,
      "loss": 0.3296,
      "step": 471
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.0221555233001709,
      "learning_rate": 0.018035788597586352,
      "loss": 0.2883,
      "step": 472
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.020956367254257202,
      "learning_rate": 0.01803162713275073,
      "loss": 0.2776,
      "step": 473
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.03914099186658859,
      "learning_rate": 0.018027465667915105,
      "loss": 0.2311,
      "step": 474
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.016052138060331345,
      "learning_rate": 0.018023304203079484,
      "loss": 0.2822,
      "step": 475
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.02345450036227703,
      "learning_rate": 0.018019142738243864,
      "loss": 0.4175,
      "step": 476
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.015222625806927681,
      "learning_rate": 0.01801498127340824,
      "loss": 0.2379,
      "step": 477
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.01661856472492218,
      "learning_rate": 0.01801081980857262,
      "loss": 0.142,
      "step": 478
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.027985790744423866,
      "learning_rate": 0.018006658343736996,
      "loss": 0.2676,
      "step": 479
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.005155233666300774,
      "learning_rate": 0.01800249687890137,
      "loss": 0.0064,
      "step": 480
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.01854134164750576,
      "learning_rate": 0.01799833541406575,
      "loss": 0.3274,
      "step": 481
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.021399671211838722,
      "learning_rate": 0.01799417394923013,
      "loss": 0.3025,
      "step": 482
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.013370412401854992,
      "learning_rate": 0.017990012484394507,
      "loss": 0.22,
      "step": 483
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.02801896072924137,
      "learning_rate": 0.017985851019558886,
      "loss": 0.5527,
      "step": 484
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.016469566151499748,
      "learning_rate": 0.017981689554723262,
      "loss": 0.3855,
      "step": 485
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.018887661397457123,
      "learning_rate": 0.017977528089887642,
      "loss": 0.1444,
      "step": 486
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.016790995374321938,
      "learning_rate": 0.017973366625052018,
      "loss": 0.1682,
      "step": 487
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.01708918623626232,
      "learning_rate": 0.017969205160216398,
      "loss": 0.2532,
      "step": 488
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.013119114562869072,
      "learning_rate": 0.017965043695380774,
      "loss": 0.2593,
      "step": 489
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.01730186864733696,
      "learning_rate": 0.017960882230545153,
      "loss": 0.4368,
      "step": 490
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.0007973984465934336,
      "learning_rate": 0.01795672076570953,
      "loss": 0.0015,
      "step": 491
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.016564833000302315,
      "learning_rate": 0.01795255930087391,
      "loss": 0.1823,
      "step": 492
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.017495324835181236,
      "learning_rate": 0.017948397836038285,
      "loss": 0.2197,
      "step": 493
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.013706967234611511,
      "learning_rate": 0.017944236371202665,
      "loss": 0.0506,
      "step": 494
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.02236577868461609,
      "learning_rate": 0.01794007490636704,
      "loss": 0.0427,
      "step": 495
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.016671421006321907,
      "learning_rate": 0.01793591344153142,
      "loss": 0.4666,
      "step": 496
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.021448083221912384,
      "learning_rate": 0.0179317519766958,
      "loss": 0.2856,
      "step": 497
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.022695811465382576,
      "learning_rate": 0.017927590511860176,
      "loss": 0.4028,
      "step": 498
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.013902803882956505,
      "learning_rate": 0.017923429047024552,
      "loss": 0.386,
      "step": 499
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.013315980322659016,
      "learning_rate": 0.01791926758218893,
      "loss": 0.0199,
      "step": 500
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.01322450116276741,
      "learning_rate": 0.017915106117353308,
      "loss": 0.0723,
      "step": 501
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.026917418465018272,
      "learning_rate": 0.017910944652517687,
      "loss": 0.4121,
      "step": 502
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.017656033858656883,
      "learning_rate": 0.017906783187682067,
      "loss": 0.166,
      "step": 503
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.016564955934882164,
      "learning_rate": 0.017902621722846443,
      "loss": 0.4065,
      "step": 504
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.008763215504586697,
      "learning_rate": 0.01789846025801082,
      "loss": 0.0299,
      "step": 505
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.0215713270008564,
      "learning_rate": 0.0178942987931752,
      "loss": 0.4661,
      "step": 506
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.011343676596879959,
      "learning_rate": 0.017890137328339575,
      "loss": 0.1272,
      "step": 507
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.021843140944838524,
      "learning_rate": 0.017885975863503954,
      "loss": 0.3967,
      "step": 508
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.005304041784256697,
      "learning_rate": 0.017881814398668334,
      "loss": 0.0106,
      "step": 509
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.014229088090360165,
      "learning_rate": 0.01787765293383271,
      "loss": 0.1688,
      "step": 510
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.02197921648621559,
      "learning_rate": 0.017873491468997086,
      "loss": 0.3103,
      "step": 511
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.0247966218739748,
      "learning_rate": 0.017869330004161466,
      "loss": 0.2664,
      "step": 512
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.017200419679284096,
      "learning_rate": 0.01786516853932584,
      "loss": 0.1678,
      "step": 513
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.01293894462287426,
      "learning_rate": 0.01786100707449022,
      "loss": 0.136,
      "step": 514
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.015838829800486565,
      "learning_rate": 0.0178568456096546,
      "loss": 0.2961,
      "step": 515
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.014234489761292934,
      "learning_rate": 0.017852684144818977,
      "loss": 0.4495,
      "step": 516
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.022854570299386978,
      "learning_rate": 0.017848522679983353,
      "loss": 0.1954,
      "step": 517
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.03496082127094269,
      "learning_rate": 0.017844361215147733,
      "loss": 0.8726,
      "step": 518
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.007512242998927832,
      "learning_rate": 0.01784019975031211,
      "loss": 0.0353,
      "step": 519
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.012978347018361092,
      "learning_rate": 0.017836038285476488,
      "loss": 0.47,
      "step": 520
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.0132742365822196,
      "learning_rate": 0.017831876820640868,
      "loss": 0.0959,
      "step": 521
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.023542677983641624,
      "learning_rate": 0.017827715355805244,
      "loss": 0.217,
      "step": 522
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.01400215644389391,
      "learning_rate": 0.01782355389096962,
      "loss": 0.0919,
      "step": 523
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.010237505659461021,
      "learning_rate": 0.017819392426134,
      "loss": 0.0629,
      "step": 524
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.015069641172885895,
      "learning_rate": 0.017815230961298376,
      "loss": 0.1357,
      "step": 525
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.011609483510255814,
      "learning_rate": 0.017811069496462755,
      "loss": 0.0459,
      "step": 526
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.01928263157606125,
      "learning_rate": 0.017806908031627135,
      "loss": 0.3618,
      "step": 527
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.015118389390408993,
      "learning_rate": 0.01780274656679151,
      "loss": 0.1812,
      "step": 528
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.030556663870811462,
      "learning_rate": 0.01779858510195589,
      "loss": 0.4568,
      "step": 529
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.02257608063519001,
      "learning_rate": 0.017794423637120266,
      "loss": 0.2603,
      "step": 530
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.02450978383421898,
      "learning_rate": 0.017790262172284643,
      "loss": 0.3921,
      "step": 531
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.018064459785819054,
      "learning_rate": 0.017786100707449022,
      "loss": 0.1653,
      "step": 532
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.014579844661056995,
      "learning_rate": 0.0177819392426134,
      "loss": 0.5142,
      "step": 533
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.018851060420274734,
      "learning_rate": 0.017777777777777778,
      "loss": 0.2979,
      "step": 534
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.01819692738354206,
      "learning_rate": 0.017773616312942157,
      "loss": 0.4919,
      "step": 535
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.012211748398840427,
      "learning_rate": 0.017769454848106533,
      "loss": 0.3062,
      "step": 536
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.019866351038217545,
      "learning_rate": 0.01776529338327091,
      "loss": 0.5698,
      "step": 537
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.01991969533264637,
      "learning_rate": 0.01776113191843529,
      "loss": 0.4614,
      "step": 538
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.01758577860891819,
      "learning_rate": 0.01775697045359967,
      "loss": 0.1968,
      "step": 539
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.011264219880104065,
      "learning_rate": 0.017752808988764045,
      "loss": 0.0826,
      "step": 540
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.018839716911315918,
      "learning_rate": 0.017748647523928424,
      "loss": 0.4075,
      "step": 541
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.012899573892354965,
      "learning_rate": 0.0177444860590928,
      "loss": 0.2368,
      "step": 542
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.02240430936217308,
      "learning_rate": 0.017740324594257176,
      "loss": 0.2886,
      "step": 543
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.01477088499814272,
      "learning_rate": 0.017736163129421556,
      "loss": 0.0632,
      "step": 544
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.0271300058811903,
      "learning_rate": 0.017732001664585936,
      "loss": 0.98,
      "step": 545
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.03747643902897835,
      "learning_rate": 0.01772784019975031,
      "loss": 0.2114,
      "step": 546
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.013209293596446514,
      "learning_rate": 0.01772367873491469,
      "loss": 0.1293,
      "step": 547
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.023283572867512703,
      "learning_rate": 0.017719517270079067,
      "loss": 0.1272,
      "step": 548
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.02200518362224102,
      "learning_rate": 0.017715355805243447,
      "loss": 0.6167,
      "step": 549
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.018459653481841087,
      "learning_rate": 0.017711194340407823,
      "loss": 0.1938,
      "step": 550
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.01839401386678219,
      "learning_rate": 0.017707032875572203,
      "loss": 0.4368,
      "step": 551
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.031117385253310204,
      "learning_rate": 0.017702871410736582,
      "loss": 0.5078,
      "step": 552
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.016220442950725555,
      "learning_rate": 0.017698709945900958,
      "loss": 0.2343,
      "step": 553
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.028234465047717094,
      "learning_rate": 0.017694548481065334,
      "loss": 0.4602,
      "step": 554
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.021623658016324043,
      "learning_rate": 0.017690387016229714,
      "loss": 0.241,
      "step": 555
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.018278444185853004,
      "learning_rate": 0.01768622555139409,
      "loss": 0.2126,
      "step": 556
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.018106509000062943,
      "learning_rate": 0.01768206408655847,
      "loss": 0.4719,
      "step": 557
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.012071166187524796,
      "learning_rate": 0.01767790262172285,
      "loss": 0.1615,
      "step": 558
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.026161981746554375,
      "learning_rate": 0.017673741156887225,
      "loss": 0.6118,
      "step": 559
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.01915688067674637,
      "learning_rate": 0.0176695796920516,
      "loss": 0.0596,
      "step": 560
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.01590966060757637,
      "learning_rate": 0.01766541822721598,
      "loss": 0.1901,
      "step": 561
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.009362030774354935,
      "learning_rate": 0.017661256762380357,
      "loss": 0.0426,
      "step": 562
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.015719180926680565,
      "learning_rate": 0.017657095297544737,
      "loss": 0.2035,
      "step": 563
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.014808082953095436,
      "learning_rate": 0.017652933832709116,
      "loss": 0.1589,
      "step": 564
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.022726433351635933,
      "learning_rate": 0.017648772367873492,
      "loss": 0.2559,
      "step": 565
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.02343447506427765,
      "learning_rate": 0.017644610903037868,
      "loss": 0.4946,
      "step": 566
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.016504652798175812,
      "learning_rate": 0.017640449438202248,
      "loss": 0.1726,
      "step": 567
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.016712628304958344,
      "learning_rate": 0.017636287973366624,
      "loss": 0.2517,
      "step": 568
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.023331880569458008,
      "learning_rate": 0.017632126508531003,
      "loss": 0.3794,
      "step": 569
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.005479931831359863,
      "learning_rate": 0.017627965043695383,
      "loss": 0.0262,
      "step": 570
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.015947291627526283,
      "learning_rate": 0.01762380357885976,
      "loss": 0.2195,
      "step": 571
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.0175675917416811,
      "learning_rate": 0.01761964211402414,
      "loss": 0.1621,
      "step": 572
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.02074829861521721,
      "learning_rate": 0.017615480649188515,
      "loss": 0.27,
      "step": 573
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.0031139347702264786,
      "learning_rate": 0.01761131918435289,
      "loss": 0.005,
      "step": 574
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.013330874964594841,
      "learning_rate": 0.01760715771951727,
      "loss": 0.2313,
      "step": 575
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.018731823191046715,
      "learning_rate": 0.01760299625468165,
      "loss": 0.3057,
      "step": 576
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.03177861496806145,
      "learning_rate": 0.017598834789846026,
      "loss": 0.3982,
      "step": 577
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.022089118137955666,
      "learning_rate": 0.017594673325010406,
      "loss": 0.3254,
      "step": 578
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.010733268223702908,
      "learning_rate": 0.017590511860174782,
      "loss": 0.0645,
      "step": 579
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.022312788292765617,
      "learning_rate": 0.017586350395339158,
      "loss": 0.4197,
      "step": 580
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.020953146740794182,
      "learning_rate": 0.017582188930503537,
      "loss": 0.4355,
      "step": 581
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.014730071648955345,
      "learning_rate": 0.017578027465667917,
      "loss": 0.3032,
      "step": 582
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.008113733492791653,
      "learning_rate": 0.017573866000832293,
      "loss": 0.0445,
      "step": 583
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.015508532524108887,
      "learning_rate": 0.017569704535996673,
      "loss": 0.1904,
      "step": 584
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.028984874486923218,
      "learning_rate": 0.01756554307116105,
      "loss": 0.522,
      "step": 585
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.017001159489154816,
      "learning_rate": 0.017561381606325425,
      "loss": 0.5171,
      "step": 586
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.006894498132169247,
      "learning_rate": 0.017557220141489804,
      "loss": 0.0688,
      "step": 587
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.020242296159267426,
      "learning_rate": 0.017553058676654184,
      "loss": 0.3547,
      "step": 588
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.02047143317759037,
      "learning_rate": 0.01754889721181856,
      "loss": 0.1675,
      "step": 589
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.026067951694130898,
      "learning_rate": 0.01754473574698294,
      "loss": 0.1515,
      "step": 590
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.030283140018582344,
      "learning_rate": 0.017540574282147316,
      "loss": 0.5273,
      "step": 591
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.018824715167284012,
      "learning_rate": 0.017536412817311692,
      "loss": 0.4636,
      "step": 592
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.013927336782217026,
      "learning_rate": 0.01753225135247607,
      "loss": 0.332,
      "step": 593
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.014410550706088543,
      "learning_rate": 0.01752808988764045,
      "loss": 0.1453,
      "step": 594
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.0069348462857306,
      "learning_rate": 0.017523928422804827,
      "loss": 0.0132,
      "step": 595
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.014951486140489578,
      "learning_rate": 0.017519766957969207,
      "loss": 0.2013,
      "step": 596
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.023131584748625755,
      "learning_rate": 0.017515605493133583,
      "loss": 0.5049,
      "step": 597
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.016293618828058243,
      "learning_rate": 0.01751144402829796,
      "loss": 0.303,
      "step": 598
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.011231700889766216,
      "learning_rate": 0.01750728256346234,
      "loss": 0.1244,
      "step": 599
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.02238783799111843,
      "learning_rate": 0.017503121098626718,
      "loss": 0.4756,
      "step": 600
    },
    {
      "epoch": 0.75,
      "eval_loss": 0.2890625,
      "eval_runtime": 183.0178,
      "eval_samples_per_second": 1.098,
      "eval_steps_per_second": 0.552,
      "step": 600
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.007596211973577738,
      "learning_rate": 0.017498959633791094,
      "loss": 0.0335,
      "step": 601
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.023596379905939102,
      "learning_rate": 0.017494798168955474,
      "loss": 0.281,
      "step": 602
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.015985485166311264,
      "learning_rate": 0.01749063670411985,
      "loss": 0.1886,
      "step": 603
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.017524354159832,
      "learning_rate": 0.01748647523928423,
      "loss": 0.3987,
      "step": 604
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.03007538802921772,
      "learning_rate": 0.017482313774448605,
      "loss": 0.2864,
      "step": 605
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.019697928801178932,
      "learning_rate": 0.017478152309612985,
      "loss": 0.2177,
      "step": 606
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.029588017612695694,
      "learning_rate": 0.01747399084477736,
      "loss": 0.1004,
      "step": 607
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.010724203661084175,
      "learning_rate": 0.01746982937994174,
      "loss": 0.0515,
      "step": 608
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.011881942860782146,
      "learning_rate": 0.017465667915106117,
      "loss": 0.0547,
      "step": 609
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.03622346371412277,
      "learning_rate": 0.017461506450270496,
      "loss": 0.5923,
      "step": 610
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.019429568201303482,
      "learning_rate": 0.017457344985434872,
      "loss": 0.2776,
      "step": 611
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.028999803587794304,
      "learning_rate": 0.017453183520599252,
      "loss": 0.25,
      "step": 612
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.012266678735613823,
      "learning_rate": 0.017449022055763628,
      "loss": 0.0726,
      "step": 613
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.007757336366921663,
      "learning_rate": 0.017444860590928007,
      "loss": 0.0082,
      "step": 614
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.023186028003692627,
      "learning_rate": 0.017440699126092387,
      "loss": 0.4475,
      "step": 615
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.01969732716679573,
      "learning_rate": 0.017436537661256763,
      "loss": 0.0712,
      "step": 616
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.03147481009364128,
      "learning_rate": 0.01743237619642114,
      "loss": 0.3835,
      "step": 617
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.03012148104608059,
      "learning_rate": 0.01742821473158552,
      "loss": 0.4275,
      "step": 618
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.022663293406367302,
      "learning_rate": 0.0174240532667499,
      "loss": 0.4573,
      "step": 619
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.015420160256326199,
      "learning_rate": 0.017419891801914274,
      "loss": 0.068,
      "step": 620
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.014434471726417542,
      "learning_rate": 0.017415730337078654,
      "loss": 0.1478,
      "step": 621
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.013026262633502483,
      "learning_rate": 0.01741156887224303,
      "loss": 0.0424,
      "step": 622
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.02457118220627308,
      "learning_rate": 0.017407407407407406,
      "loss": 0.4966,
      "step": 623
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.017207419499754906,
      "learning_rate": 0.017403245942571786,
      "loss": 0.332,
      "step": 624
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.020753245800733566,
      "learning_rate": 0.017399084477736165,
      "loss": 0.2368,
      "step": 625
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.007681884802877903,
      "learning_rate": 0.01739492301290054,
      "loss": 0.0363,
      "step": 626
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.020835790783166885,
      "learning_rate": 0.01739076154806492,
      "loss": 0.4446,
      "step": 627
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.02038772776722908,
      "learning_rate": 0.017386600083229297,
      "loss": 0.3643,
      "step": 628
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.015672018751502037,
      "learning_rate": 0.017382438618393673,
      "loss": 0.1254,
      "step": 629
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.033103521913290024,
      "learning_rate": 0.017378277153558053,
      "loss": 0.4282,
      "step": 630
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.00956597737967968,
      "learning_rate": 0.017374115688722432,
      "loss": 0.0905,
      "step": 631
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.011758465319871902,
      "learning_rate": 0.01736995422388681,
      "loss": 0.0837,
      "step": 632
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.007047138176858425,
      "learning_rate": 0.017365792759051188,
      "loss": 0.0301,
      "step": 633
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.011444054543972015,
      "learning_rate": 0.017361631294215564,
      "loss": 0.2379,
      "step": 634
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.020829778164625168,
      "learning_rate": 0.01735746982937994,
      "loss": 0.2778,
      "step": 635
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.0010183991398662329,
      "learning_rate": 0.01735330836454432,
      "loss": 0.0013,
      "step": 636
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.023843087255954742,
      "learning_rate": 0.0173491468997087,
      "loss": 0.2979,
      "step": 637
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.015374841168522835,
      "learning_rate": 0.017344985434873075,
      "loss": 0.1357,
      "step": 638
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.01591675356030464,
      "learning_rate": 0.017340823970037455,
      "loss": 0.2201,
      "step": 639
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.009581562131643295,
      "learning_rate": 0.01733666250520183,
      "loss": 0.0497,
      "step": 640
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.04708545282483101,
      "learning_rate": 0.017332501040366207,
      "loss": 0.3418,
      "step": 641
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.02598581276834011,
      "learning_rate": 0.017328339575530587,
      "loss": 0.5991,
      "step": 642
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.021948708221316338,
      "learning_rate": 0.017324178110694966,
      "loss": 0.2468,
      "step": 643
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.0162848848849535,
      "learning_rate": 0.017320016645859342,
      "loss": 0.2642,
      "step": 644
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.01911388337612152,
      "learning_rate": 0.017315855181023722,
      "loss": 0.4583,
      "step": 645
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.012276459485292435,
      "learning_rate": 0.017311693716188098,
      "loss": 0.2476,
      "step": 646
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.020888086408376694,
      "learning_rate": 0.017307532251352477,
      "loss": 0.3142,
      "step": 647
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.01172264851629734,
      "learning_rate": 0.017303370786516854,
      "loss": 0.0508,
      "step": 648
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.016026314347982407,
      "learning_rate": 0.017299209321681233,
      "loss": 0.28,
      "step": 649
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.022636111825704575,
      "learning_rate": 0.01729504785684561,
      "loss": 0.4592,
      "step": 650
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.01944619044661522,
      "learning_rate": 0.01729088639200999,
      "loss": 0.4524,
      "step": 651
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.017689630389213562,
      "learning_rate": 0.017286724927174365,
      "loss": 0.2343,
      "step": 652
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.009402558207511902,
      "learning_rate": 0.017282563462338744,
      "loss": 0.1232,
      "step": 653
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.02710762619972229,
      "learning_rate": 0.01727840199750312,
      "loss": 0.2598,
      "step": 654
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.028154436498880386,
      "learning_rate": 0.0172742405326675,
      "loss": 0.3254,
      "step": 655
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.013529193587601185,
      "learning_rate": 0.017270079067831876,
      "loss": 0.2168,
      "step": 656
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.01863241009414196,
      "learning_rate": 0.017265917602996256,
      "loss": 0.4363,
      "step": 657
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.01757151633501053,
      "learning_rate": 0.017261756138160635,
      "loss": 0.3752,
      "step": 658
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.01853569597005844,
      "learning_rate": 0.01725759467332501,
      "loss": 0.1473,
      "step": 659
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.00975044071674347,
      "learning_rate": 0.017253433208489388,
      "loss": 0.0475,
      "step": 660
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.010218817740678787,
      "learning_rate": 0.017249271743653767,
      "loss": 0.0728,
      "step": 661
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.025808019563555717,
      "learning_rate": 0.017245110278818143,
      "loss": 0.5747,
      "step": 662
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.01887434720993042,
      "learning_rate": 0.017240948813982523,
      "loss": 0.2776,
      "step": 663
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.019917313009500504,
      "learning_rate": 0.017236787349146902,
      "loss": 0.3391,
      "step": 664
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.020668426528573036,
      "learning_rate": 0.01723262588431128,
      "loss": 0.1902,
      "step": 665
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.013723928481340408,
      "learning_rate": 0.017228464419475654,
      "loss": 0.103,
      "step": 666
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.002858448540791869,
      "learning_rate": 0.017224302954640034,
      "loss": 0.0057,
      "step": 667
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.010656685568392277,
      "learning_rate": 0.01722014148980441,
      "loss": 0.0876,
      "step": 668
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.02592632547020912,
      "learning_rate": 0.01721598002496879,
      "loss": 0.7422,
      "step": 669
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.015538211911916733,
      "learning_rate": 0.01721181856013317,
      "loss": 0.2396,
      "step": 670
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.02267232909798622,
      "learning_rate": 0.017207657095297545,
      "loss": 0.2925,
      "step": 671
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.018011275678873062,
      "learning_rate": 0.01720349563046192,
      "loss": 0.4634,
      "step": 672
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.01288380566984415,
      "learning_rate": 0.0171993341656263,
      "loss": 0.1312,
      "step": 673
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.024027708917856216,
      "learning_rate": 0.017195172700790677,
      "loss": 0.2993,
      "step": 674
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.007085581310093403,
      "learning_rate": 0.017191011235955057,
      "loss": 0.0382,
      "step": 675
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.02409108355641365,
      "learning_rate": 0.017186849771119436,
      "loss": 0.1493,
      "step": 676
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.018357884138822556,
      "learning_rate": 0.017182688306283812,
      "loss": 0.2805,
      "step": 677
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.017132099717855453,
      "learning_rate": 0.01717852684144819,
      "loss": 0.249,
      "step": 678
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.02037871442735195,
      "learning_rate": 0.017174365376612568,
      "loss": 0.5376,
      "step": 679
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.023976625874638557,
      "learning_rate": 0.017170203911776944,
      "loss": 0.1683,
      "step": 680
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.034002043306827545,
      "learning_rate": 0.017166042446941324,
      "loss": 0.2203,
      "step": 681
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.013092076405882835,
      "learning_rate": 0.017161880982105703,
      "loss": 0.0602,
      "step": 682
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.01644989661872387,
      "learning_rate": 0.01715771951727008,
      "loss": 0.2192,
      "step": 683
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.020339403301477432,
      "learning_rate": 0.017153558052434455,
      "loss": 0.4336,
      "step": 684
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.00023564025468658656,
      "learning_rate": 0.017149396587598835,
      "loss": 0.0004,
      "step": 685
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.018476612865924835,
      "learning_rate": 0.01714523512276321,
      "loss": 0.2593,
      "step": 686
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.020325573161244392,
      "learning_rate": 0.01714107365792759,
      "loss": 0.2556,
      "step": 687
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.024439135566353798,
      "learning_rate": 0.01713691219309197,
      "loss": 0.2673,
      "step": 688
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.024870112538337708,
      "learning_rate": 0.017132750728256346,
      "loss": 0.5039,
      "step": 689
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.02139340527355671,
      "learning_rate": 0.017128589263420726,
      "loss": 0.2881,
      "step": 690
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.019896239042282104,
      "learning_rate": 0.017124427798585102,
      "loss": 0.261,
      "step": 691
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.021802013739943504,
      "learning_rate": 0.01712026633374948,
      "loss": 0.3359,
      "step": 692
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.024331288412213326,
      "learning_rate": 0.017116104868913858,
      "loss": 0.5405,
      "step": 693
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.0015833770157769322,
      "learning_rate": 0.017111943404078237,
      "loss": 0.0021,
      "step": 694
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.016407884657382965,
      "learning_rate": 0.017107781939242613,
      "loss": 0.3918,
      "step": 695
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.016509268432855606,
      "learning_rate": 0.017103620474406993,
      "loss": 0.1449,
      "step": 696
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.013785818591713905,
      "learning_rate": 0.01709945900957137,
      "loss": 0.0516,
      "step": 697
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.023755798116326332,
      "learning_rate": 0.01709529754473575,
      "loss": 0.2493,
      "step": 698
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.01698322221636772,
      "learning_rate": 0.017091136079900125,
      "loss": 0.2065,
      "step": 699
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.015968430787324905,
      "learning_rate": 0.017086974615064504,
      "loss": 0.153,
      "step": 700
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.02535528875887394,
      "learning_rate": 0.017082813150228884,
      "loss": 0.4792,
      "step": 701
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.013174477964639664,
      "learning_rate": 0.01707865168539326,
      "loss": 0.1099,
      "step": 702
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.016354139894247055,
      "learning_rate": 0.017074490220557636,
      "loss": 0.16,
      "step": 703
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.025194713845849037,
      "learning_rate": 0.017070328755722015,
      "loss": 0.6577,
      "step": 704
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.017464514821767807,
      "learning_rate": 0.01706616729088639,
      "loss": 0.2629,
      "step": 705
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.026380835101008415,
      "learning_rate": 0.01706200582605077,
      "loss": 0.3901,
      "step": 706
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.00990260485559702,
      "learning_rate": 0.01705784436121515,
      "loss": 0.0551,
      "step": 707
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.01918584108352661,
      "learning_rate": 0.017053682896379527,
      "loss": 0.5674,
      "step": 708
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.020656105130910873,
      "learning_rate": 0.017049521431543903,
      "loss": 0.184,
      "step": 709
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.0175065565854311,
      "learning_rate": 0.017045359966708282,
      "loss": 0.224,
      "step": 710
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.01879909820854664,
      "learning_rate": 0.01704119850187266,
      "loss": 0.1205,
      "step": 711
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.017924316227436066,
      "learning_rate": 0.017037037037037038,
      "loss": 0.3433,
      "step": 712
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.027081698179244995,
      "learning_rate": 0.017032875572201418,
      "loss": 0.7124,
      "step": 713
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.010771729983389378,
      "learning_rate": 0.017028714107365794,
      "loss": 0.0534,
      "step": 714
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.0018802395788952708,
      "learning_rate": 0.01702455264253017,
      "loss": 0.0037,
      "step": 715
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.0325336679816246,
      "learning_rate": 0.01702039117769455,
      "loss": 0.3564,
      "step": 716
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.018861383199691772,
      "learning_rate": 0.017016229712858925,
      "loss": 0.3057,
      "step": 717
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.015037810429930687,
      "learning_rate": 0.017012068248023305,
      "loss": 0.1246,
      "step": 718
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.01766980066895485,
      "learning_rate": 0.017007906783187685,
      "loss": 0.4333,
      "step": 719
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.030631523579359055,
      "learning_rate": 0.01700374531835206,
      "loss": 0.9087,
      "step": 720
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.01510903611779213,
      "learning_rate": 0.016999583853516437,
      "loss": 0.2042,
      "step": 721
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.02484944276511669,
      "learning_rate": 0.016995422388680816,
      "loss": 0.1501,
      "step": 722
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.021651616320014,
      "learning_rate": 0.016991260923845192,
      "loss": 0.5435,
      "step": 723
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.02143322303891182,
      "learning_rate": 0.016987099459009572,
      "loss": 0.303,
      "step": 724
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.021336205303668976,
      "learning_rate": 0.01698293799417395,
      "loss": 0.2406,
      "step": 725
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.027396924793720245,
      "learning_rate": 0.016978776529338328,
      "loss": 0.201,
      "step": 726
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.021244604140520096,
      "learning_rate": 0.016974615064502704,
      "loss": 0.3933,
      "step": 727
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.01537284255027771,
      "learning_rate": 0.016970453599667083,
      "loss": 0.3054,
      "step": 728
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.010397098027169704,
      "learning_rate": 0.01696629213483146,
      "loss": 0.0867,
      "step": 729
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.019817698746919632,
      "learning_rate": 0.01696213066999584,
      "loss": 0.4824,
      "step": 730
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.00739717110991478,
      "learning_rate": 0.01695796920516022,
      "loss": 0.0374,
      "step": 731
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.024609167128801346,
      "learning_rate": 0.016953807740324595,
      "loss": 0.3767,
      "step": 732
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.014955861493945122,
      "learning_rate": 0.016949646275488974,
      "loss": 0.1862,
      "step": 733
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.021667569875717163,
      "learning_rate": 0.01694548481065335,
      "loss": 0.188,
      "step": 734
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.01052304357290268,
      "learning_rate": 0.016941323345817726,
      "loss": 0.0376,
      "step": 735
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.021990731358528137,
      "learning_rate": 0.016937161880982106,
      "loss": 0.1248,
      "step": 736
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.02426757477223873,
      "learning_rate": 0.016933000416146485,
      "loss": 0.3538,
      "step": 737
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.013726136647164822,
      "learning_rate": 0.01692883895131086,
      "loss": 0.1019,
      "step": 738
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.02444680966436863,
      "learning_rate": 0.01692467748647524,
      "loss": 0.3425,
      "step": 739
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.020823568105697632,
      "learning_rate": 0.016920516021639617,
      "loss": 0.3403,
      "step": 740
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.017308739945292473,
      "learning_rate": 0.016916354556803993,
      "loss": 0.2034,
      "step": 741
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.019694337621331215,
      "learning_rate": 0.016912193091968373,
      "loss": 0.2474,
      "step": 742
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.026313232257962227,
      "learning_rate": 0.016908031627132752,
      "loss": 0.334,
      "step": 743
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.011907829903066158,
      "learning_rate": 0.01690387016229713,
      "loss": 0.1154,
      "step": 744
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.02314845472574234,
      "learning_rate": 0.016899708697461508,
      "loss": 0.4189,
      "step": 745
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.017764708027243614,
      "learning_rate": 0.016895547232625884,
      "loss": 0.3669,
      "step": 746
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.02391207590699196,
      "learning_rate": 0.01689138576779026,
      "loss": 0.5166,
      "step": 747
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.01798044703900814,
      "learning_rate": 0.01688722430295464,
      "loss": 0.2668,
      "step": 748
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.016684608533978462,
      "learning_rate": 0.01688306283811902,
      "loss": 0.1847,
      "step": 749
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.02295888401567936,
      "learning_rate": 0.016878901373283395,
      "loss": 0.4199,
      "step": 750
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.01934513822197914,
      "learning_rate": 0.016874739908447775,
      "loss": 0.4692,
      "step": 751
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.020473016425967216,
      "learning_rate": 0.01687057844361215,
      "loss": 0.2822,
      "step": 752
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.013607135973870754,
      "learning_rate": 0.016866416978776527,
      "loss": 0.0914,
      "step": 753
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.015915434807538986,
      "learning_rate": 0.016862255513940907,
      "loss": 0.0856,
      "step": 754
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.015103944577276707,
      "learning_rate": 0.016858094049105286,
      "loss": 0.2494,
      "step": 755
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.0011906675063073635,
      "learning_rate": 0.016853932584269662,
      "loss": 0.0012,
      "step": 756
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.029274487867951393,
      "learning_rate": 0.016849771119434042,
      "loss": 0.4189,
      "step": 757
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.020976923406124115,
      "learning_rate": 0.016845609654598418,
      "loss": 0.3796,
      "step": 758
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.012252322398126125,
      "learning_rate": 0.016841448189762794,
      "loss": 0.1074,
      "step": 759
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.018603961914777756,
      "learning_rate": 0.016837286724927174,
      "loss": 0.1809,
      "step": 760
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.023209350183606148,
      "learning_rate": 0.016833125260091553,
      "loss": 0.4783,
      "step": 761
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.009813662618398666,
      "learning_rate": 0.01682896379525593,
      "loss": 0.064,
      "step": 762
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.012941231951117516,
      "learning_rate": 0.01682480233042031,
      "loss": 0.1528,
      "step": 763
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.03905802220106125,
      "learning_rate": 0.016820640865584685,
      "loss": 0.6968,
      "step": 764
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.0210886150598526,
      "learning_rate": 0.016816479400749065,
      "loss": 0.3132,
      "step": 765
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.02608836255967617,
      "learning_rate": 0.01681231793591344,
      "loss": 0.3147,
      "step": 766
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.017373185604810715,
      "learning_rate": 0.01680815647107782,
      "loss": 0.3047,
      "step": 767
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.02733996883034706,
      "learning_rate": 0.0168039950062422,
      "loss": 0.2959,
      "step": 768
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.013021211139857769,
      "learning_rate": 0.016799833541406576,
      "loss": 0.0486,
      "step": 769
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.03494340926408768,
      "learning_rate": 0.016795672076570952,
      "loss": 0.6533,
      "step": 770
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.00021946057677268982,
      "learning_rate": 0.01679151061173533,
      "loss": 0.0003,
      "step": 771
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.019399845972657204,
      "learning_rate": 0.016787349146899708,
      "loss": 0.2708,
      "step": 772
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.040895044803619385,
      "learning_rate": 0.016783187682064087,
      "loss": 0.4102,
      "step": 773
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.025216389447450638,
      "learning_rate": 0.016779026217228467,
      "loss": 0.3115,
      "step": 774
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.018137844279408455,
      "learning_rate": 0.016774864752392843,
      "loss": 0.2991,
      "step": 775
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.018322160467505455,
      "learning_rate": 0.016770703287557222,
      "loss": 0.1591,
      "step": 776
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.019225873053073883,
      "learning_rate": 0.0167665418227216,
      "loss": 0.283,
      "step": 777
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.01226904895156622,
      "learning_rate": 0.016762380357885975,
      "loss": 0.1577,
      "step": 778
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.003205365501344204,
      "learning_rate": 0.016758218893050354,
      "loss": 0.0049,
      "step": 779
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.022741660475730896,
      "learning_rate": 0.016754057428214734,
      "loss": 0.355,
      "step": 780
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.01638094149529934,
      "learning_rate": 0.01674989596337911,
      "loss": 0.2808,
      "step": 781
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.025196842849254608,
      "learning_rate": 0.01674573449854349,
      "loss": 0.6519,
      "step": 782
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.012154725380241871,
      "learning_rate": 0.016741573033707866,
      "loss": 0.1292,
      "step": 783
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.010786495171487331,
      "learning_rate": 0.01673741156887224,
      "loss": 0.0632,
      "step": 784
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.017939181998372078,
      "learning_rate": 0.01673325010403662,
      "loss": 0.2375,
      "step": 785
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.021562866866588593,
      "learning_rate": 0.016729088639201,
      "loss": 0.4019,
      "step": 786
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.01772688515484333,
      "learning_rate": 0.016724927174365377,
      "loss": 0.1752,
      "step": 787
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.017077792435884476,
      "learning_rate": 0.016720765709529756,
      "loss": 0.126,
      "step": 788
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.0011668333318084478,
      "learning_rate": 0.016716604244694133,
      "loss": 0.001,
      "step": 789
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.014091866090893745,
      "learning_rate": 0.01671244277985851,
      "loss": 0.1432,
      "step": 790
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.015959175303578377,
      "learning_rate": 0.016708281315022888,
      "loss": 0.3792,
      "step": 791
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.03273507580161095,
      "learning_rate": 0.016704119850187268,
      "loss": 0.4983,
      "step": 792
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.02138064242899418,
      "learning_rate": 0.016699958385351644,
      "loss": 0.5034,
      "step": 793
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.02423279918730259,
      "learning_rate": 0.016695796920516023,
      "loss": 0.1621,
      "step": 794
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.014763862825930119,
      "learning_rate": 0.0166916354556804,
      "loss": 0.127,
      "step": 795
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.021140828728675842,
      "learning_rate": 0.016687473990844776,
      "loss": 0.4648,
      "step": 796
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.016006823629140854,
      "learning_rate": 0.016683312526009155,
      "loss": 0.2067,
      "step": 797
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.01839813031256199,
      "learning_rate": 0.016679151061173535,
      "loss": 0.3167,
      "step": 798
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.017348794266581535,
      "learning_rate": 0.01667498959633791,
      "loss": 0.1687,
      "step": 799
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.0005906976875849068,
      "learning_rate": 0.01667082813150229,
      "loss": 0.0008,
      "step": 800
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.06840900331735611,
      "learning_rate": 0.016666666666666666,
      "loss": 0.4583,
      "step": 801
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.013588778674602509,
      "learning_rate": 0.016662505201831043,
      "loss": 0.4026,
      "step": 802
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.008593741804361343,
      "learning_rate": 0.016658343736995422,
      "loss": 0.0325,
      "step": 803
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.002787481527775526,
      "learning_rate": 0.0166541822721598,
      "loss": 0.0017,
      "step": 804
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.022920945659279823,
      "learning_rate": 0.016650020807324178,
      "loss": 0.4495,
      "step": 805
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.036856673657894135,
      "learning_rate": 0.016645859342488557,
      "loss": 1.2549,
      "step": 806
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.012767734006047249,
      "learning_rate": 0.016641697877652933,
      "loss": 0.3547,
      "step": 807
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.01914498209953308,
      "learning_rate": 0.01663753641281731,
      "loss": 0.2715,
      "step": 808
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.015170822851359844,
      "learning_rate": 0.01663337494798169,
      "loss": 0.261,
      "step": 809
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.020282737910747528,
      "learning_rate": 0.01662921348314607,
      "loss": 0.181,
      "step": 810
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.01731625199317932,
      "learning_rate": 0.016625052018310445,
      "loss": 0.2266,
      "step": 811
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.016341226175427437,
      "learning_rate": 0.016620890553474824,
      "loss": 0.1481,
      "step": 812
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.015706125646829605,
      "learning_rate": 0.0166167290886392,
      "loss": 0.3499,
      "step": 813
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.024310791864991188,
      "learning_rate": 0.01661256762380358,
      "loss": 0.2399,
      "step": 814
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.014865083619952202,
      "learning_rate": 0.016608406158967956,
      "loss": 0.2145,
      "step": 815
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.01781914383172989,
      "learning_rate": 0.016604244694132336,
      "loss": 0.3718,
      "step": 816
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.014720149338245392,
      "learning_rate": 0.01660008322929671,
      "loss": 0.3037,
      "step": 817
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.013470636680722237,
      "learning_rate": 0.01659592176446109,
      "loss": 0.0586,
      "step": 818
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.018091998994350433,
      "learning_rate": 0.016591760299625467,
      "loss": 0.3838,
      "step": 819
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.013013910502195358,
      "learning_rate": 0.016587598834789847,
      "loss": 0.1472,
      "step": 820
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.014777330681681633,
      "learning_rate": 0.016583437369954223,
      "loss": 0.1665,
      "step": 821
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.023757070302963257,
      "learning_rate": 0.016579275905118603,
      "loss": 0.6221,
      "step": 822
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.008868034929037094,
      "learning_rate": 0.01657511444028298,
      "loss": 0.0116,
      "step": 823
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.009676007553935051,
      "learning_rate": 0.016570952975447358,
      "loss": 0.0316,
      "step": 824
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.023485155776143074,
      "learning_rate": 0.016566791510611738,
      "loss": 0.3381,
      "step": 825
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.011961746960878372,
      "learning_rate": 0.016562630045776114,
      "loss": 0.039,
      "step": 826
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.07419464737176895,
      "learning_rate": 0.01655846858094049,
      "loss": 0.2639,
      "step": 827
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.026102904230356216,
      "learning_rate": 0.01655430711610487,
      "loss": 0.1685,
      "step": 828
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.01779910735785961,
      "learning_rate": 0.016550145651269246,
      "loss": 0.1917,
      "step": 829
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.01312586572021246,
      "learning_rate": 0.016545984186433625,
      "loss": 0.1495,
      "step": 830
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.023067975416779518,
      "learning_rate": 0.016541822721598005,
      "loss": 0.481,
      "step": 831
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.01735624298453331,
      "learning_rate": 0.01653766125676238,
      "loss": 0.235,
      "step": 832
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.022691868245601654,
      "learning_rate": 0.016533499791926757,
      "loss": 0.3594,
      "step": 833
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.02032083086669445,
      "learning_rate": 0.016529338327091136,
      "loss": 0.0811,
      "step": 834
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.011677929200232029,
      "learning_rate": 0.016525176862255513,
      "loss": 0.0311,
      "step": 835
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.0012912238016724586,
      "learning_rate": 0.016521015397419892,
      "loss": 0.0021,
      "step": 836
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.009877496398985386,
      "learning_rate": 0.01651685393258427,
      "loss": 0.0571,
      "step": 837
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.022816717624664307,
      "learning_rate": 0.016512692467748648,
      "loss": 0.2988,
      "step": 838
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.020957378670573235,
      "learning_rate": 0.016508531002913024,
      "loss": 0.3513,
      "step": 839
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.01921050064265728,
      "learning_rate": 0.016504369538077403,
      "loss": 0.3225,
      "step": 840
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.015538414008915424,
      "learning_rate": 0.016500208073241783,
      "loss": 0.2856,
      "step": 841
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.005771830677986145,
      "learning_rate": 0.01649604660840616,
      "loss": 0.0359,
      "step": 842
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.019735774025321007,
      "learning_rate": 0.01649188514357054,
      "loss": 0.2627,
      "step": 843
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.01537058874964714,
      "learning_rate": 0.016487723678734915,
      "loss": 0.1748,
      "step": 844
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.017002243548631668,
      "learning_rate": 0.01648356221389929,
      "loss": 0.208,
      "step": 845
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.011843948625028133,
      "learning_rate": 0.01647940074906367,
      "loss": 0.0623,
      "step": 846
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.013152760453522205,
      "learning_rate": 0.01647523928422805,
      "loss": 0.0202,
      "step": 847
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.01586482860147953,
      "learning_rate": 0.016471077819392426,
      "loss": 0.2474,
      "step": 848
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.021015800535678864,
      "learning_rate": 0.016466916354556806,
      "loss": 0.3752,
      "step": 849
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.01488250121474266,
      "learning_rate": 0.01646275488972118,
      "loss": 0.2264,
      "step": 850
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.01454408373683691,
      "learning_rate": 0.016458593424885558,
      "loss": 0.1473,
      "step": 851
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.021884668618440628,
      "learning_rate": 0.016454431960049937,
      "loss": 0.1755,
      "step": 852
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.024333517998456955,
      "learning_rate": 0.016450270495214317,
      "loss": 0.2444,
      "step": 853
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.025943482294678688,
      "learning_rate": 0.016446109030378693,
      "loss": 0.2568,
      "step": 854
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.02077079750597477,
      "learning_rate": 0.016441947565543073,
      "loss": 0.2686,
      "step": 855
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.009182083420455456,
      "learning_rate": 0.01643778610070745,
      "loss": 0.0546,
      "step": 856
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.027752196416258812,
      "learning_rate": 0.016433624635871828,
      "loss": 0.5513,
      "step": 857
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.028133325278759003,
      "learning_rate": 0.016429463171036204,
      "loss": 0.3528,
      "step": 858
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.01251843012869358,
      "learning_rate": 0.016425301706200584,
      "loss": 0.0686,
      "step": 859
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.020108910277485847,
      "learning_rate": 0.01642114024136496,
      "loss": 0.1724,
      "step": 860
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.02237214334309101,
      "learning_rate": 0.01641697877652934,
      "loss": 0.3689,
      "step": 861
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.020316746085882187,
      "learning_rate": 0.016412817311693716,
      "loss": 0.4507,
      "step": 862
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.0023943581618368626,
      "learning_rate": 0.016408655846858095,
      "loss": 0.0014,
      "step": 863
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.015778040513396263,
      "learning_rate": 0.01640449438202247,
      "loss": 0.1205,
      "step": 864
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.025118518620729446,
      "learning_rate": 0.01640033291718685,
      "loss": 0.4604,
      "step": 865
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.03616708144545555,
      "learning_rate": 0.016396171452351227,
      "loss": 0.2974,
      "step": 866
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.019496910274028778,
      "learning_rate": 0.016392009987515607,
      "loss": 0.3013,
      "step": 867
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.032156262546777725,
      "learning_rate": 0.016387848522679986,
      "loss": 0.4255,
      "step": 868
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.020332850515842438,
      "learning_rate": 0.016383687057844362,
      "loss": 0.1069,
      "step": 869
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.017447790130972862,
      "learning_rate": 0.01637952559300874,
      "loss": 0.3247,
      "step": 870
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.022485749796032906,
      "learning_rate": 0.016375364128173118,
      "loss": 0.335,
      "step": 871
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.016633182764053345,
      "learning_rate": 0.016371202663337494,
      "loss": 0.4094,
      "step": 872
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.022176867350935936,
      "learning_rate": 0.016367041198501874,
      "loss": 0.4968,
      "step": 873
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.023500703275203705,
      "learning_rate": 0.016362879733666253,
      "loss": 0.166,
      "step": 874
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.018345315009355545,
      "learning_rate": 0.01635871826883063,
      "loss": 0.134,
      "step": 875
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.020953403785824776,
      "learning_rate": 0.016354556803995005,
      "loss": 0.2262,
      "step": 876
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.019372910261154175,
      "learning_rate": 0.016350395339159385,
      "loss": 0.3599,
      "step": 877
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.01696900837123394,
      "learning_rate": 0.01634623387432376,
      "loss": 0.0999,
      "step": 878
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.02973988838493824,
      "learning_rate": 0.01634207240948814,
      "loss": 0.0488,
      "step": 879
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.01587802916765213,
      "learning_rate": 0.01633791094465252,
      "loss": 0.1545,
      "step": 880
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.013657576404511929,
      "learning_rate": 0.016333749479816896,
      "loss": 0.0683,
      "step": 881
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.01823948137462139,
      "learning_rate": 0.016329588014981272,
      "loss": 0.1791,
      "step": 882
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.020346296951174736,
      "learning_rate": 0.016325426550145652,
      "loss": 0.4141,
      "step": 883
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.011030556634068489,
      "learning_rate": 0.016321265085310028,
      "loss": 0.0443,
      "step": 884
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.019400158897042274,
      "learning_rate": 0.016317103620474407,
      "loss": 0.3291,
      "step": 885
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.019549163058400154,
      "learning_rate": 0.016312942155638787,
      "loss": 0.1863,
      "step": 886
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.018476836383342743,
      "learning_rate": 0.016308780690803163,
      "loss": 0.4438,
      "step": 887
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.025145960971713066,
      "learning_rate": 0.01630461922596754,
      "loss": 0.3721,
      "step": 888
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.01449675764888525,
      "learning_rate": 0.01630045776113192,
      "loss": 0.0373,
      "step": 889
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.025906091555953026,
      "learning_rate": 0.016296296296296295,
      "loss": 0.3435,
      "step": 890
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.02867051772773266,
      "learning_rate": 0.016292134831460674,
      "loss": 0.4929,
      "step": 891
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.004738725256174803,
      "learning_rate": 0.016287973366625054,
      "loss": 0.0032,
      "step": 892
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.017011573538184166,
      "learning_rate": 0.01628381190178943,
      "loss": 0.0698,
      "step": 893
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.02346462942659855,
      "learning_rate": 0.016279650436953806,
      "loss": 0.3755,
      "step": 894
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.01730034500360489,
      "learning_rate": 0.016275488972118186,
      "loss": 0.191,
      "step": 895
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.011661549098789692,
      "learning_rate": 0.016271327507282562,
      "loss": 0.0641,
      "step": 896
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.02210068143904209,
      "learning_rate": 0.01626716604244694,
      "loss": 0.2524,
      "step": 897
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.01413202378898859,
      "learning_rate": 0.01626300457761132,
      "loss": 0.1211,
      "step": 898
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.0009288507280871272,
      "learning_rate": 0.016258843112775697,
      "loss": 0.0004,
      "step": 899
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.7877373695373535,
      "learning_rate": 0.016254681647940077,
      "loss": 0.2069,
      "step": 900
    },
    {
      "epoch": 1.12,
      "eval_loss": 0.276611328125,
      "eval_runtime": 183.0948,
      "eval_samples_per_second": 1.098,
      "eval_steps_per_second": 0.552,
      "step": 900
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.026519136503338814,
      "learning_rate": 0.016250520183104453,
      "loss": 0.0829,
      "step": 901
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.03849050775170326,
      "learning_rate": 0.01624635871826883,
      "loss": 0.7671,
      "step": 902
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.018702959641814232,
      "learning_rate": 0.01624219725343321,
      "loss": 0.345,
      "step": 903
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.018841372802853584,
      "learning_rate": 0.016238035788597588,
      "loss": 0.2462,
      "step": 904
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.016912473365664482,
      "learning_rate": 0.016233874323761964,
      "loss": 0.1604,
      "step": 905
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.0141952745616436,
      "learning_rate": 0.016229712858926344,
      "loss": 0.0792,
      "step": 906
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.01381248701363802,
      "learning_rate": 0.01622555139409072,
      "loss": 0.101,
      "step": 907
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.021296629682183266,
      "learning_rate": 0.016221389929255096,
      "loss": 0.1182,
      "step": 908
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.02041725255548954,
      "learning_rate": 0.016217228464419475,
      "loss": 0.2544,
      "step": 909
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.014363840222358704,
      "learning_rate": 0.016213066999583855,
      "loss": 0.0547,
      "step": 910
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.028791187331080437,
      "learning_rate": 0.016208905534748234,
      "loss": 0.3252,
      "step": 911
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.020284051075577736,
      "learning_rate": 0.01620474406991261,
      "loss": 0.0414,
      "step": 912
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.01956641674041748,
      "learning_rate": 0.016200582605076987,
      "loss": 0.144,
      "step": 913
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.020686237141489983,
      "learning_rate": 0.016196421140241366,
      "loss": 0.2286,
      "step": 914
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.01920429617166519,
      "learning_rate": 0.016192259675405742,
      "loss": 0.1573,
      "step": 915
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.021561048924922943,
      "learning_rate": 0.016188098210570122,
      "loss": 0.5024,
      "step": 916
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.011054680682718754,
      "learning_rate": 0.0161839367457345,
      "loss": 0.0472,
      "step": 917
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.021469352766871452,
      "learning_rate": 0.016179775280898877,
      "loss": 0.1558,
      "step": 918
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.028537174686789513,
      "learning_rate": 0.016175613816063254,
      "loss": 0.3579,
      "step": 919
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.027993837371468544,
      "learning_rate": 0.016171452351227633,
      "loss": 0.5728,
      "step": 920
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.020287811756134033,
      "learning_rate": 0.01616729088639201,
      "loss": 0.3977,
      "step": 921
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.028396226465702057,
      "learning_rate": 0.01616312942155639,
      "loss": 0.2693,
      "step": 922
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.019021200016140938,
      "learning_rate": 0.01615896795672077,
      "loss": 0.2737,
      "step": 923
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.022501075640320778,
      "learning_rate": 0.016154806491885144,
      "loss": 0.2081,
      "step": 924
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.029850833117961884,
      "learning_rate": 0.01615064502704952,
      "loss": 0.4607,
      "step": 925
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.005168376490473747,
      "learning_rate": 0.0161464835622139,
      "loss": 0.0029,
      "step": 926
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.017600636929273605,
      "learning_rate": 0.016142322097378276,
      "loss": 0.2366,
      "step": 927
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.0022962039802223444,
      "learning_rate": 0.016138160632542656,
      "loss": 0.0027,
      "step": 928
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.025218483060598373,
      "learning_rate": 0.016133999167707035,
      "loss": 0.2805,
      "step": 929
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.0006191764841787517,
      "learning_rate": 0.01612983770287141,
      "loss": 0.0008,
      "step": 930
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.02196386642754078,
      "learning_rate": 0.016125676238035788,
      "loss": 0.1945,
      "step": 931
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.017645791172981262,
      "learning_rate": 0.016121514773200167,
      "loss": 0.3062,
      "step": 932
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.023195844143629074,
      "learning_rate": 0.016117353308364543,
      "loss": 0.2666,
      "step": 933
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.022655079141259193,
      "learning_rate": 0.016113191843528923,
      "loss": 0.1132,
      "step": 934
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.02040504291653633,
      "learning_rate": 0.016109030378693302,
      "loss": 0.3386,
      "step": 935
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.015803825110197067,
      "learning_rate": 0.01610486891385768,
      "loss": 0.3064,
      "step": 936
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.011600911617279053,
      "learning_rate": 0.016100707449022054,
      "loss": 0.0737,
      "step": 937
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.016042442992329597,
      "learning_rate": 0.016096545984186434,
      "loss": 0.1802,
      "step": 938
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.015973592177033424,
      "learning_rate": 0.01609238451935081,
      "loss": 0.2563,
      "step": 939
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.02279144898056984,
      "learning_rate": 0.01608822305451519,
      "loss": 0.1506,
      "step": 940
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.025131290778517723,
      "learning_rate": 0.01608406158967957,
      "loss": 0.3315,
      "step": 941
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.03296956792473793,
      "learning_rate": 0.016079900124843945,
      "loss": 0.3972,
      "step": 942
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.02016415260732174,
      "learning_rate": 0.016075738660008325,
      "loss": 0.3557,
      "step": 943
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.025722770020365715,
      "learning_rate": 0.0160715771951727,
      "loss": 0.1357,
      "step": 944
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.01275906153023243,
      "learning_rate": 0.016067415730337077,
      "loss": 0.2053,
      "step": 945
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.007159693632274866,
      "learning_rate": 0.016063254265501457,
      "loss": 0.0319,
      "step": 946
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.012666679918766022,
      "learning_rate": 0.016059092800665836,
      "loss": 0.0644,
      "step": 947
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.023768091574311256,
      "learning_rate": 0.016054931335830212,
      "loss": 0.3425,
      "step": 948
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.018956877291202545,
      "learning_rate": 0.016050769870994592,
      "loss": 0.205,
      "step": 949
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.022229792550206184,
      "learning_rate": 0.016046608406158968,
      "loss": 0.2661,
      "step": 950
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.02749771997332573,
      "learning_rate": 0.016042446941323344,
      "loss": 0.3875,
      "step": 951
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.016075553372502327,
      "learning_rate": 0.016038285476487724,
      "loss": 0.2316,
      "step": 952
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.013823479413986206,
      "learning_rate": 0.016034124011652103,
      "loss": 0.2084,
      "step": 953
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.018673235550522804,
      "learning_rate": 0.01602996254681648,
      "loss": 0.2793,
      "step": 954
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.00583814550191164,
      "learning_rate": 0.01602580108198086,
      "loss": 0.0182,
      "step": 955
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.03060997650027275,
      "learning_rate": 0.016021639617145235,
      "loss": 0.4751,
      "step": 956
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.02101307176053524,
      "learning_rate": 0.01601747815230961,
      "loss": 0.4214,
      "step": 957
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.011083472520112991,
      "learning_rate": 0.01601331668747399,
      "loss": 0.1252,
      "step": 958
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.027596939355134964,
      "learning_rate": 0.01600915522263837,
      "loss": 0.2883,
      "step": 959
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.022441310808062553,
      "learning_rate": 0.016004993757802746,
      "loss": 0.304,
      "step": 960
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.0004867761745117605,
      "learning_rate": 0.016000832292967126,
      "loss": 0.0007,
      "step": 961
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.021003400906920433,
      "learning_rate": 0.015996670828131502,
      "loss": 0.4141,
      "step": 962
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.01835128851234913,
      "learning_rate": 0.015992509363295878,
      "loss": 0.3289,
      "step": 963
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.015276538208127022,
      "learning_rate": 0.015988347898460258,
      "loss": 0.1052,
      "step": 964
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.009803126566112041,
      "learning_rate": 0.015984186433624637,
      "loss": 0.0461,
      "step": 965
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.02007918432354927,
      "learning_rate": 0.015980024968789013,
      "loss": 0.074,
      "step": 966
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.020967651158571243,
      "learning_rate": 0.015975863503953393,
      "loss": 0.4387,
      "step": 967
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.016827093437314034,
      "learning_rate": 0.01597170203911777,
      "loss": 0.0786,
      "step": 968
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.015784306451678276,
      "learning_rate": 0.015967540574282145,
      "loss": 0.244,
      "step": 969
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.015032459981739521,
      "learning_rate": 0.015963379109446525,
      "loss": 0.172,
      "step": 970
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.011052594520151615,
      "learning_rate": 0.015959217644610904,
      "loss": 0.0458,
      "step": 971
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.03352367877960205,
      "learning_rate": 0.01595505617977528,
      "loss": 0.5415,
      "step": 972
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.015644092112779617,
      "learning_rate": 0.01595089471493966,
      "loss": 0.0603,
      "step": 973
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.00045275763841345906,
      "learning_rate": 0.015946733250104036,
      "loss": 0.0006,
      "step": 974
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.01635799929499626,
      "learning_rate": 0.015942571785268415,
      "loss": 0.3152,
      "step": 975
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.021508242934942245,
      "learning_rate": 0.01593841032043279,
      "loss": 0.2922,
      "step": 976
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.021619176492094994,
      "learning_rate": 0.01593424885559717,
      "loss": 0.3474,
      "step": 977
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.019374271854758263,
      "learning_rate": 0.015930087390761547,
      "loss": 0.4309,
      "step": 978
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.021594762802124023,
      "learning_rate": 0.015925925925925927,
      "loss": 0.3157,
      "step": 979
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.02327478863298893,
      "learning_rate": 0.015921764461090303,
      "loss": 0.24,
      "step": 980
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.030785227194428444,
      "learning_rate": 0.015917602996254682,
      "loss": 0.6362,
      "step": 981
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.019039804115891457,
      "learning_rate": 0.01591344153141906,
      "loss": 0.283,
      "step": 982
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.014343260787427425,
      "learning_rate": 0.015909280066583438,
      "loss": 0.1544,
      "step": 983
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.029103873297572136,
      "learning_rate": 0.015905118601747818,
      "loss": 0.4111,
      "step": 984
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.017416486516594887,
      "learning_rate": 0.015900957136912194,
      "loss": 0.1909,
      "step": 985
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.017740193754434586,
      "learning_rate": 0.015896795672076573,
      "loss": 0.2529,
      "step": 986
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.008261576294898987,
      "learning_rate": 0.01589263420724095,
      "loss": 0.0497,
      "step": 987
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.020972803235054016,
      "learning_rate": 0.015888472742405325,
      "loss": 0.271,
      "step": 988
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.013603289611637592,
      "learning_rate": 0.015884311277569705,
      "loss": 0.1375,
      "step": 989
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.013080689124763012,
      "learning_rate": 0.015880149812734085,
      "loss": 0.0198,
      "step": 990
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.014577233232557774,
      "learning_rate": 0.01587598834789846,
      "loss": 0.2607,
      "step": 991
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.016401128843426704,
      "learning_rate": 0.01587182688306284,
      "loss": 0.132,
      "step": 992
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.024706076830625534,
      "learning_rate": 0.015867665418227216,
      "loss": 0.3445,
      "step": 993
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.038260236382484436,
      "learning_rate": 0.015863503953391592,
      "loss": 0.3057,
      "step": 994
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.016283279284834862,
      "learning_rate": 0.015859342488555972,
      "loss": 0.353,
      "step": 995
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.025337805971503258,
      "learning_rate": 0.01585518102372035,
      "loss": 0.1176,
      "step": 996
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.019213566556572914,
      "learning_rate": 0.015851019558884728,
      "loss": 0.2363,
      "step": 997
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.025822242721915245,
      "learning_rate": 0.015846858094049107,
      "loss": 0.3206,
      "step": 998
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.012141147628426552,
      "learning_rate": 0.015842696629213483,
      "loss": 0.0931,
      "step": 999
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.020724572241306305,
      "learning_rate": 0.01583853516437786,
      "loss": 0.209,
      "step": 1000
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.02043132483959198,
      "learning_rate": 0.01583437369954224,
      "loss": 0.3499,
      "step": 1001
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.012707555666565895,
      "learning_rate": 0.01583021223470662,
      "loss": 0.2217,
      "step": 1002
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.023316146805882454,
      "learning_rate": 0.015826050769870995,
      "loss": 0.3777,
      "step": 1003
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.015011832118034363,
      "learning_rate": 0.015821889305035374,
      "loss": 0.0135,
      "step": 1004
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.0184890478849411,
      "learning_rate": 0.01581772784019975,
      "loss": 0.1482,
      "step": 1005
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.01736568473279476,
      "learning_rate": 0.015813566375364126,
      "loss": 0.3743,
      "step": 1006
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.027601389214396477,
      "learning_rate": 0.015809404910528506,
      "loss": 0.3777,
      "step": 1007
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.032839711755514145,
      "learning_rate": 0.015805243445692885,
      "loss": 0.4526,
      "step": 1008
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.02121092565357685,
      "learning_rate": 0.01580108198085726,
      "loss": 0.3813,
      "step": 1009
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.019736194983124733,
      "learning_rate": 0.01579692051602164,
      "loss": 0.2817,
      "step": 1010
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.013008872978389263,
      "learning_rate": 0.015792759051186017,
      "loss": 0.1169,
      "step": 1011
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.025218408554792404,
      "learning_rate": 0.015788597586350393,
      "loss": 0.5791,
      "step": 1012
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.00462852418422699,
      "learning_rate": 0.015784436121514773,
      "loss": 0.0042,
      "step": 1013
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.019213877618312836,
      "learning_rate": 0.015780274656679152,
      "loss": 0.2571,
      "step": 1014
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.019731277599930763,
      "learning_rate": 0.01577611319184353,
      "loss": 0.4185,
      "step": 1015
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.019622253254055977,
      "learning_rate": 0.015771951727007908,
      "loss": 0.1412,
      "step": 1016
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.02248537167906761,
      "learning_rate": 0.015767790262172284,
      "loss": 0.3145,
      "step": 1017
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.023821372538805008,
      "learning_rate": 0.015763628797336664,
      "loss": 0.1793,
      "step": 1018
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.017169175669550896,
      "learning_rate": 0.01575946733250104,
      "loss": 0.0958,
      "step": 1019
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.02006416767835617,
      "learning_rate": 0.01575530586766542,
      "loss": 0.4849,
      "step": 1020
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.015900978818535805,
      "learning_rate": 0.015751144402829795,
      "loss": 0.1081,
      "step": 1021
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.019144194200634956,
      "learning_rate": 0.015746982937994175,
      "loss": 0.1746,
      "step": 1022
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.020386679098010063,
      "learning_rate": 0.01574282147315855,
      "loss": 0.3347,
      "step": 1023
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.0170699842274189,
      "learning_rate": 0.01573866000832293,
      "loss": 0.1436,
      "step": 1024
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.024188198149204254,
      "learning_rate": 0.015734498543487307,
      "loss": 0.415,
      "step": 1025
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.02380228601396084,
      "learning_rate": 0.015730337078651686,
      "loss": 0.7476,
      "step": 1026
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.02469363808631897,
      "learning_rate": 0.015726175613816062,
      "loss": 0.2203,
      "step": 1027
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.023834528401494026,
      "learning_rate": 0.015722014148980442,
      "loss": 0.1089,
      "step": 1028
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.015983764082193375,
      "learning_rate": 0.01571785268414482,
      "loss": 0.208,
      "step": 1029
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.01997104473412037,
      "learning_rate": 0.015713691219309198,
      "loss": 0.2747,
      "step": 1030
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.017408501356840134,
      "learning_rate": 0.015709529754473574,
      "loss": 0.1409,
      "step": 1031
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.023084178566932678,
      "learning_rate": 0.015705368289637953,
      "loss": 0.4807,
      "step": 1032
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.013663158752024174,
      "learning_rate": 0.01570120682480233,
      "loss": 0.0727,
      "step": 1033
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.023880543187260628,
      "learning_rate": 0.01569704535996671,
      "loss": 0.4978,
      "step": 1034
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.015810102224349976,
      "learning_rate": 0.01569288389513109,
      "loss": 0.1748,
      "step": 1035
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.016030754894018173,
      "learning_rate": 0.015688722430295465,
      "loss": 0.0733,
      "step": 1036
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.02104434370994568,
      "learning_rate": 0.01568456096545984,
      "loss": 0.2396,
      "step": 1037
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.023493843153119087,
      "learning_rate": 0.01568039950062422,
      "loss": 0.3794,
      "step": 1038
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.020526286214590073,
      "learning_rate": 0.015676238035788596,
      "loss": 0.2996,
      "step": 1039
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.01850873976945877,
      "learning_rate": 0.015672076570952976,
      "loss": 0.2571,
      "step": 1040
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.015737837180495262,
      "learning_rate": 0.015667915106117355,
      "loss": 0.1031,
      "step": 1041
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.022054312750697136,
      "learning_rate": 0.01566375364128173,
      "loss": 0.0681,
      "step": 1042
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.044910941272974014,
      "learning_rate": 0.015659592176446108,
      "loss": 0.2883,
      "step": 1043
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.012114339508116245,
      "learning_rate": 0.015655430711610487,
      "loss": 0.0986,
      "step": 1044
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.019139524549245834,
      "learning_rate": 0.015651269246774863,
      "loss": 0.1636,
      "step": 1045
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.015495910309255123,
      "learning_rate": 0.015647107781939243,
      "loss": 0.1262,
      "step": 1046
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.023599015548825264,
      "learning_rate": 0.015642946317103622,
      "loss": 0.2786,
      "step": 1047
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.02580740861594677,
      "learning_rate": 0.015638784852268,
      "loss": 0.184,
      "step": 1048
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.028933294117450714,
      "learning_rate": 0.015634623387432375,
      "loss": 0.4736,
      "step": 1049
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.04469485208392143,
      "learning_rate": 0.015630461922596754,
      "loss": 0.251,
      "step": 1050
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.02135387994349003,
      "learning_rate": 0.01562630045776113,
      "loss": 0.252,
      "step": 1051
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.014715808443725109,
      "learning_rate": 0.01562213899292551,
      "loss": 0.4316,
      "step": 1052
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.017467500641942024,
      "learning_rate": 0.015617977528089888,
      "loss": 0.2715,
      "step": 1053
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.017933281138539314,
      "learning_rate": 0.015613816063254266,
      "loss": 0.0946,
      "step": 1054
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.019199233502149582,
      "learning_rate": 0.015609654598418643,
      "loss": 0.1135,
      "step": 1055
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.02765505202114582,
      "learning_rate": 0.015605493133583021,
      "loss": 0.4438,
      "step": 1056
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.02297116257250309,
      "learning_rate": 0.0156013316687474,
      "loss": 0.1573,
      "step": 1057
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.016932332888245583,
      "learning_rate": 0.015597170203911777,
      "loss": 0.2242,
      "step": 1058
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.03125903010368347,
      "learning_rate": 0.015593008739076155,
      "loss": 0.3635,
      "step": 1059
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.02059970609843731,
      "learning_rate": 0.015588847274240534,
      "loss": 0.3572,
      "step": 1060
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.022705474868416786,
      "learning_rate": 0.01558468580940491,
      "loss": 0.2847,
      "step": 1061
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.01579454354941845,
      "learning_rate": 0.01558052434456929,
      "loss": 0.1407,
      "step": 1062
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.0002887519949581474,
      "learning_rate": 0.015576362879733668,
      "loss": 0.0004,
      "step": 1063
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.019719930365681648,
      "learning_rate": 0.015572201414898044,
      "loss": 0.3406,
      "step": 1064
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.021523548290133476,
      "learning_rate": 0.015568039950062423,
      "loss": 0.2588,
      "step": 1065
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.02310948260128498,
      "learning_rate": 0.015563878485226801,
      "loss": 0.1113,
      "step": 1066
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.012330864556133747,
      "learning_rate": 0.015559717020391177,
      "loss": 0.1165,
      "step": 1067
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.010723087936639786,
      "learning_rate": 0.015555555555555557,
      "loss": 0.0454,
      "step": 1068
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.020736204460263252,
      "learning_rate": 0.015551394090719935,
      "loss": 0.2084,
      "step": 1069
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.02228960394859314,
      "learning_rate": 0.01554723262588431,
      "loss": 0.4268,
      "step": 1070
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.019444720819592476,
      "learning_rate": 0.01554307116104869,
      "loss": 0.2075,
      "step": 1071
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.012051189318299294,
      "learning_rate": 0.015538909696213068,
      "loss": 0.0643,
      "step": 1072
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.008103182539343834,
      "learning_rate": 0.015534748231377444,
      "loss": 0.0317,
      "step": 1073
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.01383668277412653,
      "learning_rate": 0.015530586766541824,
      "loss": 0.1681,
      "step": 1074
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.02172127552330494,
      "learning_rate": 0.015526425301706202,
      "loss": 0.3667,
      "step": 1075
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.024141397327184677,
      "learning_rate": 0.015522263836870578,
      "loss": 0.3577,
      "step": 1076
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.0225079245865345,
      "learning_rate": 0.015518102372034957,
      "loss": 0.4368,
      "step": 1077
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.018179163336753845,
      "learning_rate": 0.015513940907199335,
      "loss": 0.1593,
      "step": 1078
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.018162671476602554,
      "learning_rate": 0.015509779442363711,
      "loss": 0.4954,
      "step": 1079
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.019146595150232315,
      "learning_rate": 0.01550561797752809,
      "loss": 0.2988,
      "step": 1080
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.014852888882160187,
      "learning_rate": 0.015501456512692469,
      "loss": 0.0483,
      "step": 1081
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.019325925037264824,
      "learning_rate": 0.015497295047856845,
      "loss": 0.2666,
      "step": 1082
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.014204607345163822,
      "learning_rate": 0.015493133583021224,
      "loss": 0.1718,
      "step": 1083
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.01749173365533352,
      "learning_rate": 0.015488972118185602,
      "loss": 0.2151,
      "step": 1084
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.021085506305098534,
      "learning_rate": 0.015484810653349978,
      "loss": 0.1967,
      "step": 1085
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.017188740894198418,
      "learning_rate": 0.015480649188514358,
      "loss": 0.1759,
      "step": 1086
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.02077721245586872,
      "learning_rate": 0.015476487723678736,
      "loss": 0.141,
      "step": 1087
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.028460413217544556,
      "learning_rate": 0.015472326258843112,
      "loss": 0.1639,
      "step": 1088
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.016213104128837585,
      "learning_rate": 0.015468164794007491,
      "loss": 0.2114,
      "step": 1089
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.024326445534825325,
      "learning_rate": 0.015464003329171869,
      "loss": 0.2467,
      "step": 1090
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.026028770953416824,
      "learning_rate": 0.015459841864336245,
      "loss": 0.6675,
      "step": 1091
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.016207613050937653,
      "learning_rate": 0.015455680399500625,
      "loss": 0.2339,
      "step": 1092
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.019064977765083313,
      "learning_rate": 0.015451518934665003,
      "loss": 0.23,
      "step": 1093
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.015190163627266884,
      "learning_rate": 0.01544735746982938,
      "loss": 0.1476,
      "step": 1094
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.01640726625919342,
      "learning_rate": 0.015443196004993758,
      "loss": 0.2102,
      "step": 1095
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.02259083464741707,
      "learning_rate": 0.015439034540158136,
      "loss": 0.2588,
      "step": 1096
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.020244941115379333,
      "learning_rate": 0.015434873075322514,
      "loss": 0.2026,
      "step": 1097
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.0171633493155241,
      "learning_rate": 0.015430711610486892,
      "loss": 0.1781,
      "step": 1098
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.002251014579087496,
      "learning_rate": 0.01542655014565127,
      "loss": 0.0016,
      "step": 1099
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.023644737899303436,
      "learning_rate": 0.015422388680815647,
      "loss": 0.3811,
      "step": 1100
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.01999722421169281,
      "learning_rate": 0.015418227215980025,
      "loss": 0.3342,
      "step": 1101
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.01838837005198002,
      "learning_rate": 0.015414065751144403,
      "loss": 0.3562,
      "step": 1102
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.01500898040831089,
      "learning_rate": 0.01540990428630878,
      "loss": 0.3335,
      "step": 1103
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.01809258759021759,
      "learning_rate": 0.015405742821473159,
      "loss": 0.1715,
      "step": 1104
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.02568463236093521,
      "learning_rate": 0.015401581356637538,
      "loss": 0.3784,
      "step": 1105
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.009081849828362465,
      "learning_rate": 0.015397419891801914,
      "loss": 0.0245,
      "step": 1106
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.023000378161668777,
      "learning_rate": 0.015393258426966292,
      "loss": 0.2214,
      "step": 1107
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.02157086692750454,
      "learning_rate": 0.015389096962130672,
      "loss": 0.2255,
      "step": 1108
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.018225355073809624,
      "learning_rate": 0.015384935497295048,
      "loss": 0.2051,
      "step": 1109
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.034307777881622314,
      "learning_rate": 0.015380774032459426,
      "loss": 0.4041,
      "step": 1110
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.01195373386144638,
      "learning_rate": 0.015376612567623805,
      "loss": 0.0962,
      "step": 1111
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.01943417638540268,
      "learning_rate": 0.015372451102788181,
      "loss": 0.1947,
      "step": 1112
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.016698267310857773,
      "learning_rate": 0.015368289637952559,
      "loss": 0.1603,
      "step": 1113
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.023589296266436577,
      "learning_rate": 0.015364128173116939,
      "loss": 0.5039,
      "step": 1114
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.010582692921161652,
      "learning_rate": 0.015359966708281315,
      "loss": 0.0792,
      "step": 1115
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.01984834484755993,
      "learning_rate": 0.015355805243445693,
      "loss": 0.1484,
      "step": 1116
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.009592410176992416,
      "learning_rate": 0.015351643778610072,
      "loss": 0.0854,
      "step": 1117
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.018000148236751556,
      "learning_rate": 0.015347482313774448,
      "loss": 0.1635,
      "step": 1118
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.023731786757707596,
      "learning_rate": 0.015343320848938826,
      "loss": 0.2798,
      "step": 1119
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.0160891804844141,
      "learning_rate": 0.015339159384103206,
      "loss": 0.3643,
      "step": 1120
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.02015240676701069,
      "learning_rate": 0.015334997919267582,
      "loss": 0.239,
      "step": 1121
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.014112010598182678,
      "learning_rate": 0.01533083645443196,
      "loss": 0.149,
      "step": 1122
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.026829784736037254,
      "learning_rate": 0.015326674989596339,
      "loss": 0.3792,
      "step": 1123
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.015016230754554272,
      "learning_rate": 0.015322513524760715,
      "loss": 0.2939,
      "step": 1124
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.0005421704263426363,
      "learning_rate": 0.015318352059925093,
      "loss": 0.0006,
      "step": 1125
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.025483932346105576,
      "learning_rate": 0.015314190595089473,
      "loss": 0.5596,
      "step": 1126
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.015506415627896786,
      "learning_rate": 0.015310029130253849,
      "loss": 0.2438,
      "step": 1127
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.014859020709991455,
      "learning_rate": 0.015305867665418227,
      "loss": 0.3511,
      "step": 1128
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.017689988017082214,
      "learning_rate": 0.015301706200582606,
      "loss": 0.1792,
      "step": 1129
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.00029732234543189406,
      "learning_rate": 0.015297544735746984,
      "loss": 0.0004,
      "step": 1130
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.02316972427070141,
      "learning_rate": 0.01529338327091136,
      "loss": 0.5171,
      "step": 1131
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.021559979766607285,
      "learning_rate": 0.01528922180607574,
      "loss": 0.3516,
      "step": 1132
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.029548246413469315,
      "learning_rate": 0.015285060341240117,
      "loss": 0.585,
      "step": 1133
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.021956736221909523,
      "learning_rate": 0.015280898876404493,
      "loss": 0.1686,
      "step": 1134
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.018229397013783455,
      "learning_rate": 0.015276737411568873,
      "loss": 0.2147,
      "step": 1135
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.01827104762196541,
      "learning_rate": 0.01527257594673325,
      "loss": 0.1564,
      "step": 1136
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.012026326730847359,
      "learning_rate": 0.015268414481897629,
      "loss": 0.0656,
      "step": 1137
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.02198140136897564,
      "learning_rate": 0.015264253017062007,
      "loss": 0.3943,
      "step": 1138
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.02491358108818531,
      "learning_rate": 0.015260091552226384,
      "loss": 0.1785,
      "step": 1139
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.054076824337244034,
      "learning_rate": 0.015255930087390762,
      "loss": 0.4741,
      "step": 1140
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.02986018918454647,
      "learning_rate": 0.01525176862255514,
      "loss": 0.191,
      "step": 1141
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.017277082428336143,
      "learning_rate": 0.015247607157719518,
      "loss": 0.1667,
      "step": 1142
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.030948398634791374,
      "learning_rate": 0.015243445692883896,
      "loss": 0.2549,
      "step": 1143
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.02342919073998928,
      "learning_rate": 0.015239284228048273,
      "loss": 0.0709,
      "step": 1144
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.023284493014216423,
      "learning_rate": 0.015235122763212651,
      "loss": 0.2993,
      "step": 1145
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.018761197105050087,
      "learning_rate": 0.01523096129837703,
      "loss": 0.1833,
      "step": 1146
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.01601211167871952,
      "learning_rate": 0.015226799833541407,
      "loss": 0.1031,
      "step": 1147
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.023344509303569794,
      "learning_rate": 0.015222638368705787,
      "loss": 0.4573,
      "step": 1148
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.016253603622317314,
      "learning_rate": 0.015218476903870163,
      "loss": 0.1138,
      "step": 1149
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.015798864886164665,
      "learning_rate": 0.01521431543903454,
      "loss": 0.0936,
      "step": 1150
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.006655659060925245,
      "learning_rate": 0.01521015397419892,
      "loss": 0.0236,
      "step": 1151
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.015202687121927738,
      "learning_rate": 0.015205992509363296,
      "loss": 0.2206,
      "step": 1152
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.013044016435742378,
      "learning_rate": 0.015201831044527674,
      "loss": 0.11,
      "step": 1153
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.01914636231958866,
      "learning_rate": 0.015197669579692053,
      "loss": 0.2402,
      "step": 1154
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.014704947359859943,
      "learning_rate": 0.01519350811485643,
      "loss": 0.0462,
      "step": 1155
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.01785224862396717,
      "learning_rate": 0.015189346650020807,
      "loss": 0.2172,
      "step": 1156
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.033810075372457504,
      "learning_rate": 0.015185185185185187,
      "loss": 0.6226,
      "step": 1157
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.014413599856197834,
      "learning_rate": 0.015181023720349563,
      "loss": 0.1069,
      "step": 1158
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.02114710584282875,
      "learning_rate": 0.015176862255513941,
      "loss": 0.269,
      "step": 1159
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.025646749883890152,
      "learning_rate": 0.01517270079067832,
      "loss": 0.0264,
      "step": 1160
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.025219501927495003,
      "learning_rate": 0.015168539325842697,
      "loss": 0.2483,
      "step": 1161
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.019140703603625298,
      "learning_rate": 0.015164377861007074,
      "loss": 0.1626,
      "step": 1162
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.010834680870175362,
      "learning_rate": 0.015160216396171454,
      "loss": 0.0604,
      "step": 1163
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.0206084493547678,
      "learning_rate": 0.01515605493133583,
      "loss": 0.1979,
      "step": 1164
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.015438029542565346,
      "learning_rate": 0.015151893466500208,
      "loss": 0.0132,
      "step": 1165
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.014139370061457157,
      "learning_rate": 0.015147732001664587,
      "loss": 0.0623,
      "step": 1166
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.008177504874765873,
      "learning_rate": 0.015143570536828964,
      "loss": 0.0326,
      "step": 1167
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.018727127462625504,
      "learning_rate": 0.015139409071993341,
      "loss": 0.1654,
      "step": 1168
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.008063145913183689,
      "learning_rate": 0.015135247607157721,
      "loss": 0.036,
      "step": 1169
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.010104950517416,
      "learning_rate": 0.015131086142322097,
      "loss": 0.0698,
      "step": 1170
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.03561998903751373,
      "learning_rate": 0.015126924677486475,
      "loss": 0.3223,
      "step": 1171
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.018008321523666382,
      "learning_rate": 0.015122763212650854,
      "loss": 0.1576,
      "step": 1172
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.027542300522327423,
      "learning_rate": 0.01511860174781523,
      "loss": 0.3191,
      "step": 1173
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.01982768438756466,
      "learning_rate": 0.015114440282979608,
      "loss": 0.2742,
      "step": 1174
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.025857718661427498,
      "learning_rate": 0.015110278818143988,
      "loss": 0.3403,
      "step": 1175
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.016640814021229744,
      "learning_rate": 0.015106117353308364,
      "loss": 0.0975,
      "step": 1176
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.024543974548578262,
      "learning_rate": 0.015101955888472742,
      "loss": 0.1842,
      "step": 1177
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.01926487311720848,
      "learning_rate": 0.015097794423637121,
      "loss": 0.2856,
      "step": 1178
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.03251056373119354,
      "learning_rate": 0.015093632958801497,
      "loss": 0.8428,
      "step": 1179
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.07430959492921829,
      "learning_rate": 0.015089471493965877,
      "loss": 0.2742,
      "step": 1180
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.01837875507771969,
      "learning_rate": 0.015085310029130255,
      "loss": 0.1376,
      "step": 1181
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.018673159182071686,
      "learning_rate": 0.015081148564294631,
      "loss": 0.3821,
      "step": 1182
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.014461681246757507,
      "learning_rate": 0.01507698709945901,
      "loss": 0.1954,
      "step": 1183
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.023188557475805283,
      "learning_rate": 0.015072825634623388,
      "loss": 0.5127,
      "step": 1184
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.013737804256379604,
      "learning_rate": 0.015068664169787764,
      "loss": 0.0552,
      "step": 1185
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.007570568937808275,
      "learning_rate": 0.015064502704952144,
      "loss": 0.028,
      "step": 1186
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.01930762454867363,
      "learning_rate": 0.015060341240116522,
      "loss": 0.1311,
      "step": 1187
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.01913153938949108,
      "learning_rate": 0.015056179775280898,
      "loss": 0.1185,
      "step": 1188
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.035232484340667725,
      "learning_rate": 0.015052018310445277,
      "loss": 0.3857,
      "step": 1189
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.020691903308033943,
      "learning_rate": 0.015047856845609655,
      "loss": 0.3433,
      "step": 1190
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.029478132724761963,
      "learning_rate": 0.015043695380774031,
      "loss": 0.3596,
      "step": 1191
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.009989960119128227,
      "learning_rate": 0.015039533915938411,
      "loss": 0.0442,
      "step": 1192
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.021565720438957214,
      "learning_rate": 0.015035372451102789,
      "loss": 0.1177,
      "step": 1193
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.02209567278623581,
      "learning_rate": 0.015031210986267165,
      "loss": 0.0751,
      "step": 1194
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.029796341434121132,
      "learning_rate": 0.015027049521431544,
      "loss": 0.4351,
      "step": 1195
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.022179480642080307,
      "learning_rate": 0.015022888056595922,
      "loss": 0.3831,
      "step": 1196
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.012408039532601833,
      "learning_rate": 0.015018726591760298,
      "loss": 0.081,
      "step": 1197
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.041155047714710236,
      "learning_rate": 0.015014565126924678,
      "loss": 0.2688,
      "step": 1198
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.011901192367076874,
      "learning_rate": 0.015010403662089056,
      "loss": 0.0905,
      "step": 1199
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.03587112948298454,
      "learning_rate": 0.015006242197253432,
      "loss": 0.3569,
      "step": 1200
    },
    {
      "epoch": 1.5,
      "eval_loss": 0.262939453125,
      "eval_runtime": 183.0744,
      "eval_samples_per_second": 1.098,
      "eval_steps_per_second": 0.552,
      "step": 1200
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.02689652144908905,
      "learning_rate": 0.015002080732417811,
      "loss": 0.4983,
      "step": 1201
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.004521407186985016,
      "learning_rate": 0.01499791926758219,
      "loss": 0.0021,
      "step": 1202
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.008102629333734512,
      "learning_rate": 0.014993757802746569,
      "loss": 0.02,
      "step": 1203
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.02285955473780632,
      "learning_rate": 0.014989596337910945,
      "loss": 0.3142,
      "step": 1204
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.006106268614530563,
      "learning_rate": 0.014985434873075323,
      "loss": 0.0276,
      "step": 1205
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.015486959367990494,
      "learning_rate": 0.014981273408239702,
      "loss": 0.1589,
      "step": 1206
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.02652638964354992,
      "learning_rate": 0.014977111943404078,
      "loss": 0.3564,
      "step": 1207
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.0476614311337471,
      "learning_rate": 0.014972950478568456,
      "loss": 0.3833,
      "step": 1208
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.014729066751897335,
      "learning_rate": 0.014968789013732836,
      "loss": 0.0376,
      "step": 1209
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.015644939616322517,
      "learning_rate": 0.014964627548897212,
      "loss": 0.1006,
      "step": 1210
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.01606246456503868,
      "learning_rate": 0.01496046608406159,
      "loss": 0.2074,
      "step": 1211
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.014171753078699112,
      "learning_rate": 0.01495630461922597,
      "loss": 0.0427,
      "step": 1212
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.025123046711087227,
      "learning_rate": 0.014952143154390345,
      "loss": 0.334,
      "step": 1213
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.02304987423121929,
      "learning_rate": 0.014947981689554723,
      "loss": 0.3376,
      "step": 1214
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.014634921215474606,
      "learning_rate": 0.014943820224719103,
      "loss": 0.1454,
      "step": 1215
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.005565658211708069,
      "learning_rate": 0.014939658759883479,
      "loss": 0.0037,
      "step": 1216
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.014487390406429768,
      "learning_rate": 0.014935497295047857,
      "loss": 0.0923,
      "step": 1217
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.011326639913022518,
      "learning_rate": 0.014931335830212236,
      "loss": 0.134,
      "step": 1218
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.06571507453918457,
      "learning_rate": 0.014927174365376612,
      "loss": 0.3564,
      "step": 1219
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.02080959640443325,
      "learning_rate": 0.01492301290054099,
      "loss": 0.2401,
      "step": 1220
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.01263508666306734,
      "learning_rate": 0.01491885143570537,
      "loss": 0.0681,
      "step": 1221
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.018080338835716248,
      "learning_rate": 0.014914689970869746,
      "loss": 0.1833,
      "step": 1222
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.03379977121949196,
      "learning_rate": 0.014910528506034124,
      "loss": 0.1562,
      "step": 1223
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.009423679672181606,
      "learning_rate": 0.014906367041198503,
      "loss": 0.0558,
      "step": 1224
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.020535599440336227,
      "learning_rate": 0.01490220557636288,
      "loss": 0.3567,
      "step": 1225
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.028150491416454315,
      "learning_rate": 0.014898044111527259,
      "loss": 0.3809,
      "step": 1226
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.025025155395269394,
      "learning_rate": 0.014893882646691637,
      "loss": 0.2991,
      "step": 1227
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.014737282879650593,
      "learning_rate": 0.014889721181856013,
      "loss": 0.2094,
      "step": 1228
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.009373722597956657,
      "learning_rate": 0.014885559717020392,
      "loss": 0.0489,
      "step": 1229
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.010164734907448292,
      "learning_rate": 0.01488139825218477,
      "loss": 0.015,
      "step": 1230
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.02192559652030468,
      "learning_rate": 0.014877236787349146,
      "loss": 0.3835,
      "step": 1231
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.017824064940214157,
      "learning_rate": 0.014873075322513526,
      "loss": 0.1919,
      "step": 1232
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.016246765851974487,
      "learning_rate": 0.014868913857677904,
      "loss": 0.1741,
      "step": 1233
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.018382525071501732,
      "learning_rate": 0.01486475239284228,
      "loss": 0.3159,
      "step": 1234
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.029316797852516174,
      "learning_rate": 0.01486059092800666,
      "loss": 0.5205,
      "step": 1235
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.01960177719593048,
      "learning_rate": 0.014856429463171037,
      "loss": 0.2083,
      "step": 1236
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.022220298647880554,
      "learning_rate": 0.014852267998335413,
      "loss": 0.3396,
      "step": 1237
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.01176958717405796,
      "learning_rate": 0.014848106533499793,
      "loss": 0.1098,
      "step": 1238
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.01710779219865799,
      "learning_rate": 0.01484394506866417,
      "loss": 0.2,
      "step": 1239
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.027880975976586342,
      "learning_rate": 0.014839783603828547,
      "loss": 0.8281,
      "step": 1240
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.018767990171909332,
      "learning_rate": 0.014835622138992926,
      "loss": 0.2834,
      "step": 1241
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.012422457337379456,
      "learning_rate": 0.014831460674157304,
      "loss": 0.3337,
      "step": 1242
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.0187063068151474,
      "learning_rate": 0.01482729920932168,
      "loss": 0.2279,
      "step": 1243
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.018549783155322075,
      "learning_rate": 0.01482313774448606,
      "loss": 0.28,
      "step": 1244
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.01833917200565338,
      "learning_rate": 0.014818976279650438,
      "loss": 0.21,
      "step": 1245
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.027794407680630684,
      "learning_rate": 0.014814814814814814,
      "loss": 0.3555,
      "step": 1246
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.010096295736730099,
      "learning_rate": 0.014810653349979193,
      "loss": 0.1011,
      "step": 1247
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.014131364412605762,
      "learning_rate": 0.014806491885143571,
      "loss": 0.0641,
      "step": 1248
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.025988265872001648,
      "learning_rate": 0.014802330420307947,
      "loss": 0.2234,
      "step": 1249
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.01913675107061863,
      "learning_rate": 0.014798168955472327,
      "loss": 0.1346,
      "step": 1250
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.014575025998055935,
      "learning_rate": 0.014794007490636705,
      "loss": 0.0981,
      "step": 1251
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.01834517903625965,
      "learning_rate": 0.01478984602580108,
      "loss": 0.2886,
      "step": 1252
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.019583135843276978,
      "learning_rate": 0.01478568456096546,
      "loss": 0.2949,
      "step": 1253
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.02295992150902748,
      "learning_rate": 0.014781523096129838,
      "loss": 0.2228,
      "step": 1254
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.02698848955333233,
      "learning_rate": 0.014777361631294214,
      "loss": 0.5312,
      "step": 1255
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.01704975962638855,
      "learning_rate": 0.014773200166458594,
      "loss": 0.1948,
      "step": 1256
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.011658884584903717,
      "learning_rate": 0.014769038701622971,
      "loss": 0.0909,
      "step": 1257
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.025210224092006683,
      "learning_rate": 0.01476487723678735,
      "loss": 0.4639,
      "step": 1258
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.016480304300785065,
      "learning_rate": 0.014760715771951727,
      "loss": 0.2162,
      "step": 1259
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.017549768090248108,
      "learning_rate": 0.014756554307116105,
      "loss": 0.179,
      "step": 1260
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.010009512305259705,
      "learning_rate": 0.014752392842280483,
      "loss": 0.0526,
      "step": 1261
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.015168179757893085,
      "learning_rate": 0.01474823137744486,
      "loss": 0.2261,
      "step": 1262
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.014308580197393894,
      "learning_rate": 0.014744069912609238,
      "loss": 0.1192,
      "step": 1263
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.019218329340219498,
      "learning_rate": 0.014739908447773616,
      "loss": 0.0826,
      "step": 1264
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.034390877932310104,
      "learning_rate": 0.014735746982937994,
      "loss": 0.6899,
      "step": 1265
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.022100407630205154,
      "learning_rate": 0.014731585518102372,
      "loss": 0.3491,
      "step": 1266
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.028489554300904274,
      "learning_rate": 0.01472742405326675,
      "loss": 0.1444,
      "step": 1267
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.025911176577210426,
      "learning_rate": 0.014723262588431128,
      "loss": 0.3628,
      "step": 1268
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.02298724092543125,
      "learning_rate": 0.014719101123595507,
      "loss": 0.0673,
      "step": 1269
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.01847619004547596,
      "learning_rate": 0.014714939658759883,
      "loss": 0.4316,
      "step": 1270
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.017383543774485588,
      "learning_rate": 0.014710778193924261,
      "loss": 0.2588,
      "step": 1271
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.017542459070682526,
      "learning_rate": 0.01470661672908864,
      "loss": 0.15,
      "step": 1272
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.01959545910358429,
      "learning_rate": 0.014702455264253017,
      "loss": 0.1798,
      "step": 1273
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.019086269661784172,
      "learning_rate": 0.014698293799417395,
      "loss": 0.2642,
      "step": 1274
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.02074088715016842,
      "learning_rate": 0.014694132334581774,
      "loss": 0.2119,
      "step": 1275
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.013050183653831482,
      "learning_rate": 0.014689970869746152,
      "loss": 0.1073,
      "step": 1276
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.029848532751202583,
      "learning_rate": 0.014685809404910528,
      "loss": 0.2302,
      "step": 1277
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.014812586829066277,
      "learning_rate": 0.014681647940074908,
      "loss": 0.1295,
      "step": 1278
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.018400678411126137,
      "learning_rate": 0.014677486475239285,
      "loss": 0.1599,
      "step": 1279
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.013574771583080292,
      "learning_rate": 0.014673325010403662,
      "loss": 0.1953,
      "step": 1280
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.0073502687737345695,
      "learning_rate": 0.014669163545568041,
      "loss": 0.0317,
      "step": 1281
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.01186017319560051,
      "learning_rate": 0.014665002080732419,
      "loss": 0.0259,
      "step": 1282
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.021411679685115814,
      "learning_rate": 0.014660840615896795,
      "loss": 0.0804,
      "step": 1283
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.018703674897551537,
      "learning_rate": 0.014656679151061175,
      "loss": 0.1367,
      "step": 1284
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.013119488954544067,
      "learning_rate": 0.014652517686225552,
      "loss": 0.2534,
      "step": 1285
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.035264965146780014,
      "learning_rate": 0.014648356221389928,
      "loss": 0.2292,
      "step": 1286
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.02243852987885475,
      "learning_rate": 0.014644194756554308,
      "loss": 0.3123,
      "step": 1287
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.03754565119743347,
      "learning_rate": 0.014640033291718686,
      "loss": 1.1035,
      "step": 1288
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.015481263399124146,
      "learning_rate": 0.014635871826883062,
      "loss": 0.4878,
      "step": 1289
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.015926038846373558,
      "learning_rate": 0.014631710362047442,
      "loss": 0.1475,
      "step": 1290
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.02342096157371998,
      "learning_rate": 0.01462754889721182,
      "loss": 0.2212,
      "step": 1291
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.017114590853452682,
      "learning_rate": 0.014623387432376195,
      "loss": 0.1638,
      "step": 1292
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.012445701286196709,
      "learning_rate": 0.014619225967540575,
      "loss": 0.0558,
      "step": 1293
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.026921991258859634,
      "learning_rate": 0.014615064502704953,
      "loss": 0.5972,
      "step": 1294
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.018161725252866745,
      "learning_rate": 0.014610903037869329,
      "loss": 0.2722,
      "step": 1295
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.01985139586031437,
      "learning_rate": 0.014606741573033709,
      "loss": 0.2174,
      "step": 1296
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.020754551514983177,
      "learning_rate": 0.014602580108198086,
      "loss": 0.3433,
      "step": 1297
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.01693241111934185,
      "learning_rate": 0.014598418643362462,
      "loss": 0.0758,
      "step": 1298
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.013273599557578564,
      "learning_rate": 0.014594257178526842,
      "loss": 0.1368,
      "step": 1299
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.028018536046147346,
      "learning_rate": 0.01459009571369122,
      "loss": 0.2196,
      "step": 1300
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.015074766241014004,
      "learning_rate": 0.014585934248855598,
      "loss": 0.1137,
      "step": 1301
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.022739581763744354,
      "learning_rate": 0.014581772784019975,
      "loss": 0.3552,
      "step": 1302
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.034288566559553146,
      "learning_rate": 0.014577611319184353,
      "loss": 0.1115,
      "step": 1303
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.027745721861720085,
      "learning_rate": 0.014573449854348731,
      "loss": 0.5386,
      "step": 1304
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.022506052628159523,
      "learning_rate": 0.014569288389513109,
      "loss": 0.478,
      "step": 1305
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.024695830419659615,
      "learning_rate": 0.014565126924677487,
      "loss": 0.324,
      "step": 1306
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.02637370489537716,
      "learning_rate": 0.014560965459841865,
      "loss": 0.3882,
      "step": 1307
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.019992567598819733,
      "learning_rate": 0.014556803995006242,
      "loss": 0.3813,
      "step": 1308
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.01717136986553669,
      "learning_rate": 0.01455264253017062,
      "loss": 0.153,
      "step": 1309
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.01960982382297516,
      "learning_rate": 0.014548481065334998,
      "loss": 0.4602,
      "step": 1310
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.016627389937639236,
      "learning_rate": 0.014544319600499376,
      "loss": 0.2322,
      "step": 1311
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.008769506588578224,
      "learning_rate": 0.014540158135663755,
      "loss": 0.0365,
      "step": 1312
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.029798436909914017,
      "learning_rate": 0.014535996670828132,
      "loss": 0.6934,
      "step": 1313
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.015637395903468132,
      "learning_rate": 0.01453183520599251,
      "loss": 0.1306,
      "step": 1314
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.017693694680929184,
      "learning_rate": 0.014527673741156889,
      "loss": 0.2228,
      "step": 1315
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.0280127115547657,
      "learning_rate": 0.014523512276321265,
      "loss": 0.5562,
      "step": 1316
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.019847366958856583,
      "learning_rate": 0.014519350811485643,
      "loss": 0.2783,
      "step": 1317
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.023070378229022026,
      "learning_rate": 0.014515189346650022,
      "loss": 0.4016,
      "step": 1318
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.02012251503765583,
      "learning_rate": 0.014511027881814399,
      "loss": 0.2477,
      "step": 1319
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.02138359285891056,
      "learning_rate": 0.014506866416978776,
      "loss": 0.0745,
      "step": 1320
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.015071318484842777,
      "learning_rate": 0.014502704952143156,
      "loss": 0.3311,
      "step": 1321
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.021886644884943962,
      "learning_rate": 0.014498543487307532,
      "loss": 0.2004,
      "step": 1322
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.019193286076188087,
      "learning_rate": 0.01449438202247191,
      "loss": 0.1064,
      "step": 1323
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.017603464424610138,
      "learning_rate": 0.01449022055763629,
      "loss": 0.2197,
      "step": 1324
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.029079299420118332,
      "learning_rate": 0.014486059092800666,
      "loss": 0.4321,
      "step": 1325
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.009835940785706043,
      "learning_rate": 0.014481897627965043,
      "loss": 0.0257,
      "step": 1326
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.01926361583173275,
      "learning_rate": 0.014477736163129423,
      "loss": 0.1555,
      "step": 1327
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.01583191193640232,
      "learning_rate": 0.014473574698293799,
      "loss": 0.1516,
      "step": 1328
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.019557448104023933,
      "learning_rate": 0.014469413233458177,
      "loss": 0.2129,
      "step": 1329
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.015506278723478317,
      "learning_rate": 0.014465251768622556,
      "loss": 0.2209,
      "step": 1330
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.03219875693321228,
      "learning_rate": 0.014461090303786932,
      "loss": 0.0999,
      "step": 1331
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.01541612483561039,
      "learning_rate": 0.01445692883895131,
      "loss": 0.1244,
      "step": 1332
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.022204864770174026,
      "learning_rate": 0.01445276737411569,
      "loss": 0.3271,
      "step": 1333
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.01713612489402294,
      "learning_rate": 0.014448605909280066,
      "loss": 0.1743,
      "step": 1334
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.02419763058423996,
      "learning_rate": 0.014444444444444444,
      "loss": 0.4541,
      "step": 1335
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.00032571720657870173,
      "learning_rate": 0.014440282979608823,
      "loss": 0.0004,
      "step": 1336
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.018572412431240082,
      "learning_rate": 0.0144361215147732,
      "loss": 0.2213,
      "step": 1337
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.016445094719529152,
      "learning_rate": 0.014431960049937577,
      "loss": 0.2042,
      "step": 1338
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.017806435003876686,
      "learning_rate": 0.014427798585101957,
      "loss": 0.3235,
      "step": 1339
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.015416833572089672,
      "learning_rate": 0.014423637120266333,
      "loss": 0.2715,
      "step": 1340
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.02536718361079693,
      "learning_rate": 0.01441947565543071,
      "loss": 0.1826,
      "step": 1341
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.02097124606370926,
      "learning_rate": 0.01441531419059509,
      "loss": 0.3977,
      "step": 1342
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.02159344032406807,
      "learning_rate": 0.014411152725759466,
      "loss": 0.1628,
      "step": 1343
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.02138870395720005,
      "learning_rate": 0.014406991260923846,
      "loss": 0.4041,
      "step": 1344
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.028393961489200592,
      "learning_rate": 0.014402829796088224,
      "loss": 0.3564,
      "step": 1345
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.02029264159500599,
      "learning_rate": 0.0143986683312526,
      "loss": 0.345,
      "step": 1346
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.024640792980790138,
      "learning_rate": 0.01439450686641698,
      "loss": 0.3521,
      "step": 1347
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.016772327944636345,
      "learning_rate": 0.014390345401581357,
      "loss": 0.1578,
      "step": 1348
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.01774672605097294,
      "learning_rate": 0.014386183936745735,
      "loss": 0.1373,
      "step": 1349
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.021757176145911217,
      "learning_rate": 0.014382022471910113,
      "loss": 0.1776,
      "step": 1350
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.022427218034863472,
      "learning_rate": 0.01437786100707449,
      "loss": 0.1674,
      "step": 1351
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.024645455181598663,
      "learning_rate": 0.014373699542238869,
      "loss": 0.4194,
      "step": 1352
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.016755057498812675,
      "learning_rate": 0.014369538077403246,
      "loss": 0.269,
      "step": 1353
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.02398335002362728,
      "learning_rate": 0.014365376612567624,
      "loss": 0.3391,
      "step": 1354
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.02188781090080738,
      "learning_rate": 0.014361215147732004,
      "loss": 0.1846,
      "step": 1355
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.026626283302903175,
      "learning_rate": 0.01435705368289638,
      "loss": 0.3413,
      "step": 1356
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.007642584387212992,
      "learning_rate": 0.014352892218060758,
      "loss": 0.0321,
      "step": 1357
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.01641274429857731,
      "learning_rate": 0.014348730753225137,
      "loss": 0.1602,
      "step": 1358
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.01177943404763937,
      "learning_rate": 0.014344569288389513,
      "loss": 0.0375,
      "step": 1359
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.017289074137806892,
      "learning_rate": 0.014340407823553891,
      "loss": 0.2212,
      "step": 1360
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.02059534192085266,
      "learning_rate": 0.01433624635871827,
      "loss": 0.0945,
      "step": 1361
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.01597408764064312,
      "learning_rate": 0.014332084893882647,
      "loss": 0.1144,
      "step": 1362
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.011774923652410507,
      "learning_rate": 0.014327923429047025,
      "loss": 0.0533,
      "step": 1363
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.0320376418530941,
      "learning_rate": 0.014323761964211404,
      "loss": 0.2842,
      "step": 1364
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.0030445631127804518,
      "learning_rate": 0.01431960049937578,
      "loss": 0.0031,
      "step": 1365
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.02051527611911297,
      "learning_rate": 0.014315439034540158,
      "loss": 0.2717,
      "step": 1366
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.036784928292036057,
      "learning_rate": 0.014311277569704538,
      "loss": 0.2264,
      "step": 1367
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.014145884662866592,
      "learning_rate": 0.014307116104868914,
      "loss": 0.0837,
      "step": 1368
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.016393741592764854,
      "learning_rate": 0.014302954640033292,
      "loss": 0.1398,
      "step": 1369
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.018714239820837975,
      "learning_rate": 0.014298793175197671,
      "loss": 0.3721,
      "step": 1370
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.02131592109799385,
      "learning_rate": 0.014294631710362047,
      "loss": 0.2935,
      "step": 1371
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.02309492789208889,
      "learning_rate": 0.014290470245526425,
      "loss": 0.21,
      "step": 1372
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.026801444590091705,
      "learning_rate": 0.014286308780690805,
      "loss": 0.2778,
      "step": 1373
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.023400824517011642,
      "learning_rate": 0.01428214731585518,
      "loss": 0.3831,
      "step": 1374
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.014906673692166805,
      "learning_rate": 0.014277985851019559,
      "loss": 0.2432,
      "step": 1375
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.022430352866649628,
      "learning_rate": 0.014273824386183938,
      "loss": 0.1935,
      "step": 1376
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.019792569801211357,
      "learning_rate": 0.014269662921348314,
      "loss": 0.1677,
      "step": 1377
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.00937770027667284,
      "learning_rate": 0.014265501456512692,
      "loss": 0.0387,
      "step": 1378
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.013458197005093098,
      "learning_rate": 0.014261339991677072,
      "loss": 0.1322,
      "step": 1379
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.0007534924079664052,
      "learning_rate": 0.014257178526841448,
      "loss": 0.001,
      "step": 1380
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.016316169872879982,
      "learning_rate": 0.014253017062005826,
      "loss": 0.101,
      "step": 1381
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.01418232824653387,
      "learning_rate": 0.014248855597170205,
      "loss": 0.1289,
      "step": 1382
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.0020648986101150513,
      "learning_rate": 0.014244694132334581,
      "loss": 0.0013,
      "step": 1383
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.01654881052672863,
      "learning_rate": 0.014240532667498959,
      "loss": 0.1458,
      "step": 1384
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.021646007895469666,
      "learning_rate": 0.014236371202663339,
      "loss": 0.2888,
      "step": 1385
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.055870696902275085,
      "learning_rate": 0.014232209737827715,
      "loss": 0.8643,
      "step": 1386
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.02059507742524147,
      "learning_rate": 0.014228048272992094,
      "loss": 0.2094,
      "step": 1387
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.014149104245007038,
      "learning_rate": 0.014223886808156472,
      "loss": 0.0392,
      "step": 1388
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.020512647926807404,
      "learning_rate": 0.014219725343320848,
      "loss": 0.0986,
      "step": 1389
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.015520925633609295,
      "learning_rate": 0.014215563878485228,
      "loss": 0.1333,
      "step": 1390
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.02369903400540352,
      "learning_rate": 0.014211402413649606,
      "loss": 0.3264,
      "step": 1391
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.018396614119410515,
      "learning_rate": 0.014207240948813982,
      "loss": 0.17,
      "step": 1392
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.005517400335520506,
      "learning_rate": 0.014203079483978361,
      "loss": 0.0175,
      "step": 1393
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.017674177885055542,
      "learning_rate": 0.014198918019142739,
      "loss": 0.2925,
      "step": 1394
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.02093116194009781,
      "learning_rate": 0.014194756554307115,
      "loss": 0.2559,
      "step": 1395
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.017458520829677582,
      "learning_rate": 0.014190595089471495,
      "loss": 0.1471,
      "step": 1396
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.01619010791182518,
      "learning_rate": 0.014186433624635873,
      "loss": 0.1913,
      "step": 1397
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.020769834518432617,
      "learning_rate": 0.014182272159800249,
      "loss": 0.3037,
      "step": 1398
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.027286134660243988,
      "learning_rate": 0.014178110694964628,
      "loss": 0.4504,
      "step": 1399
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.024402379989624023,
      "learning_rate": 0.014173949230129006,
      "loss": 0.4822,
      "step": 1400
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.019336512312293053,
      "learning_rate": 0.014169787765293382,
      "loss": 0.1427,
      "step": 1401
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.015416628681123257,
      "learning_rate": 0.014165626300457762,
      "loss": 0.2786,
      "step": 1402
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.018515951931476593,
      "learning_rate": 0.01416146483562214,
      "loss": 0.259,
      "step": 1403
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.013942866586148739,
      "learning_rate": 0.014157303370786516,
      "loss": 0.1501,
      "step": 1404
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.012686737813055515,
      "learning_rate": 0.014153141905950895,
      "loss": 0.1641,
      "step": 1405
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.015417816117405891,
      "learning_rate": 0.014148980441115273,
      "loss": 0.0716,
      "step": 1406
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.014001313596963882,
      "learning_rate": 0.014144818976279649,
      "loss": 0.1852,
      "step": 1407
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.016791053116321564,
      "learning_rate": 0.014140657511444029,
      "loss": 0.1738,
      "step": 1408
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.0012489767977967858,
      "learning_rate": 0.014136496046608407,
      "loss": 0.0022,
      "step": 1409
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.020686307922005653,
      "learning_rate": 0.014132334581772783,
      "loss": 0.1094,
      "step": 1410
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.01566472090780735,
      "learning_rate": 0.014128173116937162,
      "loss": 0.0838,
      "step": 1411
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.024927310645580292,
      "learning_rate": 0.01412401165210154,
      "loss": 0.1459,
      "step": 1412
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.02150912955403328,
      "learning_rate": 0.014119850187265916,
      "loss": 0.1733,
      "step": 1413
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.014271970838308334,
      "learning_rate": 0.014115688722430296,
      "loss": 0.1306,
      "step": 1414
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.01386289857327938,
      "learning_rate": 0.014111527257594673,
      "loss": 0.0351,
      "step": 1415
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.03796965628862381,
      "learning_rate": 0.01410736579275905,
      "loss": 0.3135,
      "step": 1416
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.017932679504156113,
      "learning_rate": 0.014103204327923429,
      "loss": 0.1917,
      "step": 1417
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.030670197680592537,
      "learning_rate": 0.014099042863087807,
      "loss": 0.4041,
      "step": 1418
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.017793040722608566,
      "learning_rate": 0.014094881398252185,
      "loss": 0.2169,
      "step": 1419
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.015402521938085556,
      "learning_rate": 0.014090719933416563,
      "loss": 0.0597,
      "step": 1420
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.020960144698619843,
      "learning_rate": 0.01408655846858094,
      "loss": 0.3179,
      "step": 1421
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.020554885268211365,
      "learning_rate": 0.01408239700374532,
      "loss": 0.1812,
      "step": 1422
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.01604786328971386,
      "learning_rate": 0.014078235538909696,
      "loss": 0.129,
      "step": 1423
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.0004181724216323346,
      "learning_rate": 0.014074074074074074,
      "loss": 0.0006,
      "step": 1424
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.02337593026459217,
      "learning_rate": 0.014069912609238453,
      "loss": 0.2957,
      "step": 1425
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.04740644618868828,
      "learning_rate": 0.01406575114440283,
      "loss": 0.2076,
      "step": 1426
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.01808987930417061,
      "learning_rate": 0.014061589679567207,
      "loss": 0.2023,
      "step": 1427
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.035245489329099655,
      "learning_rate": 0.014057428214731587,
      "loss": 0.4993,
      "step": 1428
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.01764986850321293,
      "learning_rate": 0.014053266749895963,
      "loss": 0.1997,
      "step": 1429
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.03306521847844124,
      "learning_rate": 0.014049105285060343,
      "loss": 0.3669,
      "step": 1430
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.021353241056203842,
      "learning_rate": 0.01404494382022472,
      "loss": 0.2961,
      "step": 1431
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.025218380615115166,
      "learning_rate": 0.014040782355389097,
      "loss": 0.2878,
      "step": 1432
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.01481439545750618,
      "learning_rate": 0.014036620890553476,
      "loss": 0.1694,
      "step": 1433
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.015485993586480618,
      "learning_rate": 0.014032459425717854,
      "loss": 0.1014,
      "step": 1434
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.009433499537408352,
      "learning_rate": 0.01402829796088223,
      "loss": 0.0399,
      "step": 1435
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.016402481123805046,
      "learning_rate": 0.01402413649604661,
      "loss": 0.1014,
      "step": 1436
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.02093060128390789,
      "learning_rate": 0.014019975031210987,
      "loss": 0.3584,
      "step": 1437
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.017392264679074287,
      "learning_rate": 0.014015813566375364,
      "loss": 0.1449,
      "step": 1438
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.0166440699249506,
      "learning_rate": 0.014011652101539743,
      "loss": 0.2472,
      "step": 1439
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.012438010424375534,
      "learning_rate": 0.014007490636704121,
      "loss": 0.0868,
      "step": 1440
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.02194707840681076,
      "learning_rate": 0.014003329171868497,
      "loss": 0.3672,
      "step": 1441
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.015828276053071022,
      "learning_rate": 0.013999167707032877,
      "loss": 0.1791,
      "step": 1442
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.026127304881811142,
      "learning_rate": 0.013995006242197254,
      "loss": 0.2229,
      "step": 1443
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.03021225333213806,
      "learning_rate": 0.01399084477736163,
      "loss": 0.5078,
      "step": 1444
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.027687614783644676,
      "learning_rate": 0.01398668331252601,
      "loss": 0.4944,
      "step": 1445
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.02552678808569908,
      "learning_rate": 0.013982521847690388,
      "loss": 0.3657,
      "step": 1446
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.01932724565267563,
      "learning_rate": 0.013978360382854764,
      "loss": 0.4089,
      "step": 1447
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.01600070483982563,
      "learning_rate": 0.013974198918019144,
      "loss": 0.0834,
      "step": 1448
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.01791650988161564,
      "learning_rate": 0.013970037453183521,
      "loss": 0.1436,
      "step": 1449
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.01735217683017254,
      "learning_rate": 0.013965875988347897,
      "loss": 0.311,
      "step": 1450
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.021060368046164513,
      "learning_rate": 0.013961714523512277,
      "loss": 0.4624,
      "step": 1451
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.02310653030872345,
      "learning_rate": 0.013957553058676655,
      "loss": 0.1981,
      "step": 1452
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.011255250312387943,
      "learning_rate": 0.013953391593841031,
      "loss": 0.0776,
      "step": 1453
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.0183134526014328,
      "learning_rate": 0.01394923012900541,
      "loss": 0.1495,
      "step": 1454
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.039122916758060455,
      "learning_rate": 0.013945068664169788,
      "loss": 0.4363,
      "step": 1455
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.03267544507980347,
      "learning_rate": 0.013940907199334164,
      "loss": 0.1599,
      "step": 1456
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.014447005465626717,
      "learning_rate": 0.013936745734498544,
      "loss": 0.2357,
      "step": 1457
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.026220016181468964,
      "learning_rate": 0.013932584269662922,
      "loss": 0.3113,
      "step": 1458
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.021332180127501488,
      "learning_rate": 0.013928422804827298,
      "loss": 0.4189,
      "step": 1459
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.022160692140460014,
      "learning_rate": 0.013924261339991677,
      "loss": 0.1917,
      "step": 1460
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.018558040261268616,
      "learning_rate": 0.013920099875156055,
      "loss": 0.0836,
      "step": 1461
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.02270011231303215,
      "learning_rate": 0.013915938410320433,
      "loss": 0.2104,
      "step": 1462
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.018085598945617676,
      "learning_rate": 0.013911776945484811,
      "loss": 0.3501,
      "step": 1463
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.0162225142121315,
      "learning_rate": 0.013907615480649189,
      "loss": 0.2053,
      "step": 1464
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.03380531072616577,
      "learning_rate": 0.013903454015813567,
      "loss": 0.3596,
      "step": 1465
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.024515025317668915,
      "learning_rate": 0.013899292550977944,
      "loss": 0.3176,
      "step": 1466
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.016738541424274445,
      "learning_rate": 0.013895131086142322,
      "loss": 0.2124,
      "step": 1467
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.07320991158485413,
      "learning_rate": 0.0138909696213067,
      "loss": 0.1891,
      "step": 1468
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.03906169533729553,
      "learning_rate": 0.013886808156471078,
      "loss": 0.3323,
      "step": 1469
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.02159789577126503,
      "learning_rate": 0.013882646691635456,
      "loss": 0.1887,
      "step": 1470
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.015183995477855206,
      "learning_rate": 0.013878485226799834,
      "loss": 0.0684,
      "step": 1471
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.016216358169913292,
      "learning_rate": 0.013874323761964211,
      "loss": 0.1088,
      "step": 1472
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.018191451206803322,
      "learning_rate": 0.013870162297128591,
      "loss": 0.1843,
      "step": 1473
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.02661026082932949,
      "learning_rate": 0.013866000832292967,
      "loss": 0.2686,
      "step": 1474
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.020341236144304276,
      "learning_rate": 0.013861839367457345,
      "loss": 0.1282,
      "step": 1475
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.0009772197809070349,
      "learning_rate": 0.013857677902621724,
      "loss": 0.0012,
      "step": 1476
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.018385665491223335,
      "learning_rate": 0.0138535164377861,
      "loss": 0.2412,
      "step": 1477
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.01054059062153101,
      "learning_rate": 0.013849354972950478,
      "loss": 0.0426,
      "step": 1478
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.0234739538282156,
      "learning_rate": 0.013845193508114858,
      "loss": 0.2435,
      "step": 1479
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.011829260736703873,
      "learning_rate": 0.013841032043279234,
      "loss": 0.0867,
      "step": 1480
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.009333239868283272,
      "learning_rate": 0.013836870578443612,
      "loss": 0.0369,
      "step": 1481
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.016677506268024445,
      "learning_rate": 0.013832709113607991,
      "loss": 0.2181,
      "step": 1482
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.034937627613544464,
      "learning_rate": 0.013828547648772367,
      "loss": 0.3247,
      "step": 1483
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.029054462909698486,
      "learning_rate": 0.013824386183936745,
      "loss": 0.1885,
      "step": 1484
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.023464389145374298,
      "learning_rate": 0.013820224719101125,
      "loss": 0.2864,
      "step": 1485
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.017824359238147736,
      "learning_rate": 0.013816063254265501,
      "loss": 0.1014,
      "step": 1486
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.034080225974321365,
      "learning_rate": 0.013811901789429879,
      "loss": 0.4673,
      "step": 1487
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.020294196903705597,
      "learning_rate": 0.013807740324594258,
      "loss": 0.2617,
      "step": 1488
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.016916709020733833,
      "learning_rate": 0.013803578859758634,
      "loss": 0.0597,
      "step": 1489
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.019215894863009453,
      "learning_rate": 0.013799417394923012,
      "loss": 0.181,
      "step": 1490
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.02869659848511219,
      "learning_rate": 0.013795255930087392,
      "loss": 0.2944,
      "step": 1491
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.011239245533943176,
      "learning_rate": 0.013791094465251768,
      "loss": 0.0978,
      "step": 1492
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.022165564820170403,
      "learning_rate": 0.013786933000416146,
      "loss": 0.2957,
      "step": 1493
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.023557430133223534,
      "learning_rate": 0.013782771535580525,
      "loss": 0.3359,
      "step": 1494
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.031115056946873665,
      "learning_rate": 0.013778610070744903,
      "loss": 0.562,
      "step": 1495
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.0164656825363636,
      "learning_rate": 0.01377444860590928,
      "loss": 0.2074,
      "step": 1496
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.03633042424917221,
      "learning_rate": 0.013770287141073659,
      "loss": 0.3157,
      "step": 1497
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.0232245996594429,
      "learning_rate": 0.013766125676238037,
      "loss": 0.3623,
      "step": 1498
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.01717163436114788,
      "learning_rate": 0.013761964211402413,
      "loss": 0.0588,
      "step": 1499
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.02366768568754196,
      "learning_rate": 0.013757802746566792,
      "loss": 0.3667,
      "step": 1500
    },
    {
      "epoch": 1.87,
      "eval_loss": 0.255859375,
      "eval_runtime": 183.1825,
      "eval_samples_per_second": 1.097,
      "eval_steps_per_second": 0.551,
      "step": 1500
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.02006642334163189,
      "learning_rate": 0.01375364128173117,
      "loss": 0.4419,
      "step": 1501
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.01816656067967415,
      "learning_rate": 0.013749479816895546,
      "loss": 0.3906,
      "step": 1502
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.018241431564092636,
      "learning_rate": 0.013745318352059926,
      "loss": 0.142,
      "step": 1503
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.02170722931623459,
      "learning_rate": 0.013741156887224304,
      "loss": 0.2006,
      "step": 1504
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.02239539660513401,
      "learning_rate": 0.013736995422388681,
      "loss": 0.2683,
      "step": 1505
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.02385118417441845,
      "learning_rate": 0.01373283395755306,
      "loss": 0.3203,
      "step": 1506
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.019192039966583252,
      "learning_rate": 0.013728672492717437,
      "loss": 0.4382,
      "step": 1507
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.019752196967601776,
      "learning_rate": 0.013724511027881815,
      "loss": 0.141,
      "step": 1508
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.028852222487330437,
      "learning_rate": 0.013720349563046193,
      "loss": 0.27,
      "step": 1509
    },
    {
      "epoch": 1.89,
      "grad_norm": 0.03203233703970909,
      "learning_rate": 0.01371618809821057,
      "loss": 0.333,
      "step": 1510
    },
    {
      "epoch": 1.89,
      "grad_norm": 0.04278022050857544,
      "learning_rate": 0.013712026633374948,
      "loss": 0.1697,
      "step": 1511
    },
    {
      "epoch": 1.89,
      "grad_norm": 0.02357189543545246,
      "learning_rate": 0.013707865168539326,
      "loss": 0.2625,
      "step": 1512
    },
    {
      "epoch": 1.89,
      "grad_norm": 0.02417321875691414,
      "learning_rate": 0.013703703703703704,
      "loss": 0.3477,
      "step": 1513
    },
    {
      "epoch": 1.89,
      "grad_norm": 0.001250821864232421,
      "learning_rate": 0.013699542238868082,
      "loss": 0.0017,
      "step": 1514
    },
    {
      "epoch": 1.89,
      "grad_norm": 0.0113756088539958,
      "learning_rate": 0.01369538077403246,
      "loss": 0.0731,
      "step": 1515
    },
    {
      "epoch": 1.89,
      "grad_norm": 0.03172725439071655,
      "learning_rate": 0.01369121930919684,
      "loss": 0.0421,
      "step": 1516
    },
    {
      "epoch": 1.89,
      "grad_norm": 0.03159737586975098,
      "learning_rate": 0.013687057844361215,
      "loss": 0.325,
      "step": 1517
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.02726905792951584,
      "learning_rate": 0.013682896379525593,
      "loss": 0.5132,
      "step": 1518
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.06112120300531387,
      "learning_rate": 0.013678734914689973,
      "loss": 0.2749,
      "step": 1519
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.03497801721096039,
      "learning_rate": 0.013674573449854349,
      "loss": 0.5059,
      "step": 1520
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.024013888090848923,
      "learning_rate": 0.013670411985018727,
      "loss": 0.3801,
      "step": 1521
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.01821964979171753,
      "learning_rate": 0.013666250520183106,
      "loss": 0.1104,
      "step": 1522
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.021777618676424026,
      "learning_rate": 0.013662089055347482,
      "loss": 0.4121,
      "step": 1523
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.011656336486339569,
      "learning_rate": 0.01365792759051186,
      "loss": 0.1368,
      "step": 1524
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.021475238725543022,
      "learning_rate": 0.01365376612567624,
      "loss": 0.2457,
      "step": 1525
    },
    {
      "epoch": 1.91,
      "grad_norm": 0.01610015332698822,
      "learning_rate": 0.013649604660840616,
      "loss": 0.1185,
      "step": 1526
    },
    {
      "epoch": 1.91,
      "grad_norm": 0.01488636713474989,
      "learning_rate": 0.013645443196004994,
      "loss": 0.1858,
      "step": 1527
    },
    {
      "epoch": 1.91,
      "grad_norm": 0.019006485119462013,
      "learning_rate": 0.013641281731169373,
      "loss": 0.1826,
      "step": 1528
    },
    {
      "epoch": 1.91,
      "grad_norm": 0.03319217637181282,
      "learning_rate": 0.01363712026633375,
      "loss": 0.6514,
      "step": 1529
    },
    {
      "epoch": 1.91,
      "grad_norm": 0.01714898645877838,
      "learning_rate": 0.013632958801498127,
      "loss": 0.2625,
      "step": 1530
    },
    {
      "epoch": 1.91,
      "grad_norm": 0.09312057495117188,
      "learning_rate": 0.013628797336662507,
      "loss": 0.314,
      "step": 1531
    },
    {
      "epoch": 1.91,
      "grad_norm": 0.02570984698832035,
      "learning_rate": 0.013624635871826883,
      "loss": 0.4517,
      "step": 1532
    },
    {
      "epoch": 1.91,
      "grad_norm": 0.012067770585417747,
      "learning_rate": 0.01362047440699126,
      "loss": 0.0739,
      "step": 1533
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.027837583795189857,
      "learning_rate": 0.01361631294215564,
      "loss": 0.1808,
      "step": 1534
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.014074801467359066,
      "learning_rate": 0.013612151477320016,
      "loss": 0.098,
      "step": 1535
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.034389570355415344,
      "learning_rate": 0.013607990012484394,
      "loss": 0.3005,
      "step": 1536
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.029705261811614037,
      "learning_rate": 0.013603828547648774,
      "loss": 0.5361,
      "step": 1537
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.01633676514029503,
      "learning_rate": 0.01359966708281315,
      "loss": 0.2241,
      "step": 1538
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.02213212475180626,
      "learning_rate": 0.013595505617977528,
      "loss": 0.5161,
      "step": 1539
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.0207045990973711,
      "learning_rate": 0.013591344153141907,
      "loss": 0.2563,
      "step": 1540
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.01963883824646473,
      "learning_rate": 0.013587182688306283,
      "loss": 0.3442,
      "step": 1541
    },
    {
      "epoch": 1.93,
      "grad_norm": 0.00888076238334179,
      "learning_rate": 0.013583021223470661,
      "loss": 0.0267,
      "step": 1542
    },
    {
      "epoch": 1.93,
      "grad_norm": 0.018314221873879433,
      "learning_rate": 0.01357885975863504,
      "loss": 0.2981,
      "step": 1543
    },
    {
      "epoch": 1.93,
      "grad_norm": 0.0006296020583249629,
      "learning_rate": 0.013574698293799417,
      "loss": 0.0009,
      "step": 1544
    },
    {
      "epoch": 1.93,
      "grad_norm": 0.02064560540020466,
      "learning_rate": 0.013570536828963795,
      "loss": 0.2698,
      "step": 1545
    },
    {
      "epoch": 1.93,
      "grad_norm": 0.0189712792634964,
      "learning_rate": 0.013566375364128174,
      "loss": 0.2489,
      "step": 1546
    },
    {
      "epoch": 1.93,
      "grad_norm": 0.016451096162199974,
      "learning_rate": 0.01356221389929255,
      "loss": 0.2373,
      "step": 1547
    },
    {
      "epoch": 1.93,
      "grad_norm": 0.021989211440086365,
      "learning_rate": 0.01355805243445693,
      "loss": 0.2542,
      "step": 1548
    },
    {
      "epoch": 1.93,
      "grad_norm": 0.0185481458902359,
      "learning_rate": 0.013553890969621308,
      "loss": 0.3467,
      "step": 1549
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.021945621818304062,
      "learning_rate": 0.013549729504785684,
      "loss": 0.2551,
      "step": 1550
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.010594727471470833,
      "learning_rate": 0.013545568039950063,
      "loss": 0.0572,
      "step": 1551
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.006432410329580307,
      "learning_rate": 0.013541406575114441,
      "loss": 0.0213,
      "step": 1552
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.021695928648114204,
      "learning_rate": 0.013537245110278817,
      "loss": 0.234,
      "step": 1553
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.014550553634762764,
      "learning_rate": 0.013533083645443197,
      "loss": 0.0184,
      "step": 1554
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.02522454783320427,
      "learning_rate": 0.013528922180607575,
      "loss": 0.6035,
      "step": 1555
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.0181414857506752,
      "learning_rate": 0.01352476071577195,
      "loss": 0.2759,
      "step": 1556
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.018419595435261726,
      "learning_rate": 0.01352059925093633,
      "loss": 0.2913,
      "step": 1557
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.02607671171426773,
      "learning_rate": 0.013516437786100708,
      "loss": 0.6367,
      "step": 1558
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.02497745305299759,
      "learning_rate": 0.013512276321265084,
      "loss": 0.3782,
      "step": 1559
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.014643196016550064,
      "learning_rate": 0.013508114856429464,
      "loss": 0.1885,
      "step": 1560
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.01669475995004177,
      "learning_rate": 0.013503953391593842,
      "loss": 0.1608,
      "step": 1561
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.02063913457095623,
      "learning_rate": 0.013499791926758218,
      "loss": 0.3447,
      "step": 1562
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.0247049480676651,
      "learning_rate": 0.013495630461922597,
      "loss": 0.1067,
      "step": 1563
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.025522813200950623,
      "learning_rate": 0.013491468997086975,
      "loss": 0.2252,
      "step": 1564
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.020269270986318588,
      "learning_rate": 0.013487307532251351,
      "loss": 0.2181,
      "step": 1565
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.01655460149049759,
      "learning_rate": 0.01348314606741573,
      "loss": 0.0418,
      "step": 1566
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.017600474879145622,
      "learning_rate": 0.013478984602580108,
      "loss": 0.3569,
      "step": 1567
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.0014165084576234221,
      "learning_rate": 0.013474823137744488,
      "loss": 0.0015,
      "step": 1568
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.018878618255257607,
      "learning_rate": 0.013470661672908864,
      "loss": 0.2224,
      "step": 1569
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.03756897523999214,
      "learning_rate": 0.013466500208073242,
      "loss": 0.4622,
      "step": 1570
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.015494448132812977,
      "learning_rate": 0.013462338743237622,
      "loss": 0.1035,
      "step": 1571
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.012156233191490173,
      "learning_rate": 0.013458177278401998,
      "loss": 0.0742,
      "step": 1572
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.020984916016459465,
      "learning_rate": 0.013454015813566375,
      "loss": 0.4297,
      "step": 1573
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.0228821262717247,
      "learning_rate": 0.013449854348730755,
      "loss": 0.1675,
      "step": 1574
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.013339287601411343,
      "learning_rate": 0.013445692883895131,
      "loss": 0.0551,
      "step": 1575
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.01159537211060524,
      "learning_rate": 0.013441531419059509,
      "loss": 0.054,
      "step": 1576
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.02583775483071804,
      "learning_rate": 0.013437369954223888,
      "loss": 0.3721,
      "step": 1577
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.013470176607370377,
      "learning_rate": 0.013433208489388265,
      "loss": 0.0488,
      "step": 1578
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.020074065774679184,
      "learning_rate": 0.013429047024552642,
      "loss": 0.1904,
      "step": 1579
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.015542983077466488,
      "learning_rate": 0.013424885559717022,
      "loss": 0.1875,
      "step": 1580
    },
    {
      "epoch": 1.97,
      "grad_norm": 0.009043223224580288,
      "learning_rate": 0.013420724094881398,
      "loss": 0.0396,
      "step": 1581
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.01617331989109516,
      "learning_rate": 0.013416562630045776,
      "loss": 0.2766,
      "step": 1582
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.004869926255196333,
      "learning_rate": 0.013412401165210155,
      "loss": 0.0125,
      "step": 1583
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.012283380143344402,
      "learning_rate": 0.013408239700374532,
      "loss": 0.1777,
      "step": 1584
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.015024757012724876,
      "learning_rate": 0.01340407823553891,
      "loss": 0.2671,
      "step": 1585
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.01381976343691349,
      "learning_rate": 0.013399916770703289,
      "loss": 0.2205,
      "step": 1586
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.027577077969908714,
      "learning_rate": 0.013395755305867665,
      "loss": 0.2961,
      "step": 1587
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.02123187854886055,
      "learning_rate": 0.013391593841032043,
      "loss": 0.3511,
      "step": 1588
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.02514972910284996,
      "learning_rate": 0.013387432376196422,
      "loss": 0.208,
      "step": 1589
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.018576644361019135,
      "learning_rate": 0.013383270911360799,
      "loss": 0.1674,
      "step": 1590
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.0422707125544548,
      "learning_rate": 0.013379109446525178,
      "loss": 0.0893,
      "step": 1591
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.02010701783001423,
      "learning_rate": 0.013374947981689556,
      "loss": 0.0777,
      "step": 1592
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.020661858841776848,
      "learning_rate": 0.013370786516853932,
      "loss": 0.1682,
      "step": 1593
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.018853288143873215,
      "learning_rate": 0.013366625052018312,
      "loss": 0.2285,
      "step": 1594
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.041938621550798416,
      "learning_rate": 0.01336246358718269,
      "loss": 0.4617,
      "step": 1595
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.02083117887377739,
      "learning_rate": 0.013358302122347065,
      "loss": 0.1902,
      "step": 1596
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.0237300805747509,
      "learning_rate": 0.013354140657511445,
      "loss": 0.1667,
      "step": 1597
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.02026430144906044,
      "learning_rate": 0.013349979192675823,
      "loss": 0.293,
      "step": 1598
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.016819776967167854,
      "learning_rate": 0.013345817727840199,
      "loss": 0.1488,
      "step": 1599
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.02222897671163082,
      "learning_rate": 0.013341656263004579,
      "loss": 0.3335,
      "step": 1600
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.023801453411579132,
      "learning_rate": 0.013337494798168956,
      "loss": 0.3625,
      "step": 1601
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.023083828389644623,
      "learning_rate": 0.013333333333333332,
      "loss": 0.2576,
      "step": 1602
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.020124264061450958,
      "learning_rate": 0.013329171868497712,
      "loss": 0.1449,
      "step": 1603
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.015237260609865189,
      "learning_rate": 0.01332501040366209,
      "loss": 0.1158,
      "step": 1604
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.005561564117670059,
      "learning_rate": 0.013320848938826466,
      "loss": 0.0088,
      "step": 1605
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.019450634717941284,
      "learning_rate": 0.013316687473990845,
      "loss": 0.2148,
      "step": 1606
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.025494640693068504,
      "learning_rate": 0.013312526009155223,
      "loss": 0.2524,
      "step": 1607
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.012134755030274391,
      "learning_rate": 0.0133083645443196,
      "loss": 0.1444,
      "step": 1608
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.037740446627140045,
      "learning_rate": 0.013304203079483979,
      "loss": 0.5859,
      "step": 1609
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.012504205107688904,
      "learning_rate": 0.013300041614648357,
      "loss": 0.0752,
      "step": 1610
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.010789690539240837,
      "learning_rate": 0.013295880149812733,
      "loss": 0.0747,
      "step": 1611
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.024266045540571213,
      "learning_rate": 0.013291718684977112,
      "loss": 0.0599,
      "step": 1612
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.01707211695611477,
      "learning_rate": 0.01328755722014149,
      "loss": 0.1428,
      "step": 1613
    },
    {
      "epoch": 2.01,
      "grad_norm": 0.011687228456139565,
      "learning_rate": 0.013283395755305866,
      "loss": 0.0512,
      "step": 1614
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.016663536429405212,
      "learning_rate": 0.013279234290470246,
      "loss": 0.1967,
      "step": 1615
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.018442049622535706,
      "learning_rate": 0.013275072825634624,
      "loss": 0.2179,
      "step": 1616
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.014540496282279491,
      "learning_rate": 0.013270911360799,
      "loss": 0.187,
      "step": 1617
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.016471195966005325,
      "learning_rate": 0.01326674989596338,
      "loss": 0.1472,
      "step": 1618
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.012928253039717674,
      "learning_rate": 0.013262588431127757,
      "loss": 0.1372,
      "step": 1619
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.017855726182460785,
      "learning_rate": 0.013258426966292133,
      "loss": 0.199,
      "step": 1620
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.017865898087620735,
      "learning_rate": 0.013254265501456513,
      "loss": 0.1903,
      "step": 1621
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.0011013002367690206,
      "learning_rate": 0.01325010403662089,
      "loss": 0.0012,
      "step": 1622
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.020644966512918472,
      "learning_rate": 0.013245942571785269,
      "loss": 0.3376,
      "step": 1623
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.02086360938847065,
      "learning_rate": 0.013241781106949646,
      "loss": 0.2625,
      "step": 1624
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.016935108229517937,
      "learning_rate": 0.013237619642114024,
      "loss": 0.0122,
      "step": 1625
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.013805147260427475,
      "learning_rate": 0.013233458177278402,
      "loss": 0.0886,
      "step": 1626
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.013609356246888638,
      "learning_rate": 0.01322929671244278,
      "loss": 0.0729,
      "step": 1627
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.02055077999830246,
      "learning_rate": 0.013225135247607158,
      "loss": 0.2935,
      "step": 1628
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.022046174854040146,
      "learning_rate": 0.013220973782771536,
      "loss": 0.2053,
      "step": 1629
    },
    {
      "epoch": 2.03,
      "grad_norm": 0.021528590470552444,
      "learning_rate": 0.013216812317935913,
      "loss": 0.1631,
      "step": 1630
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.031411800533533096,
      "learning_rate": 0.013212650853100291,
      "loss": 0.2817,
      "step": 1631
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.02549927867949009,
      "learning_rate": 0.013208489388264669,
      "loss": 0.2854,
      "step": 1632
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.017522094771265984,
      "learning_rate": 0.013204327923429047,
      "loss": 0.2261,
      "step": 1633
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.030025748535990715,
      "learning_rate": 0.013200166458593426,
      "loss": 0.5234,
      "step": 1634
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.028304047882556915,
      "learning_rate": 0.013196004993757803,
      "loss": 0.2539,
      "step": 1635
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.012077707797288895,
      "learning_rate": 0.01319184352892218,
      "loss": 0.0184,
      "step": 1636
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.01997710019350052,
      "learning_rate": 0.01318768206408656,
      "loss": 0.1492,
      "step": 1637
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.025474051013588905,
      "learning_rate": 0.013183520599250936,
      "loss": 0.343,
      "step": 1638
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.0002958629629574716,
      "learning_rate": 0.013179359134415314,
      "loss": 0.0003,
      "step": 1639
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.021819638088345528,
      "learning_rate": 0.013175197669579693,
      "loss": 0.2086,
      "step": 1640
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.011794944293797016,
      "learning_rate": 0.013171036204744071,
      "loss": 0.0276,
      "step": 1641
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.01659025438129902,
      "learning_rate": 0.013166874739908447,
      "loss": 0.182,
      "step": 1642
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.012915789149701595,
      "learning_rate": 0.013162713275072827,
      "loss": 0.0558,
      "step": 1643
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.017592914402484894,
      "learning_rate": 0.013158551810237205,
      "loss": 0.1312,
      "step": 1644
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.0005364430835470557,
      "learning_rate": 0.01315439034540158,
      "loss": 0.0005,
      "step": 1645
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.01412869431078434,
      "learning_rate": 0.01315022888056596,
      "loss": 0.1173,
      "step": 1646
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.0199842918664217,
      "learning_rate": 0.013146067415730338,
      "loss": 0.2228,
      "step": 1647
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.01201882865279913,
      "learning_rate": 0.013141905950894714,
      "loss": 0.0471,
      "step": 1648
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.013119431212544441,
      "learning_rate": 0.013137744486059094,
      "loss": 0.1334,
      "step": 1649
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.01940539851784706,
      "learning_rate": 0.013133583021223472,
      "loss": 0.1909,
      "step": 1650
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.009452982805669308,
      "learning_rate": 0.013129421556387848,
      "loss": 0.0349,
      "step": 1651
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.024382147938013077,
      "learning_rate": 0.013125260091552227,
      "loss": 0.2847,
      "step": 1652
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.023987339809536934,
      "learning_rate": 0.013121098626716605,
      "loss": 0.3291,
      "step": 1653
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.023165792226791382,
      "learning_rate": 0.013116937161880981,
      "loss": 0.272,
      "step": 1654
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.021906182169914246,
      "learning_rate": 0.01311277569704536,
      "loss": 0.4868,
      "step": 1655
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.02048407308757305,
      "learning_rate": 0.013108614232209739,
      "loss": 0.0849,
      "step": 1656
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.015138584189116955,
      "learning_rate": 0.013104452767374115,
      "loss": 0.1545,
      "step": 1657
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.018052522093057632,
      "learning_rate": 0.013100291302538494,
      "loss": 0.1119,
      "step": 1658
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.02519669197499752,
      "learning_rate": 0.013096129837702872,
      "loss": 0.1721,
      "step": 1659
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.018198765814304352,
      "learning_rate": 0.013091968372867248,
      "loss": 0.1637,
      "step": 1660
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.02443428337574005,
      "learning_rate": 0.013087806908031628,
      "loss": 0.324,
      "step": 1661
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.022600075230002403,
      "learning_rate": 0.013083645443196006,
      "loss": 0.2937,
      "step": 1662
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.015107464976608753,
      "learning_rate": 0.013079483978360382,
      "loss": 0.111,
      "step": 1663
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.025980006903409958,
      "learning_rate": 0.013075322513524761,
      "loss": 0.4175,
      "step": 1664
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.0201786607503891,
      "learning_rate": 0.013071161048689139,
      "loss": 0.2271,
      "step": 1665
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.01839342713356018,
      "learning_rate": 0.013066999583853517,
      "loss": 0.2261,
      "step": 1666
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.015817902982234955,
      "learning_rate": 0.013062838119017895,
      "loss": 0.0122,
      "step": 1667
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.010912503115832806,
      "learning_rate": 0.013058676654182273,
      "loss": 0.0321,
      "step": 1668
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.010636689141392708,
      "learning_rate": 0.01305451518934665,
      "loss": 0.0421,
      "step": 1669
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.018897121772170067,
      "learning_rate": 0.013050353724511028,
      "loss": 0.1213,
      "step": 1670
    },
    {
      "epoch": 2.09,
      "grad_norm": 0.02843371406197548,
      "learning_rate": 0.013046192259675406,
      "loss": 0.6953,
      "step": 1671
    },
    {
      "epoch": 2.09,
      "grad_norm": 0.022800462320446968,
      "learning_rate": 0.013042030794839784,
      "loss": 0.4038,
      "step": 1672
    },
    {
      "epoch": 2.09,
      "grad_norm": 0.02318783476948738,
      "learning_rate": 0.013037869330004162,
      "loss": 0.14,
      "step": 1673
    },
    {
      "epoch": 2.09,
      "grad_norm": 0.022268908098340034,
      "learning_rate": 0.01303370786516854,
      "loss": 0.324,
      "step": 1674
    },
    {
      "epoch": 2.09,
      "grad_norm": 0.011991837993264198,
      "learning_rate": 0.013029546400332917,
      "loss": 0.0533,
      "step": 1675
    },
    {
      "epoch": 2.09,
      "grad_norm": 0.03072371706366539,
      "learning_rate": 0.013025384935497295,
      "loss": 0.562,
      "step": 1676
    },
    {
      "epoch": 2.09,
      "grad_norm": 0.019452229142189026,
      "learning_rate": 0.013021223470661675,
      "loss": 0.1631,
      "step": 1677
    },
    {
      "epoch": 2.09,
      "grad_norm": 0.017278265208005905,
      "learning_rate": 0.01301706200582605,
      "loss": 0.2473,
      "step": 1678
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.0201958566904068,
      "learning_rate": 0.013012900540990429,
      "loss": 0.1833,
      "step": 1679
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.01935829594731331,
      "learning_rate": 0.013008739076154808,
      "loss": 0.2496,
      "step": 1680
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.0225104708224535,
      "learning_rate": 0.013004577611319184,
      "loss": 0.2739,
      "step": 1681
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.02873700112104416,
      "learning_rate": 0.013000416146483562,
      "loss": 0.262,
      "step": 1682
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.017906788736581802,
      "learning_rate": 0.012996254681647942,
      "loss": 0.134,
      "step": 1683
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.022372907027602196,
      "learning_rate": 0.012992093216812318,
      "loss": 0.2134,
      "step": 1684
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.02147507853806019,
      "learning_rate": 0.012987931751976696,
      "loss": 0.2135,
      "step": 1685
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.034443970769643784,
      "learning_rate": 0.012983770287141075,
      "loss": 0.8525,
      "step": 1686
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.022964680567383766,
      "learning_rate": 0.012979608822305451,
      "loss": 0.1963,
      "step": 1687
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.014793927781283855,
      "learning_rate": 0.012975447357469829,
      "loss": 0.0506,
      "step": 1688
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.025145243853330612,
      "learning_rate": 0.012971285892634209,
      "loss": 0.3696,
      "step": 1689
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.024199428036808968,
      "learning_rate": 0.012967124427798585,
      "loss": 0.142,
      "step": 1690
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.028435448184609413,
      "learning_rate": 0.012962962962962963,
      "loss": 0.3413,
      "step": 1691
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.02907705307006836,
      "learning_rate": 0.012958801498127342,
      "loss": 0.4097,
      "step": 1692
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.02558666095137596,
      "learning_rate": 0.012954640033291718,
      "loss": 0.3674,
      "step": 1693
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.014742371626198292,
      "learning_rate": 0.012950478568456096,
      "loss": 0.1328,
      "step": 1694
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.023654207587242126,
      "learning_rate": 0.012946317103620476,
      "loss": 0.2292,
      "step": 1695
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.010317325592041016,
      "learning_rate": 0.012942155638784852,
      "loss": 0.0391,
      "step": 1696
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.017321204766631126,
      "learning_rate": 0.01293799417394923,
      "loss": 0.1637,
      "step": 1697
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.016671061515808105,
      "learning_rate": 0.012933832709113609,
      "loss": 0.1227,
      "step": 1698
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.017771786078810692,
      "learning_rate": 0.012929671244277985,
      "loss": 0.1838,
      "step": 1699
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.018872709944844246,
      "learning_rate": 0.012925509779442363,
      "loss": 0.3276,
      "step": 1700
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.016168350353837013,
      "learning_rate": 0.012921348314606743,
      "loss": 0.197,
      "step": 1701
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.021536260843276978,
      "learning_rate": 0.012917186849771119,
      "loss": 0.3518,
      "step": 1702
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.010096617043018341,
      "learning_rate": 0.012913025384935497,
      "loss": 0.0455,
      "step": 1703
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.018981944769620895,
      "learning_rate": 0.012908863920099876,
      "loss": 0.2463,
      "step": 1704
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.03540986031293869,
      "learning_rate": 0.012904702455264252,
      "loss": 0.4004,
      "step": 1705
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.024125607684254646,
      "learning_rate": 0.01290054099042863,
      "loss": 0.4705,
      "step": 1706
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.01373572088778019,
      "learning_rate": 0.01289637952559301,
      "loss": 0.1232,
      "step": 1707
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.0213787741959095,
      "learning_rate": 0.012892218060757386,
      "loss": 0.3372,
      "step": 1708
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.02095050737261772,
      "learning_rate": 0.012888056595921765,
      "loss": 0.2788,
      "step": 1709
    },
    {
      "epoch": 2.13,
      "grad_norm": 0.012310524471104145,
      "learning_rate": 0.012883895131086143,
      "loss": 0.0779,
      "step": 1710
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.011398956179618835,
      "learning_rate": 0.01287973366625052,
      "loss": 0.0616,
      "step": 1711
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.017373640090227127,
      "learning_rate": 0.012875572201414899,
      "loss": 0.1494,
      "step": 1712
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.012314626015722752,
      "learning_rate": 0.012871410736579277,
      "loss": 0.118,
      "step": 1713
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.022293422371149063,
      "learning_rate": 0.012867249271743654,
      "loss": 0.2737,
      "step": 1714
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.019693925976753235,
      "learning_rate": 0.012863087806908032,
      "loss": 0.21,
      "step": 1715
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.02608320489525795,
      "learning_rate": 0.01285892634207241,
      "loss": 0.2211,
      "step": 1716
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.021513689309358597,
      "learning_rate": 0.012854764877236788,
      "loss": 0.3203,
      "step": 1717
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.013514185324311256,
      "learning_rate": 0.012850603412401166,
      "loss": 0.1149,
      "step": 1718
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.011935219168663025,
      "learning_rate": 0.012846441947565544,
      "loss": 0.0847,
      "step": 1719
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.020702729001641273,
      "learning_rate": 0.012842280482729923,
      "loss": 0.1835,
      "step": 1720
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.01812521554529667,
      "learning_rate": 0.0128381190178943,
      "loss": 0.1819,
      "step": 1721
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.020141540095210075,
      "learning_rate": 0.012833957553058677,
      "loss": 0.1794,
      "step": 1722
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.021272391080856323,
      "learning_rate": 0.012829796088223057,
      "loss": 0.1572,
      "step": 1723
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.030137259513139725,
      "learning_rate": 0.012825634623387433,
      "loss": 0.2944,
      "step": 1724
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.016211992129683495,
      "learning_rate": 0.01282147315855181,
      "loss": 0.0772,
      "step": 1725
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.021852759644389153,
      "learning_rate": 0.01281731169371619,
      "loss": 0.2075,
      "step": 1726
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.0005227791261859238,
      "learning_rate": 0.012813150228880566,
      "loss": 0.0003,
      "step": 1727
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.01951328106224537,
      "learning_rate": 0.012808988764044944,
      "loss": 0.1294,
      "step": 1728
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.013617655262351036,
      "learning_rate": 0.012804827299209324,
      "loss": 0.0812,
      "step": 1729
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.01828266680240631,
      "learning_rate": 0.0128006658343737,
      "loss": 0.2556,
      "step": 1730
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.013393322005867958,
      "learning_rate": 0.012796504369538077,
      "loss": 0.0816,
      "step": 1731
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.008665254339575768,
      "learning_rate": 0.012792342904702457,
      "loss": 0.0369,
      "step": 1732
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.029513906687498093,
      "learning_rate": 0.012788181439866833,
      "loss": 0.4473,
      "step": 1733
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.013012977316975594,
      "learning_rate": 0.012784019975031211,
      "loss": 0.0293,
      "step": 1734
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.022755471989512444,
      "learning_rate": 0.01277985851019559,
      "loss": 0.2761,
      "step": 1735
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.023543981835246086,
      "learning_rate": 0.012775697045359967,
      "loss": 0.2292,
      "step": 1736
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.015376131050288677,
      "learning_rate": 0.012771535580524344,
      "loss": 0.1335,
      "step": 1737
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.01816118322312832,
      "learning_rate": 0.012767374115688724,
      "loss": 0.1533,
      "step": 1738
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.01712544821202755,
      "learning_rate": 0.0127632126508531,
      "loss": 0.1747,
      "step": 1739
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.02379106730222702,
      "learning_rate": 0.012759051186017478,
      "loss": 0.3462,
      "step": 1740
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.019225219264626503,
      "learning_rate": 0.012754889721181857,
      "loss": 0.241,
      "step": 1741
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.024099858477711678,
      "learning_rate": 0.012750728256346234,
      "loss": 0.3552,
      "step": 1742
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.025145454332232475,
      "learning_rate": 0.012746566791510611,
      "loss": 0.386,
      "step": 1743
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.021665409207344055,
      "learning_rate": 0.012742405326674991,
      "loss": 0.3665,
      "step": 1744
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.021115843206644058,
      "learning_rate": 0.012738243861839367,
      "loss": 0.408,
      "step": 1745
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.027014058083295822,
      "learning_rate": 0.012734082397003745,
      "loss": 0.342,
      "step": 1746
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.025000402703881264,
      "learning_rate": 0.012729920932168124,
      "loss": 0.2671,
      "step": 1747
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.02543284185230732,
      "learning_rate": 0.0127257594673325,
      "loss": 0.072,
      "step": 1748
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.018894080072641373,
      "learning_rate": 0.012721598002496878,
      "loss": 0.2123,
      "step": 1749
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.019118893891572952,
      "learning_rate": 0.012717436537661258,
      "loss": 0.2274,
      "step": 1750
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.0197413582354784,
      "learning_rate": 0.012713275072825634,
      "loss": 0.1769,
      "step": 1751
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.022043924778699875,
      "learning_rate": 0.012709113607990014,
      "loss": 0.3005,
      "step": 1752
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.03925062716007233,
      "learning_rate": 0.012704952143154391,
      "loss": 0.3586,
      "step": 1753
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.024564363062381744,
      "learning_rate": 0.012700790678318767,
      "loss": 0.2976,
      "step": 1754
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.015781976282596588,
      "learning_rate": 0.012696629213483147,
      "loss": 0.0469,
      "step": 1755
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.018775371834635735,
      "learning_rate": 0.012692467748647525,
      "loss": 0.2109,
      "step": 1756
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.016275448724627495,
      "learning_rate": 0.012688306283811901,
      "loss": 0.1538,
      "step": 1757
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.024717513471841812,
      "learning_rate": 0.01268414481897628,
      "loss": 0.2761,
      "step": 1758
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.012261257506906986,
      "learning_rate": 0.012679983354140658,
      "loss": 0.0942,
      "step": 1759
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.011696564964950085,
      "learning_rate": 0.012675821889305034,
      "loss": 0.0612,
      "step": 1760
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.01670779660344124,
      "learning_rate": 0.012671660424469414,
      "loss": 0.1404,
      "step": 1761
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.012784263119101524,
      "learning_rate": 0.012667498959633792,
      "loss": 0.0668,
      "step": 1762
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.024536479264497757,
      "learning_rate": 0.012663337494798168,
      "loss": 0.4033,
      "step": 1763
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.024411968886852264,
      "learning_rate": 0.012659176029962547,
      "loss": 0.4546,
      "step": 1764
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.021367564797401428,
      "learning_rate": 0.012655014565126925,
      "loss": 0.1196,
      "step": 1765
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.0565548874437809,
      "learning_rate": 0.012650853100291301,
      "loss": 0.2274,
      "step": 1766
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.01742355339229107,
      "learning_rate": 0.012646691635455681,
      "loss": 0.2161,
      "step": 1767
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.021760426461696625,
      "learning_rate": 0.012642530170620059,
      "loss": 0.1804,
      "step": 1768
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.014957339502871037,
      "learning_rate": 0.012638368705784435,
      "loss": 0.1117,
      "step": 1769
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.021692868322134018,
      "learning_rate": 0.012634207240948814,
      "loss": 0.3481,
      "step": 1770
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.018708378076553345,
      "learning_rate": 0.012630045776113192,
      "loss": 0.1046,
      "step": 1771
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.020960671827197075,
      "learning_rate": 0.012625884311277568,
      "loss": 0.324,
      "step": 1772
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.020688634365797043,
      "learning_rate": 0.012621722846441948,
      "loss": 0.1423,
      "step": 1773
    },
    {
      "epoch": 2.21,
      "grad_norm": 0.02408893033862114,
      "learning_rate": 0.012617561381606326,
      "loss": 0.0943,
      "step": 1774
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.026111457496881485,
      "learning_rate": 0.012613399916770702,
      "loss": 0.1854,
      "step": 1775
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.025772029533982277,
      "learning_rate": 0.012609238451935081,
      "loss": 0.1935,
      "step": 1776
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.02326890081167221,
      "learning_rate": 0.01260507698709946,
      "loss": 0.0387,
      "step": 1777
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.02084415592253208,
      "learning_rate": 0.012600915522263835,
      "loss": 0.2612,
      "step": 1778
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.017776021733880043,
      "learning_rate": 0.012596754057428215,
      "loss": 0.0349,
      "step": 1779
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.03797101229429245,
      "learning_rate": 0.012592592592592593,
      "loss": 0.5552,
      "step": 1780
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.01763046905398369,
      "learning_rate": 0.012588431127756969,
      "loss": 0.2722,
      "step": 1781
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.01851477101445198,
      "learning_rate": 0.012584269662921348,
      "loss": 0.0421,
      "step": 1782
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.017429951578378677,
      "learning_rate": 0.012580108198085726,
      "loss": 0.1458,
      "step": 1783
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.021787606179714203,
      "learning_rate": 0.012575946733250104,
      "loss": 0.0351,
      "step": 1784
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.02523845247924328,
      "learning_rate": 0.012571785268414482,
      "loss": 0.2856,
      "step": 1785
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.021925432607531548,
      "learning_rate": 0.01256762380357886,
      "loss": 0.1719,
      "step": 1786
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.02145892009139061,
      "learning_rate": 0.01256346233874324,
      "loss": 0.2744,
      "step": 1787
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.013567307032644749,
      "learning_rate": 0.012559300873907615,
      "loss": 0.0576,
      "step": 1788
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.01906873844563961,
      "learning_rate": 0.012555139409071993,
      "loss": 0.2202,
      "step": 1789
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.00021833447681274265,
      "learning_rate": 0.012550977944236373,
      "loss": 0.0002,
      "step": 1790
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.021508393809199333,
      "learning_rate": 0.012546816479400749,
      "loss": 0.2742,
      "step": 1791
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.023440269753336906,
      "learning_rate": 0.012542655014565127,
      "loss": 0.1541,
      "step": 1792
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.020933520048856735,
      "learning_rate": 0.012538493549729506,
      "loss": 0.1992,
      "step": 1793
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.021189365535974503,
      "learning_rate": 0.012534332084893882,
      "loss": 0.2407,
      "step": 1794
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.018494414165616035,
      "learning_rate": 0.012530170620058262,
      "loss": 0.1201,
      "step": 1795
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.022760309278964996,
      "learning_rate": 0.01252600915522264,
      "loss": 0.4058,
      "step": 1796
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.010575392283499241,
      "learning_rate": 0.012521847690387016,
      "loss": 0.056,
      "step": 1797
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.011404070071876049,
      "learning_rate": 0.012517686225551395,
      "loss": 0.0747,
      "step": 1798
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.023342575877904892,
      "learning_rate": 0.012513524760715773,
      "loss": 0.2676,
      "step": 1799
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.013618412427604198,
      "learning_rate": 0.01250936329588015,
      "loss": 0.0645,
      "step": 1800
    },
    {
      "epoch": 2.25,
      "eval_loss": 0.251953125,
      "eval_runtime": 183.1445,
      "eval_samples_per_second": 1.097,
      "eval_steps_per_second": 0.551,
      "step": 1800
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.016486255452036858,
      "learning_rate": 0.012505201831044529,
      "loss": 0.0886,
      "step": 1801
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.008469241671264172,
      "learning_rate": 0.012501040366208907,
      "loss": 0.0339,
      "step": 1802
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.03356185927987099,
      "learning_rate": 0.012496878901373283,
      "loss": 0.2452,
      "step": 1803
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.014215693809092045,
      "learning_rate": 0.012492717436537662,
      "loss": 0.0206,
      "step": 1804
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.021544774994254112,
      "learning_rate": 0.01248855597170204,
      "loss": 0.2705,
      "step": 1805
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.02142452634871006,
      "learning_rate": 0.012484394506866416,
      "loss": 0.26,
      "step": 1806
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.026130953803658485,
      "learning_rate": 0.012480233042030796,
      "loss": 0.2656,
      "step": 1807
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.021546989679336548,
      "learning_rate": 0.012476071577195174,
      "loss": 0.2825,
      "step": 1808
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.013771620579063892,
      "learning_rate": 0.01247191011235955,
      "loss": 0.1382,
      "step": 1809
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.019164208322763443,
      "learning_rate": 0.01246774864752393,
      "loss": 0.1929,
      "step": 1810
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.02758360095322132,
      "learning_rate": 0.012463587182688307,
      "loss": 0.2996,
      "step": 1811
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.022719966247677803,
      "learning_rate": 0.012459425717852683,
      "loss": 0.366,
      "step": 1812
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.005049921106547117,
      "learning_rate": 0.012455264253017063,
      "loss": 0.0114,
      "step": 1813
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.0013500327477231622,
      "learning_rate": 0.01245110278818144,
      "loss": 0.0011,
      "step": 1814
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.02077244222164154,
      "learning_rate": 0.012446941323345817,
      "loss": 0.1272,
      "step": 1815
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.0193160567432642,
      "learning_rate": 0.012442779858510196,
      "loss": 0.1309,
      "step": 1816
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.01908847689628601,
      "learning_rate": 0.012438618393674574,
      "loss": 0.2634,
      "step": 1817
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.031074656173586845,
      "learning_rate": 0.01243445692883895,
      "loss": 0.3201,
      "step": 1818
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.018704503774642944,
      "learning_rate": 0.01243029546400333,
      "loss": 0.1006,
      "step": 1819
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.010487022809684277,
      "learning_rate": 0.012426133999167708,
      "loss": 0.0319,
      "step": 1820
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.01835411973297596,
      "learning_rate": 0.012421972534332084,
      "loss": 0.2015,
      "step": 1821
    },
    {
      "epoch": 2.27,
      "grad_norm": 0.01159248873591423,
      "learning_rate": 0.012417811069496463,
      "loss": 0.0353,
      "step": 1822
    },
    {
      "epoch": 2.28,
      "grad_norm": 0.014018703252077103,
      "learning_rate": 0.012413649604660841,
      "loss": 0.0803,
      "step": 1823
    },
    {
      "epoch": 2.28,
      "grad_norm": 0.025129087269306183,
      "learning_rate": 0.012409488139825217,
      "loss": 0.1327,
      "step": 1824
    },
    {
      "epoch": 2.28,
      "grad_norm": 0.017635060474276543,
      "learning_rate": 0.012405326674989597,
      "loss": 0.4978,
      "step": 1825
    },
    {
      "epoch": 2.28,
      "grad_norm": 0.02411811798810959,
      "learning_rate": 0.012401165210153975,
      "loss": 0.3347,
      "step": 1826
    },
    {
      "epoch": 2.28,
      "grad_norm": 0.002025188645347953,
      "learning_rate": 0.012397003745318352,
      "loss": 0.0014,
      "step": 1827
    },
    {
      "epoch": 2.28,
      "grad_norm": 0.022905437275767326,
      "learning_rate": 0.01239284228048273,
      "loss": 0.3005,
      "step": 1828
    },
    {
      "epoch": 2.28,
      "grad_norm": 0.0184318870306015,
      "learning_rate": 0.012388680815647108,
      "loss": 0.1735,
      "step": 1829
    },
    {
      "epoch": 2.28,
      "grad_norm": 0.026125553995370865,
      "learning_rate": 0.012384519350811486,
      "loss": 0.2637,
      "step": 1830
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.020487291738390923,
      "learning_rate": 0.012380357885975864,
      "loss": 0.1186,
      "step": 1831
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.023066051304340363,
      "learning_rate": 0.012376196421140242,
      "loss": 0.2952,
      "step": 1832
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.012332306243479252,
      "learning_rate": 0.01237203495630462,
      "loss": 0.061,
      "step": 1833
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.0004687734181061387,
      "learning_rate": 0.012367873491468997,
      "loss": 0.0005,
      "step": 1834
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.016090085729956627,
      "learning_rate": 0.012363712026633375,
      "loss": 0.1273,
      "step": 1835
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.030367927625775337,
      "learning_rate": 0.012359550561797753,
      "loss": 0.4155,
      "step": 1836
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.017326489090919495,
      "learning_rate": 0.01235538909696213,
      "loss": 0.1104,
      "step": 1837
    },
    {
      "epoch": 2.29,
      "grad_norm": 0.03511833772063255,
      "learning_rate": 0.01235122763212651,
      "loss": 0.3936,
      "step": 1838
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.020805077627301216,
      "learning_rate": 0.012347066167290886,
      "loss": 0.1957,
      "step": 1839
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.007480517961084843,
      "learning_rate": 0.012342904702455264,
      "loss": 0.0259,
      "step": 1840
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.016606349498033524,
      "learning_rate": 0.012338743237619644,
      "loss": 0.0808,
      "step": 1841
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.017432518303394318,
      "learning_rate": 0.01233458177278402,
      "loss": 0.0542,
      "step": 1842
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.020333752036094666,
      "learning_rate": 0.012330420307948398,
      "loss": 0.1332,
      "step": 1843
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.009812995791435242,
      "learning_rate": 0.012326258843112777,
      "loss": 0.0486,
      "step": 1844
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.015797695145010948,
      "learning_rate": 0.012322097378277153,
      "loss": 0.3103,
      "step": 1845
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.019061453640460968,
      "learning_rate": 0.012317935913441531,
      "loss": 0.1917,
      "step": 1846
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.015453102067112923,
      "learning_rate": 0.01231377444860591,
      "loss": 0.098,
      "step": 1847
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.022252952679991722,
      "learning_rate": 0.012309612983770287,
      "loss": 0.1353,
      "step": 1848
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.024577675387263298,
      "learning_rate": 0.012305451518934665,
      "loss": 0.1917,
      "step": 1849
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.01806916482746601,
      "learning_rate": 0.012301290054099044,
      "loss": 0.1254,
      "step": 1850
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.014022821560502052,
      "learning_rate": 0.01229712858926342,
      "loss": 0.1105,
      "step": 1851
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.013166259042918682,
      "learning_rate": 0.012292967124427798,
      "loss": 0.0931,
      "step": 1852
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.020649565383791924,
      "learning_rate": 0.012288805659592178,
      "loss": 0.2498,
      "step": 1853
    },
    {
      "epoch": 2.31,
      "grad_norm": 0.022913789376616478,
      "learning_rate": 0.012284644194756554,
      "loss": 0.2229,
      "step": 1854
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.03150669112801552,
      "learning_rate": 0.012280482729920932,
      "loss": 0.3186,
      "step": 1855
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.02068236470222473,
      "learning_rate": 0.012276321265085311,
      "loss": 0.3945,
      "step": 1856
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.023906230926513672,
      "learning_rate": 0.012272159800249689,
      "loss": 0.2939,
      "step": 1857
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.01015520840883255,
      "learning_rate": 0.012267998335414065,
      "loss": 0.0289,
      "step": 1858
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.02434595674276352,
      "learning_rate": 0.012263836870578445,
      "loss": 0.2378,
      "step": 1859
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.01734810508787632,
      "learning_rate": 0.012259675405742822,
      "loss": 0.1169,
      "step": 1860
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.021651124581694603,
      "learning_rate": 0.012255513940907199,
      "loss": 0.1771,
      "step": 1861
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.01195518858730793,
      "learning_rate": 0.012251352476071578,
      "loss": 0.0727,
      "step": 1862
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.01687430404126644,
      "learning_rate": 0.012247191011235956,
      "loss": 0.1152,
      "step": 1863
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.010500581003725529,
      "learning_rate": 0.012243029546400332,
      "loss": 0.0439,
      "step": 1864
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.009298551827669144,
      "learning_rate": 0.012238868081564712,
      "loss": 0.0289,
      "step": 1865
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.020472731441259384,
      "learning_rate": 0.01223470661672909,
      "loss": 0.311,
      "step": 1866
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.021735133603215218,
      "learning_rate": 0.012230545151893465,
      "loss": 0.1703,
      "step": 1867
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.033106472343206406,
      "learning_rate": 0.012226383687057845,
      "loss": 0.5161,
      "step": 1868
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.01586393639445305,
      "learning_rate": 0.012222222222222223,
      "loss": 0.1373,
      "step": 1869
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.021682174876332283,
      "learning_rate": 0.0122180607573866,
      "loss": 0.1785,
      "step": 1870
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.022518862038850784,
      "learning_rate": 0.012213899292550979,
      "loss": 0.3774,
      "step": 1871
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.021802758798003197,
      "learning_rate": 0.012209737827715356,
      "loss": 0.1974,
      "step": 1872
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.024974944069981575,
      "learning_rate": 0.012205576362879734,
      "loss": 0.2634,
      "step": 1873
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.03450498357415199,
      "learning_rate": 0.012201414898044112,
      "loss": 0.5083,
      "step": 1874
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.018969617784023285,
      "learning_rate": 0.01219725343320849,
      "loss": 0.2091,
      "step": 1875
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.0331437811255455,
      "learning_rate": 0.012193091968372868,
      "loss": 0.3118,
      "step": 1876
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.01985146664083004,
      "learning_rate": 0.012188930503537245,
      "loss": 0.1472,
      "step": 1877
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.01084665022790432,
      "learning_rate": 0.012184769038701623,
      "loss": 0.0737,
      "step": 1878
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.012392666190862656,
      "learning_rate": 0.012180607573866001,
      "loss": 0.0456,
      "step": 1879
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.026068760082125664,
      "learning_rate": 0.012176446109030379,
      "loss": 0.2966,
      "step": 1880
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.008604968897998333,
      "learning_rate": 0.012172284644194759,
      "loss": 0.0271,
      "step": 1881
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.02993691898882389,
      "learning_rate": 0.012168123179359135,
      "loss": 0.4895,
      "step": 1882
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.0002020105894189328,
      "learning_rate": 0.012163961714523512,
      "loss": 0.0003,
      "step": 1883
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.01729712262749672,
      "learning_rate": 0.012159800249687892,
      "loss": 0.1438,
      "step": 1884
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.027865923941135406,
      "learning_rate": 0.012155638784852268,
      "loss": 0.3503,
      "step": 1885
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.021646926179528236,
      "learning_rate": 0.012151477320016646,
      "loss": 0.2018,
      "step": 1886
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.008811386302113533,
      "learning_rate": 0.012147315855181025,
      "loss": 0.0266,
      "step": 1887
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.02644684910774231,
      "learning_rate": 0.012143154390345402,
      "loss": 0.4045,
      "step": 1888
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.012546895071864128,
      "learning_rate": 0.01213899292550978,
      "loss": 0.1126,
      "step": 1889
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.027794374153017998,
      "learning_rate": 0.012134831460674159,
      "loss": 0.3489,
      "step": 1890
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.027903271839022636,
      "learning_rate": 0.012130669995838535,
      "loss": 0.3052,
      "step": 1891
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.008725191466510296,
      "learning_rate": 0.012126508531002913,
      "loss": 0.0288,
      "step": 1892
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.015588279813528061,
      "learning_rate": 0.012122347066167292,
      "loss": 0.0606,
      "step": 1893
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.029757807031273842,
      "learning_rate": 0.012118185601331669,
      "loss": 0.4277,
      "step": 1894
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.02568071335554123,
      "learning_rate": 0.012114024136496046,
      "loss": 0.4229,
      "step": 1895
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.0349445715546608,
      "learning_rate": 0.012109862671660426,
      "loss": 0.4414,
      "step": 1896
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.0034528723917901516,
      "learning_rate": 0.012105701206824802,
      "loss": 0.0018,
      "step": 1897
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.018802698701620102,
      "learning_rate": 0.01210153974198918,
      "loss": 0.21,
      "step": 1898
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.019617371261119843,
      "learning_rate": 0.01209737827715356,
      "loss": 0.25,
      "step": 1899
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.032445214688777924,
      "learning_rate": 0.012093216812317936,
      "loss": 0.1919,
      "step": 1900
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.019002187997102737,
      "learning_rate": 0.012089055347482313,
      "loss": 0.2349,
      "step": 1901
    },
    {
      "epoch": 2.37,
      "grad_norm": 0.02349962294101715,
      "learning_rate": 0.012084893882646693,
      "loss": 0.3059,
      "step": 1902
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.01358178723603487,
      "learning_rate": 0.012080732417811069,
      "loss": 0.0392,
      "step": 1903
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.028780268505215645,
      "learning_rate": 0.012076570952975447,
      "loss": 0.3589,
      "step": 1904
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.020696232095360756,
      "learning_rate": 0.012072409488139826,
      "loss": 0.1188,
      "step": 1905
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.023318219929933548,
      "learning_rate": 0.012068248023304202,
      "loss": 0.193,
      "step": 1906
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.0182783305644989,
      "learning_rate": 0.01206408655846858,
      "loss": 0.2217,
      "step": 1907
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.0189969465136528,
      "learning_rate": 0.01205992509363296,
      "loss": 0.1647,
      "step": 1908
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.024886412546038628,
      "learning_rate": 0.012055763628797336,
      "loss": 0.2142,
      "step": 1909
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.017747843638062477,
      "learning_rate": 0.012051602163961714,
      "loss": 0.1251,
      "step": 1910
    },
    {
      "epoch": 2.39,
      "grad_norm": 0.012248966842889786,
      "learning_rate": 0.012047440699126093,
      "loss": 0.0526,
      "step": 1911
    },
    {
      "epoch": 2.39,
      "grad_norm": 0.01696588471531868,
      "learning_rate": 0.01204327923429047,
      "loss": 0.0892,
      "step": 1912
    },
    {
      "epoch": 2.39,
      "grad_norm": 0.027560202404856682,
      "learning_rate": 0.012039117769454849,
      "loss": 0.1658,
      "step": 1913
    },
    {
      "epoch": 2.39,
      "grad_norm": 0.027480104938149452,
      "learning_rate": 0.012034956304619227,
      "loss": 0.2024,
      "step": 1914
    },
    {
      "epoch": 2.39,
      "grad_norm": 0.021120594814419746,
      "learning_rate": 0.012030794839783603,
      "loss": 0.0771,
      "step": 1915
    },
    {
      "epoch": 2.39,
      "grad_norm": 0.019520888105034828,
      "learning_rate": 0.012026633374947982,
      "loss": 0.2732,
      "step": 1916
    },
    {
      "epoch": 2.39,
      "grad_norm": 0.03307688608765602,
      "learning_rate": 0.01202247191011236,
      "loss": 0.1693,
      "step": 1917
    },
    {
      "epoch": 2.39,
      "grad_norm": 0.03119327314198017,
      "learning_rate": 0.012018310445276736,
      "loss": 0.2932,
      "step": 1918
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.018768666312098503,
      "learning_rate": 0.012014148980441116,
      "loss": 0.3735,
      "step": 1919
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.024309314787387848,
      "learning_rate": 0.012009987515605494,
      "loss": 0.1683,
      "step": 1920
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.021070802584290504,
      "learning_rate": 0.01200582605076987,
      "loss": 0.1631,
      "step": 1921
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.01973869279026985,
      "learning_rate": 0.01200166458593425,
      "loss": 0.2062,
      "step": 1922
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.01534019410610199,
      "learning_rate": 0.011997503121098627,
      "loss": 0.0764,
      "step": 1923
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.024141544476151466,
      "learning_rate": 0.011993341656263003,
      "loss": 0.2852,
      "step": 1924
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.0318789929151535,
      "learning_rate": 0.011989180191427383,
      "loss": 0.3037,
      "step": 1925
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.016148686408996582,
      "learning_rate": 0.01198501872659176,
      "loss": 0.1214,
      "step": 1926
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.023784421384334564,
      "learning_rate": 0.011980857261756137,
      "loss": 0.3372,
      "step": 1927
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.02062315307557583,
      "learning_rate": 0.011976695796920516,
      "loss": 0.1698,
      "step": 1928
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.016698716208338737,
      "learning_rate": 0.011972534332084894,
      "loss": 0.1273,
      "step": 1929
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.027297908440232277,
      "learning_rate": 0.011968372867249274,
      "loss": 0.2463,
      "step": 1930
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.02600211277604103,
      "learning_rate": 0.01196421140241365,
      "loss": 0.2112,
      "step": 1931
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.021173592656850815,
      "learning_rate": 0.011960049937578028,
      "loss": 0.1277,
      "step": 1932
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.024990539997816086,
      "learning_rate": 0.011955888472742407,
      "loss": 0.2988,
      "step": 1933
    },
    {
      "epoch": 2.41,
      "grad_norm": 0.0165854599326849,
      "learning_rate": 0.011951727007906783,
      "loss": 0.2456,
      "step": 1934
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.016070079058408737,
      "learning_rate": 0.011947565543071161,
      "loss": 0.047,
      "step": 1935
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.028081314638257027,
      "learning_rate": 0.01194340407823554,
      "loss": 0.2812,
      "step": 1936
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.03318994119763374,
      "learning_rate": 0.011939242613399917,
      "loss": 0.1187,
      "step": 1937
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.02818072773516178,
      "learning_rate": 0.011935081148564295,
      "loss": 0.4507,
      "step": 1938
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.023260237649083138,
      "learning_rate": 0.011930919683728674,
      "loss": 0.2115,
      "step": 1939
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.015680475160479546,
      "learning_rate": 0.01192675821889305,
      "loss": 0.1499,
      "step": 1940
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.0123570766299963,
      "learning_rate": 0.011922596754057428,
      "loss": 0.0749,
      "step": 1941
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.021797947585582733,
      "learning_rate": 0.011918435289221808,
      "loss": 0.2671,
      "step": 1942
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.025639662519097328,
      "learning_rate": 0.011914273824386184,
      "loss": 0.089,
      "step": 1943
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.020334098488092422,
      "learning_rate": 0.011910112359550562,
      "loss": 0.302,
      "step": 1944
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.019590269774198532,
      "learning_rate": 0.011905950894714941,
      "loss": 0.2234,
      "step": 1945
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.022233428433537483,
      "learning_rate": 0.011901789429879317,
      "loss": 0.21,
      "step": 1946
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.034159693866968155,
      "learning_rate": 0.011897627965043695,
      "loss": 0.353,
      "step": 1947
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.01791323721408844,
      "learning_rate": 0.011893466500208075,
      "loss": 0.1832,
      "step": 1948
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.021393850445747375,
      "learning_rate": 0.01188930503537245,
      "loss": 0.2583,
      "step": 1949
    },
    {
      "epoch": 2.43,
      "grad_norm": 0.014646848663687706,
      "learning_rate": 0.011885143570536829,
      "loss": 0.1036,
      "step": 1950
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.026806628331542015,
      "learning_rate": 0.011880982105701208,
      "loss": 0.4524,
      "step": 1951
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.00761722307652235,
      "learning_rate": 0.011876820640865584,
      "loss": 0.0073,
      "step": 1952
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.044747233390808105,
      "learning_rate": 0.011872659176029962,
      "loss": 0.2372,
      "step": 1953
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.017829807475209236,
      "learning_rate": 0.011868497711194342,
      "loss": 0.1729,
      "step": 1954
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.02320563793182373,
      "learning_rate": 0.011864336246358718,
      "loss": 0.2202,
      "step": 1955
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.020607536658644676,
      "learning_rate": 0.011860174781523097,
      "loss": 0.1813,
      "step": 1956
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.018707795068621635,
      "learning_rate": 0.011856013316687475,
      "loss": 0.1034,
      "step": 1957
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.020374400541186333,
      "learning_rate": 0.011851851851851851,
      "loss": 0.1608,
      "step": 1958
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.020339541137218475,
      "learning_rate": 0.01184769038701623,
      "loss": 0.1302,
      "step": 1959
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.02563556656241417,
      "learning_rate": 0.011843528922180609,
      "loss": 0.2874,
      "step": 1960
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.022346200421452522,
      "learning_rate": 0.011839367457344985,
      "loss": 0.2316,
      "step": 1961
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.024537915363907814,
      "learning_rate": 0.011835205992509364,
      "loss": 0.2129,
      "step": 1962
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.009378829039633274,
      "learning_rate": 0.011831044527673742,
      "loss": 0.0388,
      "step": 1963
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.014937441796064377,
      "learning_rate": 0.011826883062838118,
      "loss": 0.0678,
      "step": 1964
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.020893579348921776,
      "learning_rate": 0.011822721598002498,
      "loss": 0.1985,
      "step": 1965
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.005350376013666391,
      "learning_rate": 0.011818560133166876,
      "loss": 0.0184,
      "step": 1966
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.020139511674642563,
      "learning_rate": 0.011814398668331252,
      "loss": 0.1797,
      "step": 1967
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.02266424335539341,
      "learning_rate": 0.011810237203495631,
      "loss": 0.2739,
      "step": 1968
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.018839532509446144,
      "learning_rate": 0.011806075738660009,
      "loss": 0.1594,
      "step": 1969
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.02105688489973545,
      "learning_rate": 0.011801914273824385,
      "loss": 0.0537,
      "step": 1970
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.021764514967799187,
      "learning_rate": 0.011797752808988765,
      "loss": 0.0898,
      "step": 1971
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.013319112360477448,
      "learning_rate": 0.011793591344153143,
      "loss": 0.0473,
      "step": 1972
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.0188334621489048,
      "learning_rate": 0.011789429879317519,
      "loss": 0.3579,
      "step": 1973
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.025334224104881287,
      "learning_rate": 0.011785268414481898,
      "loss": 0.2717,
      "step": 1974
    },
    {
      "epoch": 2.47,
      "grad_norm": 0.026538116857409477,
      "learning_rate": 0.011781106949646276,
      "loss": 0.2815,
      "step": 1975
    },
    {
      "epoch": 2.47,
      "grad_norm": 0.014819093979895115,
      "learning_rate": 0.011776945484810652,
      "loss": 0.0517,
      "step": 1976
    },
    {
      "epoch": 2.47,
      "grad_norm": 0.0019321287982165813,
      "learning_rate": 0.011772784019975032,
      "loss": 0.0018,
      "step": 1977
    },
    {
      "epoch": 2.47,
      "grad_norm": 0.016798026859760284,
      "learning_rate": 0.01176862255513941,
      "loss": 0.1904,
      "step": 1978
    },
    {
      "epoch": 2.47,
      "grad_norm": 0.02429567463696003,
      "learning_rate": 0.011764461090303786,
      "loss": 0.0912,
      "step": 1979
    },
    {
      "epoch": 2.47,
      "grad_norm": 0.0017638233257457614,
      "learning_rate": 0.011760299625468165,
      "loss": 0.0012,
      "step": 1980
    },
    {
      "epoch": 2.47,
      "grad_norm": 0.008670228533446789,
      "learning_rate": 0.011756138160632543,
      "loss": 0.0224,
      "step": 1981
    },
    {
      "epoch": 2.47,
      "grad_norm": 0.01620330661535263,
      "learning_rate": 0.011751976695796919,
      "loss": 0.0669,
      "step": 1982
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.02096256986260414,
      "learning_rate": 0.011747815230961299,
      "loss": 0.0558,
      "step": 1983
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.029678793624043465,
      "learning_rate": 0.011743653766125677,
      "loss": 0.2676,
      "step": 1984
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.016652289777994156,
      "learning_rate": 0.011739492301290053,
      "loss": 0.1185,
      "step": 1985
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.026163695380091667,
      "learning_rate": 0.011735330836454432,
      "loss": 0.2048,
      "step": 1986
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.012465494684875011,
      "learning_rate": 0.01173116937161881,
      "loss": 0.1512,
      "step": 1987
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.016322586685419083,
      "learning_rate": 0.011727007906783188,
      "loss": 0.0916,
      "step": 1988
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.03419661149382591,
      "learning_rate": 0.011722846441947566,
      "loss": 0.4878,
      "step": 1989
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.02087435871362686,
      "learning_rate": 0.011718684977111943,
      "loss": 0.1805,
      "step": 1990
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.03075890801846981,
      "learning_rate": 0.011714523512276321,
      "loss": 0.303,
      "step": 1991
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.010636554099619389,
      "learning_rate": 0.0117103620474407,
      "loss": 0.0361,
      "step": 1992
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.019454382359981537,
      "learning_rate": 0.011706200582605077,
      "loss": 0.1602,
      "step": 1993
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.016859447583556175,
      "learning_rate": 0.011702039117769455,
      "loss": 0.0943,
      "step": 1994
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.022045627236366272,
      "learning_rate": 0.011697877652933833,
      "loss": 0.2336,
      "step": 1995
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.019401608034968376,
      "learning_rate": 0.01169371618809821,
      "loss": 0.2,
      "step": 1996
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.007986395619809628,
      "learning_rate": 0.011689554723262588,
      "loss": 0.0196,
      "step": 1997
    },
    {
      "epoch": 2.49,
      "grad_norm": 0.011389635503292084,
      "learning_rate": 0.011685393258426966,
      "loss": 0.0171,
      "step": 1998
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.013337141834199429,
      "learning_rate": 0.011681231793591346,
      "loss": 0.107,
      "step": 1999
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.024776192381978035,
      "learning_rate": 0.011677070328755722,
      "loss": 0.2339,
      "step": 2000
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.01828068122267723,
      "learning_rate": 0.0116729088639201,
      "loss": 0.0971,
      "step": 2001
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.019044531509280205,
      "learning_rate": 0.01166874739908448,
      "loss": 0.2908,
      "step": 2002
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.025801274925470352,
      "learning_rate": 0.011664585934248857,
      "loss": 0.2761,
      "step": 2003
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.025065390393137932,
      "learning_rate": 0.011660424469413233,
      "loss": 0.1676,
      "step": 2004
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.020172856748104095,
      "learning_rate": 0.011656263004577613,
      "loss": 0.3728,
      "step": 2005
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.021477796137332916,
      "learning_rate": 0.01165210153974199,
      "loss": 0.1234,
      "step": 2006
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.0145299406722188,
      "learning_rate": 0.011647940074906367,
      "loss": 0.3372,
      "step": 2007
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.025115299969911575,
      "learning_rate": 0.011643778610070746,
      "loss": 0.3069,
      "step": 2008
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.017047403380274773,
      "learning_rate": 0.011639617145235124,
      "loss": 0.1847,
      "step": 2009
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.0003101731126662344,
      "learning_rate": 0.0116354556803995,
      "loss": 0.0003,
      "step": 2010
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.021424954757094383,
      "learning_rate": 0.01163129421556388,
      "loss": 0.1871,
      "step": 2011
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.01358071155846119,
      "learning_rate": 0.011627132750728257,
      "loss": 0.1212,
      "step": 2012
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.01648964174091816,
      "learning_rate": 0.011622971285892634,
      "loss": 0.0873,
      "step": 2013
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.017280202358961105,
      "learning_rate": 0.011618809821057013,
      "loss": 0.2085,
      "step": 2014
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.02702498622238636,
      "learning_rate": 0.011614648356221391,
      "loss": 0.4531,
      "step": 2015
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.02316916175186634,
      "learning_rate": 0.011610486891385767,
      "loss": 0.1339,
      "step": 2016
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.02183007448911667,
      "learning_rate": 0.011606325426550147,
      "loss": 0.2478,
      "step": 2017
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.015223664231598377,
      "learning_rate": 0.011602163961714524,
      "loss": 0.1744,
      "step": 2018
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.012189404107630253,
      "learning_rate": 0.0115980024968789,
      "loss": 0.0419,
      "step": 2019
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.0195587370544672,
      "learning_rate": 0.01159384103204328,
      "loss": 0.1427,
      "step": 2020
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.007658170536160469,
      "learning_rate": 0.011589679567207658,
      "loss": 0.0297,
      "step": 2021
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.016460761427879333,
      "learning_rate": 0.011585518102372034,
      "loss": 0.239,
      "step": 2022
    },
    {
      "epoch": 2.53,
      "grad_norm": 0.012173684313893318,
      "learning_rate": 0.011581356637536414,
      "loss": 0.0333,
      "step": 2023
    },
    {
      "epoch": 2.53,
      "grad_norm": 0.026361901313066483,
      "learning_rate": 0.011577195172700791,
      "loss": 0.2751,
      "step": 2024
    },
    {
      "epoch": 2.53,
      "grad_norm": 0.019348539412021637,
      "learning_rate": 0.011573033707865167,
      "loss": 0.1646,
      "step": 2025
    },
    {
      "epoch": 2.53,
      "grad_norm": 0.015237891115248203,
      "learning_rate": 0.011568872243029547,
      "loss": 0.1884,
      "step": 2026
    },
    {
      "epoch": 2.53,
      "grad_norm": 0.0019912971183657646,
      "learning_rate": 0.011564710778193925,
      "loss": 0.0017,
      "step": 2027
    },
    {
      "epoch": 2.53,
      "grad_norm": 0.012761551886796951,
      "learning_rate": 0.011560549313358301,
      "loss": 0.0782,
      "step": 2028
    },
    {
      "epoch": 2.53,
      "grad_norm": 0.02551979385316372,
      "learning_rate": 0.01155638784852268,
      "loss": 0.2893,
      "step": 2029
    },
    {
      "epoch": 2.53,
      "grad_norm": 0.01596144028007984,
      "learning_rate": 0.011552226383687058,
      "loss": 0.1339,
      "step": 2030
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.01283483486622572,
      "learning_rate": 0.011548064918851434,
      "loss": 0.1261,
      "step": 2031
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.016525473445653915,
      "learning_rate": 0.011543903454015814,
      "loss": 0.1154,
      "step": 2032
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.02238711155951023,
      "learning_rate": 0.011539741989180192,
      "loss": 0.0616,
      "step": 2033
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.018807902932167053,
      "learning_rate": 0.01153558052434457,
      "loss": 0.1418,
      "step": 2034
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.028377989307045937,
      "learning_rate": 0.011531419059508947,
      "loss": 0.4375,
      "step": 2035
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.02418394386768341,
      "learning_rate": 0.011527257594673325,
      "loss": 0.1804,
      "step": 2036
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.029496291652321815,
      "learning_rate": 0.011523096129837703,
      "loss": 0.3,
      "step": 2037
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.01621628738939762,
      "learning_rate": 0.011518934665002081,
      "loss": 0.1091,
      "step": 2038
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.028652401641011238,
      "learning_rate": 0.011514773200166459,
      "loss": 0.3975,
      "step": 2039
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.01357332244515419,
      "learning_rate": 0.011510611735330837,
      "loss": 0.0743,
      "step": 2040
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.027345817536115646,
      "learning_rate": 0.011506450270495214,
      "loss": 0.08,
      "step": 2041
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.013164828531444073,
      "learning_rate": 0.011502288805659592,
      "loss": 0.0473,
      "step": 2042
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.01919567957520485,
      "learning_rate": 0.01149812734082397,
      "loss": 0.1533,
      "step": 2043
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.029514610767364502,
      "learning_rate": 0.011493965875988348,
      "loss": 0.5425,
      "step": 2044
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.018476977944374084,
      "learning_rate": 0.011489804411152727,
      "loss": 0.0695,
      "step": 2045
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.007394909858703613,
      "learning_rate": 0.011485642946317104,
      "loss": 0.0231,
      "step": 2046
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.03707277402281761,
      "learning_rate": 0.011481481481481481,
      "loss": 0.8809,
      "step": 2047
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.026237061247229576,
      "learning_rate": 0.011477320016645861,
      "loss": 0.2163,
      "step": 2048
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.01583007723093033,
      "learning_rate": 0.011473158551810237,
      "loss": 0.231,
      "step": 2049
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.027205873280763626,
      "learning_rate": 0.011468997086974615,
      "loss": 0.1342,
      "step": 2050
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.021668871864676476,
      "learning_rate": 0.011464835622138994,
      "loss": 0.0593,
      "step": 2051
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.011056841351091862,
      "learning_rate": 0.01146067415730337,
      "loss": 0.0969,
      "step": 2052
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.019810445606708527,
      "learning_rate": 0.011456512692467748,
      "loss": 0.1445,
      "step": 2053
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.02254723757505417,
      "learning_rate": 0.011452351227632128,
      "loss": 0.3347,
      "step": 2054
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.015462026000022888,
      "learning_rate": 0.011448189762796504,
      "loss": 0.1394,
      "step": 2055
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.01749865710735321,
      "learning_rate": 0.011444028297960882,
      "loss": 0.052,
      "step": 2056
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.024370141327381134,
      "learning_rate": 0.011439866833125261,
      "loss": 0.3943,
      "step": 2057
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.022522548213601112,
      "learning_rate": 0.011435705368289638,
      "loss": 0.3479,
      "step": 2058
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.021285098046064377,
      "learning_rate": 0.011431543903454015,
      "loss": 0.2314,
      "step": 2059
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.015543043613433838,
      "learning_rate": 0.011427382438618395,
      "loss": 0.1482,
      "step": 2060
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.0011899318778887391,
      "learning_rate": 0.011423220973782771,
      "loss": 0.0011,
      "step": 2061
    },
    {
      "epoch": 2.57,
      "grad_norm": 0.014806462451815605,
      "learning_rate": 0.011419059508947149,
      "loss": 0.0871,
      "step": 2062
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.028727039694786072,
      "learning_rate": 0.011414898044111528,
      "loss": 0.2,
      "step": 2063
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.0191753339022398,
      "learning_rate": 0.011410736579275904,
      "loss": 0.1086,
      "step": 2064
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.02818307839334011,
      "learning_rate": 0.011406575114440282,
      "loss": 0.3521,
      "step": 2065
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.017008915543556213,
      "learning_rate": 0.011402413649604662,
      "loss": 0.0756,
      "step": 2066
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.022144189104437828,
      "learning_rate": 0.011398252184769038,
      "loss": 0.184,
      "step": 2067
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.01909795217216015,
      "learning_rate": 0.011394090719933416,
      "loss": 0.214,
      "step": 2068
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.01209740899503231,
      "learning_rate": 0.011389929255097795,
      "loss": 0.048,
      "step": 2069
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.02771034464240074,
      "learning_rate": 0.011385767790262171,
      "loss": 0.2847,
      "step": 2070
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.02340652048587799,
      "learning_rate": 0.01138160632542655,
      "loss": 0.2279,
      "step": 2071
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.0044895377941429615,
      "learning_rate": 0.011377444860590929,
      "loss": 0.0161,
      "step": 2072
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.013708370737731457,
      "learning_rate": 0.011373283395755305,
      "loss": 0.1229,
      "step": 2073
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.021817129105329514,
      "learning_rate": 0.011369121930919683,
      "loss": 0.3433,
      "step": 2074
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.0029963403940200806,
      "learning_rate": 0.011364960466084062,
      "loss": 0.0016,
      "step": 2075
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.022851571440696716,
      "learning_rate": 0.01136079900124844,
      "loss": 0.2659,
      "step": 2076
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.02786555513739586,
      "learning_rate": 0.011356637536412818,
      "loss": 0.4678,
      "step": 2077
    },
    {
      "epoch": 2.59,
      "grad_norm": 0.020445335656404495,
      "learning_rate": 0.011352476071577196,
      "loss": 0.1147,
      "step": 2078
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.014819604344666004,
      "learning_rate": 0.011348314606741574,
      "loss": 0.0953,
      "step": 2079
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.01539096049964428,
      "learning_rate": 0.011344153141905951,
      "loss": 0.0394,
      "step": 2080
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.017781289294362068,
      "learning_rate": 0.01133999167707033,
      "loss": 0.0789,
      "step": 2081
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.021711699664592743,
      "learning_rate": 0.011335830212234707,
      "loss": 0.1622,
      "step": 2082
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.027276895940303802,
      "learning_rate": 0.011331668747399085,
      "loss": 0.2238,
      "step": 2083
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.021106796339154243,
      "learning_rate": 0.011327507282563463,
      "loss": 0.1866,
      "step": 2084
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.01454678550362587,
      "learning_rate": 0.01132334581772784,
      "loss": 0.0591,
      "step": 2085
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.026945922523736954,
      "learning_rate": 0.011319184352892218,
      "loss": 0.0849,
      "step": 2086
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.020662959665060043,
      "learning_rate": 0.011315022888056596,
      "loss": 0.2114,
      "step": 2087
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.02952844649553299,
      "learning_rate": 0.011310861423220976,
      "loss": 0.2097,
      "step": 2088
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.022755775600671768,
      "learning_rate": 0.011306699958385352,
      "loss": 0.3911,
      "step": 2089
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.020654749125242233,
      "learning_rate": 0.01130253849354973,
      "loss": 0.1726,
      "step": 2090
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.009839103557169437,
      "learning_rate": 0.01129837702871411,
      "loss": 0.0461,
      "step": 2091
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.024734919890761375,
      "learning_rate": 0.011294215563878485,
      "loss": 0.1912,
      "step": 2092
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.026316137984395027,
      "learning_rate": 0.011290054099042863,
      "loss": 0.1829,
      "step": 2093
    },
    {
      "epoch": 2.61,
      "grad_norm": 0.0007765085902065039,
      "learning_rate": 0.011285892634207243,
      "loss": 0.0007,
      "step": 2094
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.007204515393823385,
      "learning_rate": 0.011281731169371619,
      "loss": 0.019,
      "step": 2095
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.01746956817805767,
      "learning_rate": 0.011277569704535997,
      "loss": 0.3486,
      "step": 2096
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.016396144405007362,
      "learning_rate": 0.011273408239700376,
      "loss": 0.2156,
      "step": 2097
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.022103311493992805,
      "learning_rate": 0.011269246774864752,
      "loss": 0.1236,
      "step": 2098
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.028456510975956917,
      "learning_rate": 0.01126508531002913,
      "loss": 0.2126,
      "step": 2099
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.017307447269558907,
      "learning_rate": 0.01126092384519351,
      "loss": 0.058,
      "step": 2100
    },
    {
      "epoch": 2.62,
      "eval_loss": 0.254150390625,
      "eval_runtime": 183.083,
      "eval_samples_per_second": 1.098,
      "eval_steps_per_second": 0.552,
      "step": 2100
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.018896162509918213,
      "learning_rate": 0.011256762380357886,
      "loss": 0.0799,
      "step": 2101
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.012350906617939472,
      "learning_rate": 0.011252600915522264,
      "loss": 0.0504,
      "step": 2102
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.03761834651231766,
      "learning_rate": 0.011248439450686643,
      "loss": 0.3323,
      "step": 2103
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.023270580917596817,
      "learning_rate": 0.01124427798585102,
      "loss": 0.2439,
      "step": 2104
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.0007967696874402463,
      "learning_rate": 0.011240116521015397,
      "loss": 0.0006,
      "step": 2105
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.02790345437824726,
      "learning_rate": 0.011235955056179777,
      "loss": 0.2568,
      "step": 2106
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.029952365905046463,
      "learning_rate": 0.011231793591344153,
      "loss": 0.6284,
      "step": 2107
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.030794961377978325,
      "learning_rate": 0.01122763212650853,
      "loss": 0.2284,
      "step": 2108
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.021419581025838852,
      "learning_rate": 0.01122347066167291,
      "loss": 0.3059,
      "step": 2109
    },
    {
      "epoch": 2.63,
      "grad_norm": 0.021761326119303703,
      "learning_rate": 0.011219309196837286,
      "loss": 0.1365,
      "step": 2110
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.016281727701425552,
      "learning_rate": 0.011215147732001664,
      "loss": 0.1967,
      "step": 2111
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.0261228047311306,
      "learning_rate": 0.011210986267166044,
      "loss": 0.2325,
      "step": 2112
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.016307581216096878,
      "learning_rate": 0.01120682480233042,
      "loss": 0.0232,
      "step": 2113
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.018549641594290733,
      "learning_rate": 0.011202663337494798,
      "loss": 0.0729,
      "step": 2114
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.035821691155433655,
      "learning_rate": 0.011198501872659177,
      "loss": 0.2234,
      "step": 2115
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.017618387937545776,
      "learning_rate": 0.011194340407823553,
      "loss": 0.1853,
      "step": 2116
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.01963265985250473,
      "learning_rate": 0.011190178942987931,
      "loss": 0.1908,
      "step": 2117
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.03105011396110058,
      "learning_rate": 0.01118601747815231,
      "loss": 0.365,
      "step": 2118
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.010711747221648693,
      "learning_rate": 0.011181856013316687,
      "loss": 0.0252,
      "step": 2119
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.0012950556119903922,
      "learning_rate": 0.011177694548481066,
      "loss": 0.0006,
      "step": 2120
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.01877252757549286,
      "learning_rate": 0.011173533083645444,
      "loss": 0.0382,
      "step": 2121
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.02206510864198208,
      "learning_rate": 0.01116937161880982,
      "loss": 0.3403,
      "step": 2122
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.01708841137588024,
      "learning_rate": 0.0111652101539742,
      "loss": 0.219,
      "step": 2123
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.013572362251579762,
      "learning_rate": 0.011161048689138578,
      "loss": 0.0799,
      "step": 2124
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.011405447497963905,
      "learning_rate": 0.011156887224302954,
      "loss": 0.0327,
      "step": 2125
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.030020156875252724,
      "learning_rate": 0.011152725759467333,
      "loss": 0.3699,
      "step": 2126
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.024022959172725677,
      "learning_rate": 0.011148564294631711,
      "loss": 0.3606,
      "step": 2127
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.027090931311249733,
      "learning_rate": 0.011144402829796087,
      "loss": 0.2874,
      "step": 2128
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.021377384662628174,
      "learning_rate": 0.011140241364960467,
      "loss": 0.2983,
      "step": 2129
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.012591334991157055,
      "learning_rate": 0.011136079900124845,
      "loss": 0.1121,
      "step": 2130
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.027753202244639397,
      "learning_rate": 0.01113191843528922,
      "loss": 0.385,
      "step": 2131
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.01733223721385002,
      "learning_rate": 0.0111277569704536,
      "loss": 0.2734,
      "step": 2132
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.029551027342677116,
      "learning_rate": 0.011123595505617978,
      "loss": 0.2913,
      "step": 2133
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.017856517806649208,
      "learning_rate": 0.011119434040782354,
      "loss": 0.1989,
      "step": 2134
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.0196613110601902,
      "learning_rate": 0.011115272575946734,
      "loss": 0.1908,
      "step": 2135
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.019325023517012596,
      "learning_rate": 0.011111111111111112,
      "loss": 0.2759,
      "step": 2136
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.01959395408630371,
      "learning_rate": 0.011106949646275488,
      "loss": 0.2896,
      "step": 2137
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.02705358900129795,
      "learning_rate": 0.011102788181439867,
      "loss": 0.3752,
      "step": 2138
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.027229050174355507,
      "learning_rate": 0.011098626716604245,
      "loss": 0.2651,
      "step": 2139
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.02049425058066845,
      "learning_rate": 0.011094465251768621,
      "loss": 0.1498,
      "step": 2140
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.028888262808322906,
      "learning_rate": 0.011090303786933,
      "loss": 0.2576,
      "step": 2141
    },
    {
      "epoch": 2.67,
      "grad_norm": 0.014748776331543922,
      "learning_rate": 0.011086142322097379,
      "loss": 0.1295,
      "step": 2142
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.024524273350834846,
      "learning_rate": 0.011081980857261755,
      "loss": 0.2871,
      "step": 2143
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.025086650624871254,
      "learning_rate": 0.011077819392426134,
      "loss": 0.2258,
      "step": 2144
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.009429898113012314,
      "learning_rate": 0.011073657927590512,
      "loss": 0.0297,
      "step": 2145
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.022582653909921646,
      "learning_rate": 0.011069496462754888,
      "loss": 0.4624,
      "step": 2146
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.011983267031610012,
      "learning_rate": 0.011065334997919268,
      "loss": 0.0371,
      "step": 2147
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.020280499011278152,
      "learning_rate": 0.011061173533083645,
      "loss": 0.2578,
      "step": 2148
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.018198104575276375,
      "learning_rate": 0.011057012068248025,
      "loss": 0.1066,
      "step": 2149
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.019287437200546265,
      "learning_rate": 0.011052850603412401,
      "loss": 0.127,
      "step": 2150
    },
    {
      "epoch": 2.69,
      "grad_norm": 0.03531277924776077,
      "learning_rate": 0.011048689138576779,
      "loss": 0.3853,
      "step": 2151
    },
    {
      "epoch": 2.69,
      "grad_norm": 0.01971743255853653,
      "learning_rate": 0.011044527673741159,
      "loss": 0.1759,
      "step": 2152
    },
    {
      "epoch": 2.69,
      "grad_norm": 0.025513382628560066,
      "learning_rate": 0.011040366208905535,
      "loss": 0.2255,
      "step": 2153
    },
    {
      "epoch": 2.69,
      "grad_norm": 0.02455832064151764,
      "learning_rate": 0.011036204744069912,
      "loss": 0.0142,
      "step": 2154
    },
    {
      "epoch": 2.69,
      "grad_norm": 0.01571063883602619,
      "learning_rate": 0.011032043279234292,
      "loss": 0.1727,
      "step": 2155
    },
    {
      "epoch": 2.69,
      "grad_norm": 0.023694394156336784,
      "learning_rate": 0.011027881814398668,
      "loss": 0.3298,
      "step": 2156
    },
    {
      "epoch": 2.69,
      "grad_norm": 0.02189617045223713,
      "learning_rate": 0.011023720349563046,
      "loss": 0.29,
      "step": 2157
    },
    {
      "epoch": 2.69,
      "grad_norm": 0.032622043043375015,
      "learning_rate": 0.011019558884727425,
      "loss": 0.2323,
      "step": 2158
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.01879781484603882,
      "learning_rate": 0.011015397419891802,
      "loss": 0.1311,
      "step": 2159
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.018398238345980644,
      "learning_rate": 0.01101123595505618,
      "loss": 0.2401,
      "step": 2160
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.01993747614324093,
      "learning_rate": 0.011007074490220559,
      "loss": 0.1914,
      "step": 2161
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.02538641355931759,
      "learning_rate": 0.011002913025384935,
      "loss": 0.3928,
      "step": 2162
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.010440933518111706,
      "learning_rate": 0.010998751560549315,
      "loss": 0.0285,
      "step": 2163
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.02357182651758194,
      "learning_rate": 0.010994590095713692,
      "loss": 0.365,
      "step": 2164
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.02021673321723938,
      "learning_rate": 0.010990428630878069,
      "loss": 0.2458,
      "step": 2165
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.01623695343732834,
      "learning_rate": 0.010986267166042448,
      "loss": 0.1276,
      "step": 2166
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.023500993847846985,
      "learning_rate": 0.010982105701206826,
      "loss": 0.2443,
      "step": 2167
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.016294024884700775,
      "learning_rate": 0.010977944236371202,
      "loss": 0.1,
      "step": 2168
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.029227396473288536,
      "learning_rate": 0.010973782771535582,
      "loss": 0.354,
      "step": 2169
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.016016041859984398,
      "learning_rate": 0.01096962130669996,
      "loss": 0.2732,
      "step": 2170
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.015516580082476139,
      "learning_rate": 0.010965459841864336,
      "loss": 0.1385,
      "step": 2171
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.021445462480187416,
      "learning_rate": 0.010961298377028715,
      "loss": 0.2673,
      "step": 2172
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.017905928194522858,
      "learning_rate": 0.010957136912193093,
      "loss": 0.146,
      "step": 2173
    },
    {
      "epoch": 2.71,
      "grad_norm": 0.028039854019880295,
      "learning_rate": 0.010952975447357469,
      "loss": 0.2256,
      "step": 2174
    },
    {
      "epoch": 2.72,
      "grad_norm": 0.02469632215797901,
      "learning_rate": 0.010948813982521849,
      "loss": 0.2849,
      "step": 2175
    },
    {
      "epoch": 2.72,
      "grad_norm": 0.0266665518283844,
      "learning_rate": 0.010944652517686226,
      "loss": 0.3508,
      "step": 2176
    },
    {
      "epoch": 2.72,
      "grad_norm": 0.027766795828938484,
      "learning_rate": 0.010940491052850602,
      "loss": 0.2781,
      "step": 2177
    },
    {
      "epoch": 2.72,
      "grad_norm": 0.011763075366616249,
      "learning_rate": 0.010936329588014982,
      "loss": 0.1501,
      "step": 2178
    },
    {
      "epoch": 2.72,
      "grad_norm": 0.026477541774511337,
      "learning_rate": 0.01093216812317936,
      "loss": 0.3032,
      "step": 2179
    },
    {
      "epoch": 2.72,
      "grad_norm": 0.010777819901704788,
      "learning_rate": 0.010928006658343736,
      "loss": 0.0635,
      "step": 2180
    },
    {
      "epoch": 2.72,
      "grad_norm": 0.026484979316592216,
      "learning_rate": 0.010923845193508116,
      "loss": 0.2883,
      "step": 2181
    },
    {
      "epoch": 2.72,
      "grad_norm": 0.02571002021431923,
      "learning_rate": 0.010919683728672493,
      "loss": 0.522,
      "step": 2182
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.021253768354654312,
      "learning_rate": 0.01091552226383687,
      "loss": 0.1393,
      "step": 2183
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.015134699642658234,
      "learning_rate": 0.010911360799001249,
      "loss": 0.1748,
      "step": 2184
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.022003090009093285,
      "learning_rate": 0.010907199334165627,
      "loss": 0.2566,
      "step": 2185
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.022759342566132545,
      "learning_rate": 0.010903037869330003,
      "loss": 0.1935,
      "step": 2186
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.02189919538795948,
      "learning_rate": 0.010898876404494382,
      "loss": 0.243,
      "step": 2187
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.02361384779214859,
      "learning_rate": 0.01089471493965876,
      "loss": 0.1702,
      "step": 2188
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.0005996807594783604,
      "learning_rate": 0.010890553474823136,
      "loss": 0.0005,
      "step": 2189
    },
    {
      "epoch": 2.73,
      "grad_norm": 0.023513397201895714,
      "learning_rate": 0.010886392009987516,
      "loss": 0.1738,
      "step": 2190
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.016914114356040955,
      "learning_rate": 0.010882230545151894,
      "loss": 0.1503,
      "step": 2191
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.015564977191388607,
      "learning_rate": 0.01087806908031627,
      "loss": 0.1285,
      "step": 2192
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.018034396693110466,
      "learning_rate": 0.01087390761548065,
      "loss": 0.1532,
      "step": 2193
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.018877578899264336,
      "learning_rate": 0.010869746150645027,
      "loss": 0.1886,
      "step": 2194
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.033581849187612534,
      "learning_rate": 0.010865584685809405,
      "loss": 0.335,
      "step": 2195
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.002317525912076235,
      "learning_rate": 0.010861423220973783,
      "loss": 0.0019,
      "step": 2196
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.015391032211482525,
      "learning_rate": 0.01085726175613816,
      "loss": 0.067,
      "step": 2197
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.020499935373663902,
      "learning_rate": 0.010853100291302539,
      "loss": 0.1166,
      "step": 2198
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.02005499228835106,
      "learning_rate": 0.010848938826466916,
      "loss": 0.2927,
      "step": 2199
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.03162027522921562,
      "learning_rate": 0.010844777361631294,
      "loss": 0.1553,
      "step": 2200
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.01973137818276882,
      "learning_rate": 0.010840615896795672,
      "loss": 0.1219,
      "step": 2201
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.027604639530181885,
      "learning_rate": 0.01083645443196005,
      "loss": 0.2451,
      "step": 2202
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.026138022541999817,
      "learning_rate": 0.010832292967124428,
      "loss": 0.1582,
      "step": 2203
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.022931070998311043,
      "learning_rate": 0.010828131502288806,
      "loss": 0.2458,
      "step": 2204
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.02712683193385601,
      "learning_rate": 0.010823970037453183,
      "loss": 0.3223,
      "step": 2205
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.03746069222688675,
      "learning_rate": 0.010819808572617563,
      "loss": 0.5239,
      "step": 2206
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.019446194171905518,
      "learning_rate": 0.010815647107781939,
      "loss": 0.2778,
      "step": 2207
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.022288456559181213,
      "learning_rate": 0.010811485642946317,
      "loss": 0.1039,
      "step": 2208
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.01389577891677618,
      "learning_rate": 0.010807324178110696,
      "loss": 0.0685,
      "step": 2209
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.029764549806714058,
      "learning_rate": 0.010803162713275073,
      "loss": 0.3748,
      "step": 2210
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.0003120447800029069,
      "learning_rate": 0.01079900124843945,
      "loss": 0.0004,
      "step": 2211
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.025778671726584435,
      "learning_rate": 0.01079483978360383,
      "loss": 0.3701,
      "step": 2212
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.01931004598736763,
      "learning_rate": 0.010790678318768206,
      "loss": 0.1152,
      "step": 2213
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.02399304322898388,
      "learning_rate": 0.010786516853932584,
      "loss": 0.1608,
      "step": 2214
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.022080132737755775,
      "learning_rate": 0.010782355389096963,
      "loss": 0.1589,
      "step": 2215
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.03168994188308716,
      "learning_rate": 0.01077819392426134,
      "loss": 0.3206,
      "step": 2216
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.02363654598593712,
      "learning_rate": 0.010774032459425717,
      "loss": 0.2177,
      "step": 2217
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.0010802140459418297,
      "learning_rate": 0.010769870994590097,
      "loss": 0.0009,
      "step": 2218
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.020285725593566895,
      "learning_rate": 0.010765709529754473,
      "loss": 0.3562,
      "step": 2219
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.01771828718483448,
      "learning_rate": 0.01076154806491885,
      "loss": 0.1245,
      "step": 2220
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.022161029279232025,
      "learning_rate": 0.01075738660008323,
      "loss": 0.3254,
      "step": 2221
    },
    {
      "epoch": 2.77,
      "grad_norm": 0.020123887807130814,
      "learning_rate": 0.010753225135247608,
      "loss": 0.2112,
      "step": 2222
    },
    {
      "epoch": 2.78,
      "grad_norm": 0.02168109267950058,
      "learning_rate": 0.010749063670411984,
      "loss": 0.2177,
      "step": 2223
    },
    {
      "epoch": 2.78,
      "grad_norm": 0.02669345960021019,
      "learning_rate": 0.010744902205576364,
      "loss": 0.1566,
      "step": 2224
    },
    {
      "epoch": 2.78,
      "grad_norm": 0.010966538451611996,
      "learning_rate": 0.010740740740740742,
      "loss": 0.051,
      "step": 2225
    },
    {
      "epoch": 2.78,
      "grad_norm": 0.021277008578181267,
      "learning_rate": 0.010736579275905118,
      "loss": 0.1941,
      "step": 2226
    },
    {
      "epoch": 2.78,
      "grad_norm": 0.01108519360423088,
      "learning_rate": 0.010732417811069497,
      "loss": 0.0303,
      "step": 2227
    },
    {
      "epoch": 2.78,
      "grad_norm": 0.025729136541485786,
      "learning_rate": 0.010728256346233875,
      "loss": 0.2039,
      "step": 2228
    },
    {
      "epoch": 2.78,
      "grad_norm": 0.02782784029841423,
      "learning_rate": 0.010724094881398251,
      "loss": 0.2812,
      "step": 2229
    },
    {
      "epoch": 2.78,
      "grad_norm": 0.019157204777002335,
      "learning_rate": 0.01071993341656263,
      "loss": 0.1786,
      "step": 2230
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.01695295050740242,
      "learning_rate": 0.010715771951727009,
      "loss": 0.1864,
      "step": 2231
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.01897367089986801,
      "learning_rate": 0.010711610486891385,
      "loss": 0.1006,
      "step": 2232
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.022710496559739113,
      "learning_rate": 0.010707449022055764,
      "loss": 0.2314,
      "step": 2233
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.018418705090880394,
      "learning_rate": 0.010703287557220142,
      "loss": 0.1849,
      "step": 2234
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.01582447439432144,
      "learning_rate": 0.010699126092384518,
      "loss": 0.0818,
      "step": 2235
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.02084263041615486,
      "learning_rate": 0.010694964627548898,
      "loss": 0.1046,
      "step": 2236
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.023801758885383606,
      "learning_rate": 0.010690803162713276,
      "loss": 0.168,
      "step": 2237
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.01704120822250843,
      "learning_rate": 0.010686641697877653,
      "loss": 0.0861,
      "step": 2238
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.012270728126168251,
      "learning_rate": 0.010682480233042031,
      "loss": 0.2076,
      "step": 2239
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.006402794737368822,
      "learning_rate": 0.010678318768206409,
      "loss": 0.0116,
      "step": 2240
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.015556024387478828,
      "learning_rate": 0.010674157303370787,
      "loss": 0.0942,
      "step": 2241
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.024194354191422462,
      "learning_rate": 0.010669995838535165,
      "loss": 0.251,
      "step": 2242
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.020713014528155327,
      "learning_rate": 0.010665834373699543,
      "loss": 0.2161,
      "step": 2243
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.012702727690339088,
      "learning_rate": 0.01066167290886392,
      "loss": 0.0701,
      "step": 2244
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.030650902539491653,
      "learning_rate": 0.010657511444028298,
      "loss": 0.3833,
      "step": 2245
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.024958474561572075,
      "learning_rate": 0.010653349979192676,
      "loss": 0.2517,
      "step": 2246
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.02753041870892048,
      "learning_rate": 0.010649188514357054,
      "loss": 0.4014,
      "step": 2247
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.021966665983200073,
      "learning_rate": 0.010645027049521432,
      "loss": 0.2151,
      "step": 2248
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.01953025534749031,
      "learning_rate": 0.010640865584685811,
      "loss": 0.1263,
      "step": 2249
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.01903337426483631,
      "learning_rate": 0.010636704119850187,
      "loss": 0.0976,
      "step": 2250
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.021982165053486824,
      "learning_rate": 0.010632542655014565,
      "loss": 0.2435,
      "step": 2251
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.02722259610891342,
      "learning_rate": 0.010628381190178945,
      "loss": 0.2223,
      "step": 2252
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.021399516612291336,
      "learning_rate": 0.01062421972534332,
      "loss": 0.171,
      "step": 2253
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.019407158717513084,
      "learning_rate": 0.010620058260507699,
      "loss": 0.1909,
      "step": 2254
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.018010206520557404,
      "learning_rate": 0.010615896795672078,
      "loss": 0.2856,
      "step": 2255
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.023604342713952065,
      "learning_rate": 0.010611735330836454,
      "loss": 0.2228,
      "step": 2256
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.018620522692799568,
      "learning_rate": 0.010607573866000832,
      "loss": 0.1251,
      "step": 2257
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.014440278522670269,
      "learning_rate": 0.010603412401165212,
      "loss": 0.0786,
      "step": 2258
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.01210779882967472,
      "learning_rate": 0.010599250936329588,
      "loss": 0.1327,
      "step": 2259
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.00037333733052946627,
      "learning_rate": 0.010595089471493966,
      "loss": 0.0005,
      "step": 2260
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.027539050206542015,
      "learning_rate": 0.010590928006658345,
      "loss": 0.3191,
      "step": 2261
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.024443460628390312,
      "learning_rate": 0.010586766541822721,
      "loss": 0.2451,
      "step": 2262
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.0279726330190897,
      "learning_rate": 0.010582605076987099,
      "loss": 0.1899,
      "step": 2263
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.0002350588038098067,
      "learning_rate": 0.010578443612151479,
      "loss": 0.0002,
      "step": 2264
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.017479298636317253,
      "learning_rate": 0.010574282147315855,
      "loss": 0.2239,
      "step": 2265
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.020448802039027214,
      "learning_rate": 0.010570120682480233,
      "loss": 0.2179,
      "step": 2266
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.027602100744843483,
      "learning_rate": 0.010565959217644612,
      "loss": 0.2438,
      "step": 2267
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.021186701953411102,
      "learning_rate": 0.010561797752808988,
      "loss": 0.3142,
      "step": 2268
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.013428137637674809,
      "learning_rate": 0.010557636287973366,
      "loss": 0.0911,
      "step": 2269
    },
    {
      "epoch": 2.83,
      "grad_norm": 0.019601330161094666,
      "learning_rate": 0.010553474823137746,
      "loss": 0.3806,
      "step": 2270
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.01644669659435749,
      "learning_rate": 0.010549313358302122,
      "loss": 0.1444,
      "step": 2271
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.01744096353650093,
      "learning_rate": 0.0105451518934665,
      "loss": 0.1216,
      "step": 2272
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.017869403585791588,
      "learning_rate": 0.01054099042863088,
      "loss": 0.1904,
      "step": 2273
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.02864708937704563,
      "learning_rate": 0.010536828963795255,
      "loss": 0.2217,
      "step": 2274
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.005211611744016409,
      "learning_rate": 0.010532667498959633,
      "loss": 0.0164,
      "step": 2275
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.01783333718776703,
      "learning_rate": 0.010528506034124013,
      "loss": 0.1411,
      "step": 2276
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.030197225511074066,
      "learning_rate": 0.010524344569288389,
      "loss": 0.3616,
      "step": 2277
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.026270641013979912,
      "learning_rate": 0.010520183104452767,
      "loss": 0.1831,
      "step": 2278
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.021814053878188133,
      "learning_rate": 0.010516021639617146,
      "loss": 0.2913,
      "step": 2279
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.01968635618686676,
      "learning_rate": 0.010511860174781522,
      "loss": 0.0667,
      "step": 2280
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.011001880280673504,
      "learning_rate": 0.010507698709945902,
      "loss": 0.0436,
      "step": 2281
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.020367112010717392,
      "learning_rate": 0.01050353724511028,
      "loss": 0.1599,
      "step": 2282
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.016826961189508438,
      "learning_rate": 0.010499375780274656,
      "loss": 0.2001,
      "step": 2283
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.018745791167020798,
      "learning_rate": 0.010495214315439035,
      "loss": 0.3452,
      "step": 2284
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.013791165314614773,
      "learning_rate": 0.010491052850603413,
      "loss": 0.1639,
      "step": 2285
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.034656181931495667,
      "learning_rate": 0.01048689138576779,
      "loss": 0.3208,
      "step": 2286
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.010394170880317688,
      "learning_rate": 0.010482729920932169,
      "loss": 0.0422,
      "step": 2287
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.03031228668987751,
      "learning_rate": 0.010478568456096547,
      "loss": 0.1479,
      "step": 2288
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.030285999178886414,
      "learning_rate": 0.010474406991260923,
      "loss": 0.2344,
      "step": 2289
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.025744587182998657,
      "learning_rate": 0.010470245526425302,
      "loss": 0.1692,
      "step": 2290
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.01601749286055565,
      "learning_rate": 0.01046608406158968,
      "loss": 0.1232,
      "step": 2291
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.015805397182703018,
      "learning_rate": 0.010461922596754056,
      "loss": 0.1497,
      "step": 2292
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.013101669028401375,
      "learning_rate": 0.010457761131918436,
      "loss": 0.0465,
      "step": 2293
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.015499395318329334,
      "learning_rate": 0.010453599667082814,
      "loss": 0.1918,
      "step": 2294
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.023468533530831337,
      "learning_rate": 0.010449438202247193,
      "loss": 0.2847,
      "step": 2295
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.030620621517300606,
      "learning_rate": 0.01044527673741157,
      "loss": 0.3303,
      "step": 2296
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.024031227454543114,
      "learning_rate": 0.010441115272575947,
      "loss": 0.1782,
      "step": 2297
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.01350974291563034,
      "learning_rate": 0.010436953807740327,
      "loss": 0.137,
      "step": 2298
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.018704349175095558,
      "learning_rate": 0.010432792342904703,
      "loss": 0.3059,
      "step": 2299
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.009061033837497234,
      "learning_rate": 0.01042863087806908,
      "loss": 0.0351,
      "step": 2300
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.025503430515527725,
      "learning_rate": 0.01042446941323346,
      "loss": 0.2856,
      "step": 2301
    },
    {
      "epoch": 2.87,
      "grad_norm": 0.021449217572808266,
      "learning_rate": 0.010420307948397836,
      "loss": 0.3613,
      "step": 2302
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.01854928582906723,
      "learning_rate": 0.010416146483562214,
      "loss": 0.1746,
      "step": 2303
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.02633313089609146,
      "learning_rate": 0.010411985018726594,
      "loss": 0.3633,
      "step": 2304
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.03778459131717682,
      "learning_rate": 0.01040782355389097,
      "loss": 0.2754,
      "step": 2305
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.021953236311674118,
      "learning_rate": 0.010403662089055347,
      "loss": 0.2544,
      "step": 2306
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.022089865058660507,
      "learning_rate": 0.010399500624219727,
      "loss": 0.0776,
      "step": 2307
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.026373153552412987,
      "learning_rate": 0.010395339159384103,
      "loss": 0.3845,
      "step": 2308
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.018084948882460594,
      "learning_rate": 0.010391177694548481,
      "loss": 0.0474,
      "step": 2309
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.014509757980704308,
      "learning_rate": 0.01038701622971286,
      "loss": 0.0412,
      "step": 2310
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.02012368105351925,
      "learning_rate": 0.010382854764877237,
      "loss": 0.166,
      "step": 2311
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.021444309502840042,
      "learning_rate": 0.010378693300041614,
      "loss": 0.1831,
      "step": 2312
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.026529138907790184,
      "learning_rate": 0.010374531835205994,
      "loss": 0.1207,
      "step": 2313
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.019221439957618713,
      "learning_rate": 0.01037037037037037,
      "loss": 0.2463,
      "step": 2314
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.020255398005247116,
      "learning_rate": 0.010366208905534748,
      "loss": 0.1376,
      "step": 2315
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.028401967138051987,
      "learning_rate": 0.010362047440699127,
      "loss": 0.3435,
      "step": 2316
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.006789743900299072,
      "learning_rate": 0.010357885975863504,
      "loss": 0.0162,
      "step": 2317
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.021116109564900398,
      "learning_rate": 0.010353724511027881,
      "loss": 0.1869,
      "step": 2318
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.01359983254224062,
      "learning_rate": 0.010349563046192261,
      "loss": 0.071,
      "step": 2319
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.030931934714317322,
      "learning_rate": 0.010345401581356637,
      "loss": 0.4568,
      "step": 2320
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.013981071300804615,
      "learning_rate": 0.010341240116521015,
      "loss": 0.0703,
      "step": 2321
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.0009513223194517195,
      "learning_rate": 0.010337078651685394,
      "loss": 0.0006,
      "step": 2322
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.034739717841148376,
      "learning_rate": 0.01033291718684977,
      "loss": 0.3013,
      "step": 2323
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.01871790923178196,
      "learning_rate": 0.01032875572201415,
      "loss": 0.1074,
      "step": 2324
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.02246055379509926,
      "learning_rate": 0.010324594257178528,
      "loss": 0.1519,
      "step": 2325
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.02692565694451332,
      "learning_rate": 0.010320432792342904,
      "loss": 0.2178,
      "step": 2326
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.02375246025621891,
      "learning_rate": 0.010316271327507284,
      "loss": 0.3494,
      "step": 2327
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.026311181485652924,
      "learning_rate": 0.010312109862671661,
      "loss": 0.3032,
      "step": 2328
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.049828723073005676,
      "learning_rate": 0.010307948397836037,
      "loss": 0.5801,
      "step": 2329
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.017019830644130707,
      "learning_rate": 0.010303786933000417,
      "loss": 0.0891,
      "step": 2330
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.019325396046042442,
      "learning_rate": 0.010299625468164795,
      "loss": 0.3083,
      "step": 2331
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.03160206228494644,
      "learning_rate": 0.010295464003329171,
      "loss": 0.1796,
      "step": 2332
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.017973942682147026,
      "learning_rate": 0.01029130253849355,
      "loss": 0.1378,
      "step": 2333
    },
    {
      "epoch": 2.91,
      "grad_norm": 0.02240385115146637,
      "learning_rate": 0.010287141073657928,
      "loss": 0.2698,
      "step": 2334
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.02270783856511116,
      "learning_rate": 0.010282979608822304,
      "loss": 0.1609,
      "step": 2335
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.025008946657180786,
      "learning_rate": 0.010278818143986684,
      "loss": 0.303,
      "step": 2336
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.027263332158327103,
      "learning_rate": 0.010274656679151062,
      "loss": 0.386,
      "step": 2337
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.02273942157626152,
      "learning_rate": 0.010270495214315438,
      "loss": 0.2478,
      "step": 2338
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.02560199238359928,
      "learning_rate": 0.010266333749479817,
      "loss": 0.1378,
      "step": 2339
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.02013193815946579,
      "learning_rate": 0.010262172284644195,
      "loss": 0.3206,
      "step": 2340
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.012618872337043285,
      "learning_rate": 0.010258010819808571,
      "loss": 0.0716,
      "step": 2341
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.017450779676437378,
      "learning_rate": 0.010253849354972951,
      "loss": 0.1377,
      "step": 2342
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.024493584409356117,
      "learning_rate": 0.010249687890137329,
      "loss": 0.3618,
      "step": 2343
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.027467286214232445,
      "learning_rate": 0.010245526425301705,
      "loss": 0.161,
      "step": 2344
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.014886799268424511,
      "learning_rate": 0.010241364960466084,
      "loss": 0.0671,
      "step": 2345
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.015789305791258812,
      "learning_rate": 0.010237203495630462,
      "loss": 0.0986,
      "step": 2346
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.03900730237364769,
      "learning_rate": 0.010233042030794838,
      "loss": 0.3674,
      "step": 2347
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.022387534379959106,
      "learning_rate": 0.010228880565959218,
      "loss": 0.1754,
      "step": 2348
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.02683332748711109,
      "learning_rate": 0.010224719101123596,
      "loss": 0.1221,
      "step": 2349
    },
    {
      "epoch": 2.93,
      "grad_norm": 0.020926162600517273,
      "learning_rate": 0.010220557636287972,
      "loss": 0.2036,
      "step": 2350
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.03465447947382927,
      "learning_rate": 0.010216396171452351,
      "loss": 0.8013,
      "step": 2351
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.002997375326231122,
      "learning_rate": 0.01021223470661673,
      "loss": 0.0021,
      "step": 2352
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.03399604186415672,
      "learning_rate": 0.010208073241781105,
      "loss": 0.5425,
      "step": 2353
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.027589542791247368,
      "learning_rate": 0.010203911776945485,
      "loss": 0.4834,
      "step": 2354
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.024951687082648277,
      "learning_rate": 0.010199750312109863,
      "loss": 0.4287,
      "step": 2355
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.013411588966846466,
      "learning_rate": 0.01019558884727424,
      "loss": 0.0983,
      "step": 2356
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.016656961292028427,
      "learning_rate": 0.010191427382438618,
      "loss": 0.1233,
      "step": 2357
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.026162033900618553,
      "learning_rate": 0.010187265917602996,
      "loss": 0.3098,
      "step": 2358
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.015501325018703938,
      "learning_rate": 0.010183104452767374,
      "loss": 0.1719,
      "step": 2359
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.0168448518961668,
      "learning_rate": 0.010178942987931752,
      "loss": 0.1818,
      "step": 2360
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.028916187584400177,
      "learning_rate": 0.01017478152309613,
      "loss": 0.3894,
      "step": 2361
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.022329702973365784,
      "learning_rate": 0.010170620058260508,
      "loss": 0.1176,
      "step": 2362
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.012370950542390347,
      "learning_rate": 0.010166458593424885,
      "loss": 0.0428,
      "step": 2363
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.018777061253786087,
      "learning_rate": 0.010162297128589263,
      "loss": 0.134,
      "step": 2364
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.017969677224755287,
      "learning_rate": 0.010158135663753641,
      "loss": 0.2368,
      "step": 2365
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.019264405593276024,
      "learning_rate": 0.010153974198918019,
      "loss": 0.1753,
      "step": 2366
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.03495575860142708,
      "learning_rate": 0.010149812734082398,
      "loss": 0.3713,
      "step": 2367
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.0276605486869812,
      "learning_rate": 0.010145651269246776,
      "loss": 0.1427,
      "step": 2368
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.016330432146787643,
      "learning_rate": 0.010141489804411152,
      "loss": 0.1943,
      "step": 2369
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.013641185127198696,
      "learning_rate": 0.010137328339575532,
      "loss": 0.0862,
      "step": 2370
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.007164125330746174,
      "learning_rate": 0.01013316687473991,
      "loss": 0.0193,
      "step": 2371
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.027180640026926994,
      "learning_rate": 0.010129005409904286,
      "loss": 0.2944,
      "step": 2372
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.021393343806266785,
      "learning_rate": 0.010124843945068665,
      "loss": 0.234,
      "step": 2373
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.018753133714199066,
      "learning_rate": 0.010120682480233043,
      "loss": 0.1266,
      "step": 2374
    },
    {
      "epoch": 2.97,
      "grad_norm": 0.022865546867251396,
      "learning_rate": 0.01011652101539742,
      "loss": 0.3582,
      "step": 2375
    },
    {
      "epoch": 2.97,
      "grad_norm": 0.02386411465704441,
      "learning_rate": 0.010112359550561799,
      "loss": 0.2695,
      "step": 2376
    },
    {
      "epoch": 2.97,
      "grad_norm": 0.014110801741480827,
      "learning_rate": 0.010108198085726177,
      "loss": 0.1401,
      "step": 2377
    },
    {
      "epoch": 2.97,
      "grad_norm": 0.01591227576136589,
      "learning_rate": 0.010104036620890553,
      "loss": 0.1718,
      "step": 2378
    },
    {
      "epoch": 2.97,
      "grad_norm": 0.006550228223204613,
      "learning_rate": 0.010099875156054932,
      "loss": 0.0213,
      "step": 2379
    },
    {
      "epoch": 2.97,
      "grad_norm": 0.027895042672753334,
      "learning_rate": 0.01009571369121931,
      "loss": 0.3064,
      "step": 2380
    },
    {
      "epoch": 2.97,
      "grad_norm": 0.021065207198262215,
      "learning_rate": 0.010091552226383686,
      "loss": 0.2125,
      "step": 2381
    },
    {
      "epoch": 2.97,
      "grad_norm": 0.028758682310581207,
      "learning_rate": 0.010087390761548066,
      "loss": 0.2473,
      "step": 2382
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.019170215353369713,
      "learning_rate": 0.010083229296712444,
      "loss": 0.2292,
      "step": 2383
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.02462277188897133,
      "learning_rate": 0.01007906783187682,
      "loss": 0.1562,
      "step": 2384
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.03516071289777756,
      "learning_rate": 0.0100749063670412,
      "loss": 0.2761,
      "step": 2385
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.025386476889252663,
      "learning_rate": 0.010070744902205577,
      "loss": 0.2234,
      "step": 2386
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.029696503654122353,
      "learning_rate": 0.010066583437369953,
      "loss": 0.1072,
      "step": 2387
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.018433472141623497,
      "learning_rate": 0.010062421972534333,
      "loss": 0.1384,
      "step": 2388
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.021474363282322884,
      "learning_rate": 0.01005826050769871,
      "loss": 0.1725,
      "step": 2389
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.014015222899615765,
      "learning_rate": 0.010054099042863087,
      "loss": 0.1127,
      "step": 2390
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.02778368629515171,
      "learning_rate": 0.010049937578027466,
      "loss": 0.3667,
      "step": 2391
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.014408957213163376,
      "learning_rate": 0.010045776113191844,
      "loss": 0.0977,
      "step": 2392
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.013749256730079651,
      "learning_rate": 0.01004161464835622,
      "loss": 0.088,
      "step": 2393
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.0009925315389409661,
      "learning_rate": 0.0100374531835206,
      "loss": 0.0008,
      "step": 2394
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.015545278787612915,
      "learning_rate": 0.010033291718684978,
      "loss": 0.1859,
      "step": 2395
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.02967129461467266,
      "learning_rate": 0.010029130253849354,
      "loss": 0.4146,
      "step": 2396
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.01217916700989008,
      "learning_rate": 0.010024968789013733,
      "loss": 0.0683,
      "step": 2397
    },
    {
      "epoch": 2.99,
      "grad_norm": 0.019123321399092674,
      "learning_rate": 0.010020807324178111,
      "loss": 0.0633,
      "step": 2398
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.02751222439110279,
      "learning_rate": 0.010016645859342489,
      "loss": 0.344,
      "step": 2399
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.02347714640200138,
      "learning_rate": 0.010012484394506867,
      "loss": 0.5186,
      "step": 2400
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.241943359375,
      "eval_runtime": 182.8796,
      "eval_samples_per_second": 1.099,
      "eval_steps_per_second": 0.552,
      "step": 2400
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.04474431276321411,
      "learning_rate": 0.010008322929671245,
      "loss": 0.2092,
      "step": 2401
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.020384645089507103,
      "learning_rate": 0.010004161464835622,
      "loss": 0.1107,
      "step": 2402
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.021464774385094643,
      "learning_rate": 0.01,
      "loss": 0.1914,
      "step": 2403
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.0167263001203537,
      "learning_rate": 0.009995838535164378,
      "loss": 0.1849,
      "step": 2404
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.01900867372751236,
      "learning_rate": 0.009991677070328756,
      "loss": 0.1415,
      "step": 2405
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.022394275292754173,
      "learning_rate": 0.009987515605493134,
      "loss": 0.1406,
      "step": 2406
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.022576816380023956,
      "learning_rate": 0.009983354140657512,
      "loss": 0.1984,
      "step": 2407
    },
    {
      "epoch": 3.01,
      "grad_norm": 0.026300957426428795,
      "learning_rate": 0.00997919267582189,
      "loss": 0.2274,
      "step": 2408
    },
    {
      "epoch": 3.01,
      "grad_norm": 0.021521924063563347,
      "learning_rate": 0.009975031210986267,
      "loss": 0.283,
      "step": 2409
    },
    {
      "epoch": 3.01,
      "grad_norm": 0.013268372975289822,
      "learning_rate": 0.009970869746150645,
      "loss": 0.1622,
      "step": 2410
    },
    {
      "epoch": 3.01,
      "grad_norm": 0.019623691216111183,
      "learning_rate": 0.009966708281315023,
      "loss": 0.1287,
      "step": 2411
    },
    {
      "epoch": 3.01,
      "grad_norm": 0.02197312004864216,
      "learning_rate": 0.0099625468164794,
      "loss": 0.155,
      "step": 2412
    },
    {
      "epoch": 3.01,
      "grad_norm": 0.02055235765874386,
      "learning_rate": 0.009958385351643778,
      "loss": 0.1716,
      "step": 2413
    },
    {
      "epoch": 3.01,
      "grad_norm": 0.0299990214407444,
      "learning_rate": 0.009954223886808156,
      "loss": 0.2698,
      "step": 2414
    },
    {
      "epoch": 3.01,
      "grad_norm": 0.01623864658176899,
      "learning_rate": 0.009950062421972534,
      "loss": 0.29,
      "step": 2415
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.01911856420338154,
      "learning_rate": 0.009945900957136912,
      "loss": 0.2742,
      "step": 2416
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.020260749384760857,
      "learning_rate": 0.00994173949230129,
      "loss": 0.1122,
      "step": 2417
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.024163879454135895,
      "learning_rate": 0.009937578027465668,
      "loss": 0.1381,
      "step": 2418
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.02132105641067028,
      "learning_rate": 0.009933416562630045,
      "loss": 0.2408,
      "step": 2419
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.0232879389077425,
      "learning_rate": 0.009929255097794423,
      "loss": 0.1588,
      "step": 2420
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.020577607676386833,
      "learning_rate": 0.009925093632958801,
      "loss": 0.2245,
      "step": 2421
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.026655666530132294,
      "learning_rate": 0.00992093216812318,
      "loss": 0.2208,
      "step": 2422
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.016230953857302666,
      "learning_rate": 0.009916770703287557,
      "loss": 0.1177,
      "step": 2423
    },
    {
      "epoch": 3.03,
      "grad_norm": 0.02187819965183735,
      "learning_rate": 0.009912609238451935,
      "loss": 0.1848,
      "step": 2424
    },
    {
      "epoch": 3.03,
      "grad_norm": 0.02349940873682499,
      "learning_rate": 0.009908447773616314,
      "loss": 0.1589,
      "step": 2425
    },
    {
      "epoch": 3.03,
      "grad_norm": 0.027948599308729172,
      "learning_rate": 0.009904286308780692,
      "loss": 0.2844,
      "step": 2426
    },
    {
      "epoch": 3.03,
      "grad_norm": 0.031437382102012634,
      "learning_rate": 0.009900124843945068,
      "loss": 0.3066,
      "step": 2427
    },
    {
      "epoch": 3.03,
      "grad_norm": 0.02158045582473278,
      "learning_rate": 0.009895963379109448,
      "loss": 0.0782,
      "step": 2428
    },
    {
      "epoch": 3.03,
      "grad_norm": 0.028191545978188515,
      "learning_rate": 0.009891801914273825,
      "loss": 0.129,
      "step": 2429
    },
    {
      "epoch": 3.03,
      "grad_norm": 0.027839263901114464,
      "learning_rate": 0.009887640449438202,
      "loss": 0.1644,
      "step": 2430
    },
    {
      "epoch": 3.03,
      "grad_norm": 0.019427863880991936,
      "learning_rate": 0.009883478984602581,
      "loss": 0.1691,
      "step": 2431
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.012019229121506214,
      "learning_rate": 0.009879317519766959,
      "loss": 0.024,
      "step": 2432
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.02034587785601616,
      "learning_rate": 0.009875156054931335,
      "loss": 0.1655,
      "step": 2433
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.021081967279314995,
      "learning_rate": 0.009870994590095715,
      "loss": 0.2063,
      "step": 2434
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.007239439059048891,
      "learning_rate": 0.009866833125260092,
      "loss": 0.0184,
      "step": 2435
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.021766290068626404,
      "learning_rate": 0.009862671660424469,
      "loss": 0.1354,
      "step": 2436
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.0159266609698534,
      "learning_rate": 0.009858510195588848,
      "loss": 0.0924,
      "step": 2437
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.02181187830865383,
      "learning_rate": 0.009854348730753226,
      "loss": 0.1943,
      "step": 2438
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.0220184288918972,
      "learning_rate": 0.009850187265917602,
      "loss": 0.1852,
      "step": 2439
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.035126619040966034,
      "learning_rate": 0.009846025801081982,
      "loss": 0.4834,
      "step": 2440
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.020149480551481247,
      "learning_rate": 0.00984186433624636,
      "loss": 0.2466,
      "step": 2441
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.0211184024810791,
      "learning_rate": 0.009837702871410737,
      "loss": 0.0446,
      "step": 2442
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.007436677347868681,
      "learning_rate": 0.009833541406575115,
      "loss": 0.0249,
      "step": 2443
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.01836736500263214,
      "learning_rate": 0.009829379941739493,
      "loss": 0.2119,
      "step": 2444
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.012206964194774628,
      "learning_rate": 0.00982521847690387,
      "loss": 0.0828,
      "step": 2445
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.02695884369313717,
      "learning_rate": 0.009821057012068249,
      "loss": 0.243,
      "step": 2446
    },
    {
      "epoch": 3.05,
      "grad_norm": 0.018760420382022858,
      "learning_rate": 0.009816895547232626,
      "loss": 0.198,
      "step": 2447
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.023253431543707848,
      "learning_rate": 0.009812734082397004,
      "loss": 0.353,
      "step": 2448
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.008284741081297398,
      "learning_rate": 0.009808572617561382,
      "loss": 0.0204,
      "step": 2449
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.025452299043536186,
      "learning_rate": 0.00980441115272576,
      "loss": 0.2217,
      "step": 2450
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.01937575824558735,
      "learning_rate": 0.009800249687890138,
      "loss": 0.2269,
      "step": 2451
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.0473109632730484,
      "learning_rate": 0.009796088223054516,
      "loss": 0.7061,
      "step": 2452
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.02246828004717827,
      "learning_rate": 0.009791926758218893,
      "loss": 0.2323,
      "step": 2453
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.030506981536746025,
      "learning_rate": 0.009787765293383271,
      "loss": 0.4177,
      "step": 2454
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.023256130516529083,
      "learning_rate": 0.009783603828547649,
      "loss": 0.1234,
      "step": 2455
    },
    {
      "epoch": 3.07,
      "grad_norm": 0.022441059350967407,
      "learning_rate": 0.009779442363712027,
      "loss": 0.1342,
      "step": 2456
    },
    {
      "epoch": 3.07,
      "grad_norm": 0.019019201397895813,
      "learning_rate": 0.009775280898876405,
      "loss": 0.186,
      "step": 2457
    },
    {
      "epoch": 3.07,
      "grad_norm": 0.008543728850781918,
      "learning_rate": 0.009771119434040782,
      "loss": 0.0393,
      "step": 2458
    },
    {
      "epoch": 3.07,
      "grad_norm": 0.0175627451390028,
      "learning_rate": 0.00976695796920516,
      "loss": 0.1483,
      "step": 2459
    },
    {
      "epoch": 3.07,
      "grad_norm": 0.020421307533979416,
      "learning_rate": 0.009762796504369538,
      "loss": 0.1209,
      "step": 2460
    },
    {
      "epoch": 3.07,
      "grad_norm": 0.020994724705815315,
      "learning_rate": 0.009758635039533916,
      "loss": 0.2175,
      "step": 2461
    },
    {
      "epoch": 3.07,
      "grad_norm": 0.015381667762994766,
      "learning_rate": 0.009754473574698294,
      "loss": 0.058,
      "step": 2462
    },
    {
      "epoch": 3.07,
      "grad_norm": 0.021469181403517723,
      "learning_rate": 0.009750312109862672,
      "loss": 0.1759,
      "step": 2463
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.01595204696059227,
      "learning_rate": 0.00974615064502705,
      "loss": 0.0164,
      "step": 2464
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.00048734157462604344,
      "learning_rate": 0.009741989180191427,
      "loss": 0.0007,
      "step": 2465
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.021787885576486588,
      "learning_rate": 0.009737827715355805,
      "loss": 0.2625,
      "step": 2466
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.0006687971181236207,
      "learning_rate": 0.009733666250520183,
      "loss": 0.0008,
      "step": 2467
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.013538903556764126,
      "learning_rate": 0.00972950478568456,
      "loss": 0.0862,
      "step": 2468
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.01638140343129635,
      "learning_rate": 0.00972534332084894,
      "loss": 0.0764,
      "step": 2469
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.016000719740986824,
      "learning_rate": 0.009721181856013316,
      "loss": 0.1543,
      "step": 2470
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.01357727125287056,
      "learning_rate": 0.009717020391177694,
      "loss": 0.1027,
      "step": 2471
    },
    {
      "epoch": 3.09,
      "grad_norm": 0.023158587515354156,
      "learning_rate": 0.009712858926342074,
      "loss": 0.163,
      "step": 2472
    },
    {
      "epoch": 3.09,
      "grad_norm": 0.019619371742010117,
      "learning_rate": 0.00970869746150645,
      "loss": 0.186,
      "step": 2473
    },
    {
      "epoch": 3.09,
      "grad_norm": 0.015063321217894554,
      "learning_rate": 0.009704535996670828,
      "loss": 0.0676,
      "step": 2474
    },
    {
      "epoch": 3.09,
      "grad_norm": 0.021522188559174538,
      "learning_rate": 0.009700374531835207,
      "loss": 0.0129,
      "step": 2475
    },
    {
      "epoch": 3.09,
      "grad_norm": 0.011243757791817188,
      "learning_rate": 0.009696213066999583,
      "loss": 0.0579,
      "step": 2476
    },
    {
      "epoch": 3.09,
      "grad_norm": 0.0200231671333313,
      "learning_rate": 0.009692051602163961,
      "loss": 0.1572,
      "step": 2477
    },
    {
      "epoch": 3.09,
      "grad_norm": 0.014522547833621502,
      "learning_rate": 0.00968789013732834,
      "loss": 0.1207,
      "step": 2478
    },
    {
      "epoch": 3.09,
      "grad_norm": 0.01773005723953247,
      "learning_rate": 0.009683728672492717,
      "loss": 0.1554,
      "step": 2479
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.008470378816127777,
      "learning_rate": 0.009679567207657095,
      "loss": 0.0334,
      "step": 2480
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.025696830824017525,
      "learning_rate": 0.009675405742821474,
      "loss": 0.1791,
      "step": 2481
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.021830515936017036,
      "learning_rate": 0.00967124427798585,
      "loss": 0.1042,
      "step": 2482
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.015185046941041946,
      "learning_rate": 0.009667082813150228,
      "loss": 0.2104,
      "step": 2483
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.0228714682161808,
      "learning_rate": 0.009662921348314608,
      "loss": 0.1377,
      "step": 2484
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.010184620507061481,
      "learning_rate": 0.009658759883478986,
      "loss": 0.0442,
      "step": 2485
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.00041623174911364913,
      "learning_rate": 0.009654598418643362,
      "loss": 0.0004,
      "step": 2486
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.022830571979284286,
      "learning_rate": 0.009650436953807741,
      "loss": 0.1907,
      "step": 2487
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.017804017290472984,
      "learning_rate": 0.009646275488972119,
      "loss": 0.1046,
      "step": 2488
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.02518698200583458,
      "learning_rate": 0.009642114024136495,
      "loss": 0.2267,
      "step": 2489
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.021543951705098152,
      "learning_rate": 0.009637952559300875,
      "loss": 0.1138,
      "step": 2490
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.014395716600120068,
      "learning_rate": 0.009633791094465253,
      "loss": 0.1697,
      "step": 2491
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.011900425888597965,
      "learning_rate": 0.009629629629629629,
      "loss": 0.0329,
      "step": 2492
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.03548606485128403,
      "learning_rate": 0.009625468164794008,
      "loss": 0.3525,
      "step": 2493
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.014830447733402252,
      "learning_rate": 0.009621306699958386,
      "loss": 0.1154,
      "step": 2494
    },
    {
      "epoch": 3.11,
      "grad_norm": 0.016261432319879532,
      "learning_rate": 0.009617145235122764,
      "loss": 0.1439,
      "step": 2495
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.021658090874552727,
      "learning_rate": 0.009612983770287142,
      "loss": 0.0789,
      "step": 2496
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.025524383410811424,
      "learning_rate": 0.00960882230545152,
      "loss": 0.3042,
      "step": 2497
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.02667488157749176,
      "learning_rate": 0.009604660840615897,
      "loss": 0.0635,
      "step": 2498
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.015851739794015884,
      "learning_rate": 0.009600499375780275,
      "loss": 0.0883,
      "step": 2499
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.013383430428802967,
      "learning_rate": 0.009596337910944653,
      "loss": 0.1252,
      "step": 2500
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.017047205939888954,
      "learning_rate": 0.00959217644610903,
      "loss": 0.0452,
      "step": 2501
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.020063329488039017,
      "learning_rate": 0.009588014981273409,
      "loss": 0.1925,
      "step": 2502
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.02162250317633152,
      "learning_rate": 0.009583853516437786,
      "loss": 0.075,
      "step": 2503
    },
    {
      "epoch": 3.13,
      "grad_norm": 0.01959211938083172,
      "learning_rate": 0.009579692051602164,
      "loss": 0.1742,
      "step": 2504
    },
    {
      "epoch": 3.13,
      "grad_norm": 0.014069776050746441,
      "learning_rate": 0.009575530586766542,
      "loss": 0.0883,
      "step": 2505
    },
    {
      "epoch": 3.13,
      "grad_norm": 0.021891888231039047,
      "learning_rate": 0.00957136912193092,
      "loss": 0.1553,
      "step": 2506
    },
    {
      "epoch": 3.13,
      "grad_norm": 0.022016119211912155,
      "learning_rate": 0.009567207657095298,
      "loss": 0.1786,
      "step": 2507
    },
    {
      "epoch": 3.13,
      "grad_norm": 0.019769936800003052,
      "learning_rate": 0.009563046192259676,
      "loss": 0.0972,
      "step": 2508
    },
    {
      "epoch": 3.13,
      "grad_norm": 0.0328281968832016,
      "learning_rate": 0.009558884727424053,
      "loss": 0.343,
      "step": 2509
    },
    {
      "epoch": 3.13,
      "grad_norm": 0.005996908992528915,
      "learning_rate": 0.009554723262588431,
      "loss": 0.0116,
      "step": 2510
    },
    {
      "epoch": 3.13,
      "grad_norm": 0.03438617289066315,
      "learning_rate": 0.009550561797752809,
      "loss": 0.3628,
      "step": 2511
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.008164980448782444,
      "learning_rate": 0.009546400332917187,
      "loss": 0.0215,
      "step": 2512
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.023791354149580002,
      "learning_rate": 0.009542238868081565,
      "loss": 0.1295,
      "step": 2513
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.034527987241744995,
      "learning_rate": 0.009538077403245943,
      "loss": 0.0504,
      "step": 2514
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.018946828320622444,
      "learning_rate": 0.00953391593841032,
      "loss": 0.1432,
      "step": 2515
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.023394828662276268,
      "learning_rate": 0.009529754473574698,
      "loss": 0.2289,
      "step": 2516
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.024870490655303,
      "learning_rate": 0.009525593008739076,
      "loss": 0.1815,
      "step": 2517
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.013807104900479317,
      "learning_rate": 0.009521431543903454,
      "loss": 0.056,
      "step": 2518
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.018894806504249573,
      "learning_rate": 0.009517270079067832,
      "loss": 0.1672,
      "step": 2519
    },
    {
      "epoch": 3.15,
      "grad_norm": 0.020069757476449013,
      "learning_rate": 0.00951310861423221,
      "loss": 0.2883,
      "step": 2520
    },
    {
      "epoch": 3.15,
      "grad_norm": 0.020180197432637215,
      "learning_rate": 0.009508947149396587,
      "loss": 0.1802,
      "step": 2521
    },
    {
      "epoch": 3.15,
      "grad_norm": 0.01832968182861805,
      "learning_rate": 0.009504785684560965,
      "loss": 0.1207,
      "step": 2522
    },
    {
      "epoch": 3.15,
      "grad_norm": 0.017384953796863556,
      "learning_rate": 0.009500624219725343,
      "loss": 0.1187,
      "step": 2523
    },
    {
      "epoch": 3.15,
      "grad_norm": 0.018465112894773483,
      "learning_rate": 0.00949646275488972,
      "loss": 0.1277,
      "step": 2524
    },
    {
      "epoch": 3.15,
      "grad_norm": 0.00027085482724942267,
      "learning_rate": 0.009492301290054099,
      "loss": 0.0003,
      "step": 2525
    },
    {
      "epoch": 3.15,
      "grad_norm": 0.01772036962211132,
      "learning_rate": 0.009488139825218476,
      "loss": 0.0619,
      "step": 2526
    },
    {
      "epoch": 3.15,
      "grad_norm": 0.028598492965102196,
      "learning_rate": 0.009483978360382854,
      "loss": 0.2756,
      "step": 2527
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.030054278671741486,
      "learning_rate": 0.009479816895547234,
      "loss": 0.2173,
      "step": 2528
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.017732640728354454,
      "learning_rate": 0.00947565543071161,
      "loss": 0.0964,
      "step": 2529
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.011158769018948078,
      "learning_rate": 0.009471493965875988,
      "loss": 0.0255,
      "step": 2530
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.044988930225372314,
      "learning_rate": 0.009467332501040367,
      "loss": 0.0461,
      "step": 2531
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.015821319073438644,
      "learning_rate": 0.009463171036204743,
      "loss": 0.0997,
      "step": 2532
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.00718588987365365,
      "learning_rate": 0.009459009571369123,
      "loss": 0.0108,
      "step": 2533
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.019648442044854164,
      "learning_rate": 0.0094548481065335,
      "loss": 0.0533,
      "step": 2534
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.02705501951277256,
      "learning_rate": 0.009450686641697877,
      "loss": 0.3577,
      "step": 2535
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.023254407569766045,
      "learning_rate": 0.009446525176862256,
      "loss": 0.2788,
      "step": 2536
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.012933947145938873,
      "learning_rate": 0.009442363712026634,
      "loss": 0.0806,
      "step": 2537
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.024196429178118706,
      "learning_rate": 0.00943820224719101,
      "loss": 0.2007,
      "step": 2538
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.023604491725564003,
      "learning_rate": 0.00943404078235539,
      "loss": 0.1586,
      "step": 2539
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.02664104476571083,
      "learning_rate": 0.009429879317519768,
      "loss": 0.1642,
      "step": 2540
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.028598077595233917,
      "learning_rate": 0.009425717852684144,
      "loss": 0.4636,
      "step": 2541
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.020922275260090828,
      "learning_rate": 0.009421556387848523,
      "loss": 0.1594,
      "step": 2542
    },
    {
      "epoch": 3.17,
      "grad_norm": 0.017941230908036232,
      "learning_rate": 0.009417394923012901,
      "loss": 0.0911,
      "step": 2543
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.02072943188250065,
      "learning_rate": 0.009413233458177279,
      "loss": 0.2108,
      "step": 2544
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.025806430727243423,
      "learning_rate": 0.009409071993341657,
      "loss": 0.2756,
      "step": 2545
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.022617490962147713,
      "learning_rate": 0.009404910528506035,
      "loss": 0.1584,
      "step": 2546
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.0226244255900383,
      "learning_rate": 0.009400749063670413,
      "loss": 0.2576,
      "step": 2547
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.02542813867330551,
      "learning_rate": 0.00939658759883479,
      "loss": 0.2512,
      "step": 2548
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.01643618755042553,
      "learning_rate": 0.009392426133999168,
      "loss": 0.0693,
      "step": 2549
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.02597830630838871,
      "learning_rate": 0.009388264669163546,
      "loss": 0.2673,
      "step": 2550
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.012496350333094597,
      "learning_rate": 0.009384103204327924,
      "loss": 0.0215,
      "step": 2551
    },
    {
      "epoch": 3.19,
      "grad_norm": 0.01204951573163271,
      "learning_rate": 0.009379941739492302,
      "loss": 0.0295,
      "step": 2552
    },
    {
      "epoch": 3.19,
      "grad_norm": 0.03000875748693943,
      "learning_rate": 0.00937578027465668,
      "loss": 0.2825,
      "step": 2553
    },
    {
      "epoch": 3.19,
      "grad_norm": 0.01222654152661562,
      "learning_rate": 0.009371618809821057,
      "loss": 0.0747,
      "step": 2554
    },
    {
      "epoch": 3.19,
      "grad_norm": 0.02645239233970642,
      "learning_rate": 0.009367457344985435,
      "loss": 0.3013,
      "step": 2555
    },
    {
      "epoch": 3.19,
      "grad_norm": 0.009922386147081852,
      "learning_rate": 0.009363295880149813,
      "loss": 0.0479,
      "step": 2556
    },
    {
      "epoch": 3.19,
      "grad_norm": 0.022270984947681427,
      "learning_rate": 0.009359134415314191,
      "loss": 0.1327,
      "step": 2557
    },
    {
      "epoch": 3.19,
      "grad_norm": 0.04168084263801575,
      "learning_rate": 0.009354972950478569,
      "loss": 0.3081,
      "step": 2558
    },
    {
      "epoch": 3.19,
      "grad_norm": 0.017189987003803253,
      "learning_rate": 0.009350811485642947,
      "loss": 0.0768,
      "step": 2559
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.03361440822482109,
      "learning_rate": 0.009346650020807324,
      "loss": 0.1881,
      "step": 2560
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.017580794170498848,
      "learning_rate": 0.009342488555971702,
      "loss": 0.0824,
      "step": 2561
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.020315133035182953,
      "learning_rate": 0.00933832709113608,
      "loss": 0.1342,
      "step": 2562
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.017990244552493095,
      "learning_rate": 0.009334165626300458,
      "loss": 0.1403,
      "step": 2563
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.02053309604525566,
      "learning_rate": 0.009330004161464836,
      "loss": 0.1797,
      "step": 2564
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.02499476633965969,
      "learning_rate": 0.009325842696629214,
      "loss": 0.2102,
      "step": 2565
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.024845337495207787,
      "learning_rate": 0.009321681231793591,
      "loss": 0.2634,
      "step": 2566
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.011849860660731792,
      "learning_rate": 0.00931751976695797,
      "loss": 0.0435,
      "step": 2567
    },
    {
      "epoch": 3.21,
      "grad_norm": 0.007768295705318451,
      "learning_rate": 0.009313358302122347,
      "loss": 0.0307,
      "step": 2568
    },
    {
      "epoch": 3.21,
      "grad_norm": 0.01642557606101036,
      "learning_rate": 0.009309196837286725,
      "loss": 0.0644,
      "step": 2569
    },
    {
      "epoch": 3.21,
      "grad_norm": 0.0062418123707175255,
      "learning_rate": 0.009305035372451103,
      "loss": 0.0144,
      "step": 2570
    },
    {
      "epoch": 3.21,
      "grad_norm": 0.022596236318349838,
      "learning_rate": 0.009300873907615482,
      "loss": 0.0822,
      "step": 2571
    },
    {
      "epoch": 3.21,
      "grad_norm": 0.02564779855310917,
      "learning_rate": 0.009296712442779858,
      "loss": 0.2537,
      "step": 2572
    },
    {
      "epoch": 3.21,
      "grad_norm": 0.01664002798497677,
      "learning_rate": 0.009292550977944236,
      "loss": 0.0726,
      "step": 2573
    },
    {
      "epoch": 3.21,
      "grad_norm": 0.02504446916282177,
      "learning_rate": 0.009288389513108616,
      "loss": 0.2507,
      "step": 2574
    },
    {
      "epoch": 3.21,
      "grad_norm": 0.01178866159170866,
      "learning_rate": 0.009284228048272992,
      "loss": 0.0288,
      "step": 2575
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.025660453364253044,
      "learning_rate": 0.00928006658343737,
      "loss": 0.0396,
      "step": 2576
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.0390174426138401,
      "learning_rate": 0.00927590511860175,
      "loss": 0.2261,
      "step": 2577
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.008115018717944622,
      "learning_rate": 0.009271743653766125,
      "loss": 0.0116,
      "step": 2578
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.03414725512266159,
      "learning_rate": 0.009267582188930503,
      "loss": 0.0892,
      "step": 2579
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.02717483974993229,
      "learning_rate": 0.009263420724094883,
      "loss": 0.1913,
      "step": 2580
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.038689177483320236,
      "learning_rate": 0.009259259259259259,
      "loss": 0.2656,
      "step": 2581
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.02295675128698349,
      "learning_rate": 0.009255097794423637,
      "loss": 0.2249,
      "step": 2582
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.01686338521540165,
      "learning_rate": 0.009250936329588016,
      "loss": 0.0481,
      "step": 2583
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.012124914675951004,
      "learning_rate": 0.009246774864752392,
      "loss": 0.0289,
      "step": 2584
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.03647255897521973,
      "learning_rate": 0.00924261339991677,
      "loss": 0.259,
      "step": 2585
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.01829954981803894,
      "learning_rate": 0.00923845193508115,
      "loss": 0.1069,
      "step": 2586
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.02899562008678913,
      "learning_rate": 0.009234290470245527,
      "loss": 0.1813,
      "step": 2587
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.01667650230228901,
      "learning_rate": 0.009230129005409904,
      "loss": 0.0612,
      "step": 2588
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.030189182609319687,
      "learning_rate": 0.009225967540574283,
      "loss": 0.2054,
      "step": 2589
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.03184523433446884,
      "learning_rate": 0.009221806075738661,
      "loss": 0.407,
      "step": 2590
    },
    {
      "epoch": 3.23,
      "grad_norm": 0.022992687299847603,
      "learning_rate": 0.009217644610903037,
      "loss": 0.2,
      "step": 2591
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.01704871468245983,
      "learning_rate": 0.009213483146067417,
      "loss": 0.0833,
      "step": 2592
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.024289730936288834,
      "learning_rate": 0.009209321681231794,
      "loss": 0.158,
      "step": 2593
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.028591252863407135,
      "learning_rate": 0.00920516021639617,
      "loss": 0.3352,
      "step": 2594
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.026857592165470123,
      "learning_rate": 0.00920099875156055,
      "loss": 0.1373,
      "step": 2595
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.01839209906756878,
      "learning_rate": 0.009196837286724928,
      "loss": 0.1431,
      "step": 2596
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.017433999106287956,
      "learning_rate": 0.009192675821889304,
      "loss": 0.1188,
      "step": 2597
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.026039427146315575,
      "learning_rate": 0.009188514357053684,
      "loss": 0.2009,
      "step": 2598
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.009098396636545658,
      "learning_rate": 0.009184352892218061,
      "loss": 0.0228,
      "step": 2599
    },
    {
      "epoch": 3.25,
      "grad_norm": 0.02237296663224697,
      "learning_rate": 0.009180191427382437,
      "loss": 0.1628,
      "step": 2600
    },
    {
      "epoch": 3.25,
      "grad_norm": 0.031112171709537506,
      "learning_rate": 0.009176029962546817,
      "loss": 0.2837,
      "step": 2601
    },
    {
      "epoch": 3.25,
      "grad_norm": 0.006179771386086941,
      "learning_rate": 0.009171868497711195,
      "loss": 0.0163,
      "step": 2602
    },
    {
      "epoch": 3.25,
      "grad_norm": 0.009609160013496876,
      "learning_rate": 0.009167707032875573,
      "loss": 0.0273,
      "step": 2603
    },
    {
      "epoch": 3.25,
      "grad_norm": 0.028470231220126152,
      "learning_rate": 0.00916354556803995,
      "loss": 0.2764,
      "step": 2604
    },
    {
      "epoch": 3.25,
      "grad_norm": 0.03331373259425163,
      "learning_rate": 0.009159384103204328,
      "loss": 0.3008,
      "step": 2605
    },
    {
      "epoch": 3.25,
      "grad_norm": 0.01581025868654251,
      "learning_rate": 0.009155222638368706,
      "loss": 0.0686,
      "step": 2606
    },
    {
      "epoch": 3.25,
      "grad_norm": 0.02964920364320278,
      "learning_rate": 0.009151061173533084,
      "loss": 0.1688,
      "step": 2607
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.031542208045721054,
      "learning_rate": 0.009146899708697462,
      "loss": 0.2161,
      "step": 2608
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.02060599997639656,
      "learning_rate": 0.00914273824386184,
      "loss": 0.1016,
      "step": 2609
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.02767704613506794,
      "learning_rate": 0.009138576779026217,
      "loss": 0.0159,
      "step": 2610
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.007487928029149771,
      "learning_rate": 0.009134415314190595,
      "loss": 0.022,
      "step": 2611
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.019024603068828583,
      "learning_rate": 0.009130253849354973,
      "loss": 0.0847,
      "step": 2612
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.01556155551224947,
      "learning_rate": 0.009126092384519351,
      "loss": 0.0881,
      "step": 2613
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.024674419313669205,
      "learning_rate": 0.009121930919683729,
      "loss": 0.245,
      "step": 2614
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.015050250105559826,
      "learning_rate": 0.009117769454848107,
      "loss": 0.1092,
      "step": 2615
    },
    {
      "epoch": 3.27,
      "grad_norm": 0.013809808529913425,
      "learning_rate": 0.009113607990012484,
      "loss": 0.0095,
      "step": 2616
    },
    {
      "epoch": 3.27,
      "grad_norm": 0.033971235156059265,
      "learning_rate": 0.009109446525176862,
      "loss": 0.1394,
      "step": 2617
    },
    {
      "epoch": 3.27,
      "grad_norm": 0.03051542118191719,
      "learning_rate": 0.00910528506034124,
      "loss": 0.2023,
      "step": 2618
    },
    {
      "epoch": 3.27,
      "grad_norm": 0.03449365124106407,
      "learning_rate": 0.009101123595505618,
      "loss": 0.5874,
      "step": 2619
    },
    {
      "epoch": 3.27,
      "grad_norm": 0.019527500495314598,
      "learning_rate": 0.009096962130669996,
      "loss": 0.1328,
      "step": 2620
    },
    {
      "epoch": 3.27,
      "grad_norm": 0.015199102461338043,
      "learning_rate": 0.009092800665834374,
      "loss": 0.2229,
      "step": 2621
    },
    {
      "epoch": 3.27,
      "grad_norm": 0.019259827211499214,
      "learning_rate": 0.009088639200998751,
      "loss": 0.1663,
      "step": 2622
    },
    {
      "epoch": 3.27,
      "grad_norm": 0.013357378542423248,
      "learning_rate": 0.00908447773616313,
      "loss": 0.0302,
      "step": 2623
    },
    {
      "epoch": 3.28,
      "grad_norm": 0.030197937041521072,
      "learning_rate": 0.009080316271327507,
      "loss": 0.1714,
      "step": 2624
    },
    {
      "epoch": 3.28,
      "grad_norm": 0.019391518086194992,
      "learning_rate": 0.009076154806491885,
      "loss": 0.0984,
      "step": 2625
    },
    {
      "epoch": 3.28,
      "grad_norm": 0.03460409492254257,
      "learning_rate": 0.009071993341656263,
      "loss": 0.3203,
      "step": 2626
    },
    {
      "epoch": 3.28,
      "grad_norm": 0.02170703187584877,
      "learning_rate": 0.00906783187682064,
      "loss": 0.1638,
      "step": 2627
    },
    {
      "epoch": 3.28,
      "grad_norm": 0.011229234747588634,
      "learning_rate": 0.009063670411985018,
      "loss": 0.0341,
      "step": 2628
    },
    {
      "epoch": 3.28,
      "grad_norm": 0.030574314296245575,
      "learning_rate": 0.009059508947149396,
      "loss": 0.3169,
      "step": 2629
    },
    {
      "epoch": 3.28,
      "grad_norm": 0.01634969376027584,
      "learning_rate": 0.009055347482313776,
      "loss": 0.157,
      "step": 2630
    },
    {
      "epoch": 3.28,
      "grad_norm": 0.027725601568818092,
      "learning_rate": 0.009051186017478152,
      "loss": 0.3564,
      "step": 2631
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.02907095104455948,
      "learning_rate": 0.00904702455264253,
      "loss": 0.2944,
      "step": 2632
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.01694933883845806,
      "learning_rate": 0.00904286308780691,
      "loss": 0.1075,
      "step": 2633
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.02143993228673935,
      "learning_rate": 0.009038701622971285,
      "loss": 0.2942,
      "step": 2634
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.013675765134394169,
      "learning_rate": 0.009034540158135663,
      "loss": 0.0419,
      "step": 2635
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.023643000051379204,
      "learning_rate": 0.009030378693300043,
      "loss": 0.1636,
      "step": 2636
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.025515517219901085,
      "learning_rate": 0.009026217228464419,
      "loss": 0.2217,
      "step": 2637
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.02886452153325081,
      "learning_rate": 0.009022055763628797,
      "loss": 0.322,
      "step": 2638
    },
    {
      "epoch": 3.29,
      "grad_norm": 0.016863305121660233,
      "learning_rate": 0.009017894298793176,
      "loss": 0.0798,
      "step": 2639
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.025321654975414276,
      "learning_rate": 0.009013732833957552,
      "loss": 0.175,
      "step": 2640
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.026228178292512894,
      "learning_rate": 0.009009571369121932,
      "loss": 0.2296,
      "step": 2641
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.01180315762758255,
      "learning_rate": 0.00900540990428631,
      "loss": 0.0396,
      "step": 2642
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.03126484155654907,
      "learning_rate": 0.009001248439450686,
      "loss": 0.4375,
      "step": 2643
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.018091103062033653,
      "learning_rate": 0.008997086974615065,
      "loss": 0.0845,
      "step": 2644
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.03543442487716675,
      "learning_rate": 0.008992925509779443,
      "loss": 0.1895,
      "step": 2645
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.000963021710049361,
      "learning_rate": 0.008988764044943821,
      "loss": 0.0006,
      "step": 2646
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.025264540687203407,
      "learning_rate": 0.008984602580108199,
      "loss": 0.1056,
      "step": 2647
    },
    {
      "epoch": 3.31,
      "grad_norm": 0.02594422921538353,
      "learning_rate": 0.008980441115272577,
      "loss": 0.2051,
      "step": 2648
    },
    {
      "epoch": 3.31,
      "grad_norm": 0.01939859613776207,
      "learning_rate": 0.008976279650436954,
      "loss": 0.0982,
      "step": 2649
    },
    {
      "epoch": 3.31,
      "grad_norm": 0.02503388375043869,
      "learning_rate": 0.008972118185601332,
      "loss": 0.1761,
      "step": 2650
    },
    {
      "epoch": 3.31,
      "grad_norm": 0.024953236803412437,
      "learning_rate": 0.00896795672076571,
      "loss": 0.0961,
      "step": 2651
    },
    {
      "epoch": 3.31,
      "grad_norm": 0.030357329174876213,
      "learning_rate": 0.008963795255930088,
      "loss": 0.2598,
      "step": 2652
    },
    {
      "epoch": 3.31,
      "grad_norm": 0.024141918867826462,
      "learning_rate": 0.008959633791094466,
      "loss": 0.3018,
      "step": 2653
    },
    {
      "epoch": 3.31,
      "grad_norm": 0.013627313077449799,
      "learning_rate": 0.008955472326258844,
      "loss": 0.1057,
      "step": 2654
    },
    {
      "epoch": 3.31,
      "grad_norm": 0.01146067213267088,
      "learning_rate": 0.008951310861423221,
      "loss": 0.0565,
      "step": 2655
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.025223776698112488,
      "learning_rate": 0.0089471493965876,
      "loss": 0.175,
      "step": 2656
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.022137565538287163,
      "learning_rate": 0.008942987931751977,
      "loss": 0.2109,
      "step": 2657
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.02532542310655117,
      "learning_rate": 0.008938826466916355,
      "loss": 0.2766,
      "step": 2658
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.009322497062385082,
      "learning_rate": 0.008934665002080733,
      "loss": 0.0288,
      "step": 2659
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.0386841706931591,
      "learning_rate": 0.00893050353724511,
      "loss": 0.3069,
      "step": 2660
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.017039045691490173,
      "learning_rate": 0.008926342072409488,
      "loss": 0.1769,
      "step": 2661
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.01864389330148697,
      "learning_rate": 0.008922180607573866,
      "loss": 0.187,
      "step": 2662
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.0006328356103040278,
      "learning_rate": 0.008918019142738244,
      "loss": 0.0006,
      "step": 2663
    },
    {
      "epoch": 3.33,
      "grad_norm": 0.02236308716237545,
      "learning_rate": 0.008913857677902622,
      "loss": 0.117,
      "step": 2664
    },
    {
      "epoch": 3.33,
      "grad_norm": 0.021154416725039482,
      "learning_rate": 0.008909696213067,
      "loss": 0.4656,
      "step": 2665
    },
    {
      "epoch": 3.33,
      "grad_norm": 0.022737789899110794,
      "learning_rate": 0.008905534748231378,
      "loss": 0.1505,
      "step": 2666
    },
    {
      "epoch": 3.33,
      "grad_norm": 0.02034568414092064,
      "learning_rate": 0.008901373283395755,
      "loss": 0.1304,
      "step": 2667
    },
    {
      "epoch": 3.33,
      "grad_norm": 0.02606433629989624,
      "learning_rate": 0.008897211818560133,
      "loss": 0.1766,
      "step": 2668
    },
    {
      "epoch": 3.33,
      "grad_norm": 0.025475304573774338,
      "learning_rate": 0.008893050353724511,
      "loss": 0.1561,
      "step": 2669
    },
    {
      "epoch": 3.33,
      "grad_norm": 0.025865741074085236,
      "learning_rate": 0.008888888888888889,
      "loss": 0.1611,
      "step": 2670
    },
    {
      "epoch": 3.33,
      "grad_norm": 0.029380349442362785,
      "learning_rate": 0.008884727424053267,
      "loss": 0.142,
      "step": 2671
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.020789284259080887,
      "learning_rate": 0.008880565959217645,
      "loss": 0.1433,
      "step": 2672
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.021456561982631683,
      "learning_rate": 0.008876404494382022,
      "loss": 0.208,
      "step": 2673
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.028116440400481224,
      "learning_rate": 0.0088722430295464,
      "loss": 0.1981,
      "step": 2674
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.016003265976905823,
      "learning_rate": 0.008868081564710778,
      "loss": 0.0693,
      "step": 2675
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.022476254031062126,
      "learning_rate": 0.008863920099875156,
      "loss": 0.135,
      "step": 2676
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.04497523233294487,
      "learning_rate": 0.008859758635039534,
      "loss": 0.5327,
      "step": 2677
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.006921012420207262,
      "learning_rate": 0.008855597170203912,
      "loss": 0.0229,
      "step": 2678
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.022248296067118645,
      "learning_rate": 0.008851435705368291,
      "loss": 0.0181,
      "step": 2679
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.026960788294672966,
      "learning_rate": 0.008847274240532667,
      "loss": 0.244,
      "step": 2680
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.027303334325551987,
      "learning_rate": 0.008843112775697045,
      "loss": 0.2817,
      "step": 2681
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.04584592580795288,
      "learning_rate": 0.008838951310861425,
      "loss": 0.3206,
      "step": 2682
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.01855398342013359,
      "learning_rate": 0.0088347898460258,
      "loss": 0.0625,
      "step": 2683
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.03189504146575928,
      "learning_rate": 0.008830628381190178,
      "loss": 0.407,
      "step": 2684
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.022018741816282272,
      "learning_rate": 0.008826466916354558,
      "loss": 0.1382,
      "step": 2685
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.0312019195407629,
      "learning_rate": 0.008822305451518934,
      "loss": 0.2493,
      "step": 2686
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.02151739038527012,
      "learning_rate": 0.008818143986683312,
      "loss": 0.3005,
      "step": 2687
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.02674611285328865,
      "learning_rate": 0.008813982521847692,
      "loss": 0.2124,
      "step": 2688
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.0214630626142025,
      "learning_rate": 0.00880982105701207,
      "loss": 0.1237,
      "step": 2689
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.01761213131248951,
      "learning_rate": 0.008805659592176445,
      "loss": 0.0784,
      "step": 2690
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.023466164246201515,
      "learning_rate": 0.008801498127340825,
      "loss": 0.2216,
      "step": 2691
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.00018389504111837596,
      "learning_rate": 0.008797336662505203,
      "loss": 0.0002,
      "step": 2692
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.020099781453609467,
      "learning_rate": 0.008793175197669579,
      "loss": 0.194,
      "step": 2693
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.02702322229743004,
      "learning_rate": 0.008789013732833958,
      "loss": 0.3716,
      "step": 2694
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.024144737049937248,
      "learning_rate": 0.008784852267998336,
      "loss": 0.2803,
      "step": 2695
    },
    {
      "epoch": 3.37,
      "grad_norm": 0.02490830048918724,
      "learning_rate": 0.008780690803162712,
      "loss": 0.2397,
      "step": 2696
    },
    {
      "epoch": 3.37,
      "grad_norm": 0.020832791924476624,
      "learning_rate": 0.008776529338327092,
      "loss": 0.1455,
      "step": 2697
    },
    {
      "epoch": 3.37,
      "grad_norm": 0.027468057349324226,
      "learning_rate": 0.00877236787349147,
      "loss": 0.2866,
      "step": 2698
    },
    {
      "epoch": 3.37,
      "grad_norm": 0.01759127527475357,
      "learning_rate": 0.008768206408655846,
      "loss": 0.1141,
      "step": 2699
    },
    {
      "epoch": 3.37,
      "grad_norm": 0.02749781310558319,
      "learning_rate": 0.008764044943820225,
      "loss": 0.1864,
      "step": 2700
    },
    {
      "epoch": 3.37,
      "eval_loss": 0.2442626953125,
      "eval_runtime": 183.2492,
      "eval_samples_per_second": 1.097,
      "eval_steps_per_second": 0.551,
      "step": 2700
    },
    {
      "epoch": 3.37,
      "grad_norm": 0.027531808242201805,
      "learning_rate": 0.008759883478984603,
      "loss": 0.0817,
      "step": 2701
    },
    {
      "epoch": 3.37,
      "grad_norm": 0.007252815179526806,
      "learning_rate": 0.00875572201414898,
      "loss": 0.0235,
      "step": 2702
    },
    {
      "epoch": 3.37,
      "grad_norm": 0.017111804336309433,
      "learning_rate": 0.008751560549313359,
      "loss": 0.0869,
      "step": 2703
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.01851665787398815,
      "learning_rate": 0.008747399084477737,
      "loss": 0.093,
      "step": 2704
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.03231269493699074,
      "learning_rate": 0.008743237619642115,
      "loss": 0.2588,
      "step": 2705
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.02228645794093609,
      "learning_rate": 0.008739076154806492,
      "loss": 0.0773,
      "step": 2706
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.015556877478957176,
      "learning_rate": 0.00873491468997087,
      "loss": 0.1002,
      "step": 2707
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.022147489711642265,
      "learning_rate": 0.008730753225135248,
      "loss": 0.2971,
      "step": 2708
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.02056322805583477,
      "learning_rate": 0.008726591760299626,
      "loss": 0.167,
      "step": 2709
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.019417835399508476,
      "learning_rate": 0.008722430295464004,
      "loss": 0.171,
      "step": 2710
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.027736961841583252,
      "learning_rate": 0.008718268830628382,
      "loss": 0.2776,
      "step": 2711
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.026281418278813362,
      "learning_rate": 0.00871410736579276,
      "loss": 0.1378,
      "step": 2712
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.0245025884360075,
      "learning_rate": 0.008709945900957137,
      "loss": 0.0487,
      "step": 2713
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.030687853693962097,
      "learning_rate": 0.008705784436121515,
      "loss": 0.158,
      "step": 2714
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.0058065373450517654,
      "learning_rate": 0.008701622971285893,
      "loss": 0.01,
      "step": 2715
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.01900874264538288,
      "learning_rate": 0.00869746150645027,
      "loss": 0.2539,
      "step": 2716
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.03169355168938637,
      "learning_rate": 0.008693300041614649,
      "loss": 0.3271,
      "step": 2717
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.019273074343800545,
      "learning_rate": 0.008689138576779026,
      "loss": 0.0075,
      "step": 2718
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.020431047305464745,
      "learning_rate": 0.008684977111943404,
      "loss": 0.1545,
      "step": 2719
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.029787341132760048,
      "learning_rate": 0.008680815647107782,
      "loss": 0.2842,
      "step": 2720
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.01724167726933956,
      "learning_rate": 0.00867665418227216,
      "loss": 0.0975,
      "step": 2721
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.027891315519809723,
      "learning_rate": 0.008672492717436538,
      "loss": 0.1251,
      "step": 2722
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.016540469601750374,
      "learning_rate": 0.008668331252600915,
      "loss": 0.0781,
      "step": 2723
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.002254538470879197,
      "learning_rate": 0.008664169787765293,
      "loss": 0.0015,
      "step": 2724
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.026004977524280548,
      "learning_rate": 0.008660008322929671,
      "loss": 0.1027,
      "step": 2725
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.027139605954289436,
      "learning_rate": 0.008655846858094049,
      "loss": 0.1993,
      "step": 2726
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.015601463615894318,
      "learning_rate": 0.008651685393258427,
      "loss": 0.1313,
      "step": 2727
    },
    {
      "epoch": 3.41,
      "grad_norm": 0.01579982042312622,
      "learning_rate": 0.008647523928422805,
      "loss": 0.0698,
      "step": 2728
    },
    {
      "epoch": 3.41,
      "grad_norm": 0.05024229735136032,
      "learning_rate": 0.008643362463587182,
      "loss": 0.8604,
      "step": 2729
    },
    {
      "epoch": 3.41,
      "grad_norm": 0.011840304359793663,
      "learning_rate": 0.00863920099875156,
      "loss": 0.0431,
      "step": 2730
    },
    {
      "epoch": 3.41,
      "grad_norm": 0.014867421239614487,
      "learning_rate": 0.008635039533915938,
      "loss": 0.075,
      "step": 2731
    },
    {
      "epoch": 3.41,
      "grad_norm": 0.010600496083498001,
      "learning_rate": 0.008630878069080318,
      "loss": 0.036,
      "step": 2732
    },
    {
      "epoch": 3.41,
      "grad_norm": 0.04100199416279793,
      "learning_rate": 0.008626716604244694,
      "loss": 0.5415,
      "step": 2733
    },
    {
      "epoch": 3.41,
      "grad_norm": 0.01639566384255886,
      "learning_rate": 0.008622555139409072,
      "loss": 0.0602,
      "step": 2734
    },
    {
      "epoch": 3.41,
      "grad_norm": 0.02300787903368473,
      "learning_rate": 0.008618393674573451,
      "loss": 0.1753,
      "step": 2735
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.027618087828159332,
      "learning_rate": 0.008614232209737827,
      "loss": 0.1984,
      "step": 2736
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.025612149387598038,
      "learning_rate": 0.008610070744902205,
      "loss": 0.2035,
      "step": 2737
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.036267802119255066,
      "learning_rate": 0.008605909280066585,
      "loss": 0.2573,
      "step": 2738
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.02440420724451542,
      "learning_rate": 0.00860174781523096,
      "loss": 0.1826,
      "step": 2739
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.0234316885471344,
      "learning_rate": 0.008597586350395339,
      "loss": 0.2407,
      "step": 2740
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.02757832035422325,
      "learning_rate": 0.008593424885559718,
      "loss": 0.1527,
      "step": 2741
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.027780169621109962,
      "learning_rate": 0.008589263420724094,
      "loss": 0.1214,
      "step": 2742
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.029875468462705612,
      "learning_rate": 0.008585101955888472,
      "loss": 0.3445,
      "step": 2743
    },
    {
      "epoch": 3.43,
      "grad_norm": 0.0228902418166399,
      "learning_rate": 0.008580940491052852,
      "loss": 0.0624,
      "step": 2744
    },
    {
      "epoch": 3.43,
      "grad_norm": 0.01788574643433094,
      "learning_rate": 0.008576779026217228,
      "loss": 0.2379,
      "step": 2745
    },
    {
      "epoch": 3.43,
      "grad_norm": 0.031761735677719116,
      "learning_rate": 0.008572617561381606,
      "loss": 0.2208,
      "step": 2746
    },
    {
      "epoch": 3.43,
      "grad_norm": 0.004076967481523752,
      "learning_rate": 0.008568456096545985,
      "loss": 0.0045,
      "step": 2747
    },
    {
      "epoch": 3.43,
      "grad_norm": 0.015164623036980629,
      "learning_rate": 0.008564294631710363,
      "loss": 0.1196,
      "step": 2748
    },
    {
      "epoch": 3.43,
      "grad_norm": 0.017596250399947166,
      "learning_rate": 0.00856013316687474,
      "loss": 0.0636,
      "step": 2749
    },
    {
      "epoch": 3.43,
      "grad_norm": 0.014778565615415573,
      "learning_rate": 0.008555971702039119,
      "loss": 0.0209,
      "step": 2750
    },
    {
      "epoch": 3.43,
      "grad_norm": 0.015590050257742405,
      "learning_rate": 0.008551810237203496,
      "loss": 0.099,
      "step": 2751
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.01964518241584301,
      "learning_rate": 0.008547648772367874,
      "loss": 0.1334,
      "step": 2752
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.03335970267653465,
      "learning_rate": 0.008543487307532252,
      "loss": 0.3994,
      "step": 2753
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.02596265822649002,
      "learning_rate": 0.00853932584269663,
      "loss": 0.2275,
      "step": 2754
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.02311364747583866,
      "learning_rate": 0.008535164377861008,
      "loss": 0.1576,
      "step": 2755
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.015631865710020065,
      "learning_rate": 0.008531002913025386,
      "loss": 0.0398,
      "step": 2756
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.022362275049090385,
      "learning_rate": 0.008526841448189763,
      "loss": 0.0193,
      "step": 2757
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.02649908885359764,
      "learning_rate": 0.008522679983354141,
      "loss": 0.2812,
      "step": 2758
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.024502752348780632,
      "learning_rate": 0.008518518518518519,
      "loss": 0.2357,
      "step": 2759
    },
    {
      "epoch": 3.45,
      "grad_norm": 0.02856382355093956,
      "learning_rate": 0.008514357053682897,
      "loss": 0.3279,
      "step": 2760
    },
    {
      "epoch": 3.45,
      "grad_norm": 0.023205798119306564,
      "learning_rate": 0.008510195588847275,
      "loss": 0.2891,
      "step": 2761
    },
    {
      "epoch": 3.45,
      "grad_norm": 0.012406256049871445,
      "learning_rate": 0.008506034124011653,
      "loss": 0.0406,
      "step": 2762
    },
    {
      "epoch": 3.45,
      "grad_norm": 0.021085336804389954,
      "learning_rate": 0.00850187265917603,
      "loss": 0.1429,
      "step": 2763
    },
    {
      "epoch": 3.45,
      "grad_norm": 0.02563498355448246,
      "learning_rate": 0.008497711194340408,
      "loss": 0.225,
      "step": 2764
    },
    {
      "epoch": 3.45,
      "grad_norm": 0.00634630024433136,
      "learning_rate": 0.008493549729504786,
      "loss": 0.019,
      "step": 2765
    },
    {
      "epoch": 3.45,
      "grad_norm": 0.02740875817835331,
      "learning_rate": 0.008489388264669164,
      "loss": 0.2031,
      "step": 2766
    },
    {
      "epoch": 3.45,
      "grad_norm": 0.03367919102311134,
      "learning_rate": 0.008485226799833542,
      "loss": 0.3279,
      "step": 2767
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.009068374522030354,
      "learning_rate": 0.00848106533499792,
      "loss": 0.0311,
      "step": 2768
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.03300770744681358,
      "learning_rate": 0.008476903870162297,
      "loss": 0.2352,
      "step": 2769
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.02273099310696125,
      "learning_rate": 0.008472742405326675,
      "loss": 0.1161,
      "step": 2770
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.01702265813946724,
      "learning_rate": 0.008468580940491053,
      "loss": 0.0583,
      "step": 2771
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.019137347117066383,
      "learning_rate": 0.00846441947565543,
      "loss": 0.0879,
      "step": 2772
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.024620862677693367,
      "learning_rate": 0.008460258010819809,
      "loss": 0.1626,
      "step": 2773
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.02287418209016323,
      "learning_rate": 0.008456096545984186,
      "loss": 0.1188,
      "step": 2774
    },
    {
      "epoch": 3.46,
      "grad_norm": 0.018956303596496582,
      "learning_rate": 0.008451935081148564,
      "loss": 0.1077,
      "step": 2775
    },
    {
      "epoch": 3.47,
      "grad_norm": 0.00757640041410923,
      "learning_rate": 0.008447773616312942,
      "loss": 0.0127,
      "step": 2776
    },
    {
      "epoch": 3.47,
      "grad_norm": 0.036089614033699036,
      "learning_rate": 0.00844361215147732,
      "loss": 0.1807,
      "step": 2777
    },
    {
      "epoch": 3.47,
      "grad_norm": 0.03055514581501484,
      "learning_rate": 0.008439450686641698,
      "loss": 0.3384,
      "step": 2778
    },
    {
      "epoch": 3.47,
      "grad_norm": 0.036745790392160416,
      "learning_rate": 0.008435289221806076,
      "loss": 0.1931,
      "step": 2779
    },
    {
      "epoch": 3.47,
      "grad_norm": 0.013422390446066856,
      "learning_rate": 0.008431127756970453,
      "loss": 0.0614,
      "step": 2780
    },
    {
      "epoch": 3.47,
      "grad_norm": 0.021165091544389725,
      "learning_rate": 0.008426966292134831,
      "loss": 0.1478,
      "step": 2781
    },
    {
      "epoch": 3.47,
      "grad_norm": 0.020757144317030907,
      "learning_rate": 0.008422804827299209,
      "loss": 0.0548,
      "step": 2782
    },
    {
      "epoch": 3.47,
      "grad_norm": 0.0027926729526370764,
      "learning_rate": 0.008418643362463587,
      "loss": 0.0016,
      "step": 2783
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.03222700580954552,
      "learning_rate": 0.008414481897627965,
      "loss": 0.3359,
      "step": 2784
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.019381003454327583,
      "learning_rate": 0.008410320432792343,
      "loss": 0.0974,
      "step": 2785
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.0175301656126976,
      "learning_rate": 0.00840615896795672,
      "loss": 0.036,
      "step": 2786
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.03260665386915207,
      "learning_rate": 0.0084019975031211,
      "loss": 0.2869,
      "step": 2787
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.015044496394693851,
      "learning_rate": 0.008397836038285476,
      "loss": 0.1333,
      "step": 2788
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.0001478224730817601,
      "learning_rate": 0.008393674573449854,
      "loss": 0.0002,
      "step": 2789
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.03730549290776253,
      "learning_rate": 0.008389513108614233,
      "loss": 0.2515,
      "step": 2790
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.020537035539746284,
      "learning_rate": 0.008385351643778611,
      "loss": 0.2026,
      "step": 2791
    },
    {
      "epoch": 3.49,
      "grad_norm": 0.02911492809653282,
      "learning_rate": 0.008381190178942987,
      "loss": 0.0701,
      "step": 2792
    },
    {
      "epoch": 3.49,
      "grad_norm": 0.02912314422428608,
      "learning_rate": 0.008377028714107367,
      "loss": 0.1505,
      "step": 2793
    },
    {
      "epoch": 3.49,
      "grad_norm": 0.009559287689626217,
      "learning_rate": 0.008372867249271745,
      "loss": 0.0231,
      "step": 2794
    },
    {
      "epoch": 3.49,
      "grad_norm": 0.02039315365254879,
      "learning_rate": 0.00836870578443612,
      "loss": 0.1019,
      "step": 2795
    },
    {
      "epoch": 3.49,
      "grad_norm": 0.02139090746641159,
      "learning_rate": 0.0083645443196005,
      "loss": 0.2094,
      "step": 2796
    },
    {
      "epoch": 3.49,
      "grad_norm": 0.018326913937926292,
      "learning_rate": 0.008360382854764878,
      "loss": 0.0675,
      "step": 2797
    },
    {
      "epoch": 3.49,
      "grad_norm": 0.01969095505774021,
      "learning_rate": 0.008356221389929254,
      "loss": 0.1635,
      "step": 2798
    },
    {
      "epoch": 3.49,
      "grad_norm": 0.03590421751141548,
      "learning_rate": 0.008352059925093634,
      "loss": 0.2544,
      "step": 2799
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.022121265530586243,
      "learning_rate": 0.008347898460258012,
      "loss": 0.1322,
      "step": 2800
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.019827095791697502,
      "learning_rate": 0.008343736995422388,
      "loss": 0.2178,
      "step": 2801
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.013161355629563332,
      "learning_rate": 0.008339575530586767,
      "loss": 0.0295,
      "step": 2802
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.019704125821590424,
      "learning_rate": 0.008335414065751145,
      "loss": 0.1259,
      "step": 2803
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.024147510528564453,
      "learning_rate": 0.008331252600915521,
      "loss": 0.1203,
      "step": 2804
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.022922905161976814,
      "learning_rate": 0.0083270911360799,
      "loss": 0.1708,
      "step": 2805
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.02815023623406887,
      "learning_rate": 0.008322929671244279,
      "loss": 0.2274,
      "step": 2806
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.020055389031767845,
      "learning_rate": 0.008318768206408655,
      "loss": 0.1467,
      "step": 2807
    },
    {
      "epoch": 3.51,
      "grad_norm": 0.03172684088349342,
      "learning_rate": 0.008314606741573034,
      "loss": 0.1552,
      "step": 2808
    },
    {
      "epoch": 3.51,
      "grad_norm": 0.023545783013105392,
      "learning_rate": 0.008310445276737412,
      "loss": 0.1022,
      "step": 2809
    },
    {
      "epoch": 3.51,
      "grad_norm": 0.040527500212192535,
      "learning_rate": 0.00830628381190179,
      "loss": 0.1614,
      "step": 2810
    },
    {
      "epoch": 3.51,
      "grad_norm": 0.020662348717451096,
      "learning_rate": 0.008302122347066168,
      "loss": 0.1934,
      "step": 2811
    },
    {
      "epoch": 3.51,
      "grad_norm": 0.01795474998652935,
      "learning_rate": 0.008297960882230546,
      "loss": 0.053,
      "step": 2812
    },
    {
      "epoch": 3.51,
      "grad_norm": 0.024580204859375954,
      "learning_rate": 0.008293799417394923,
      "loss": 0.3381,
      "step": 2813
    },
    {
      "epoch": 3.51,
      "grad_norm": 0.023478113114833832,
      "learning_rate": 0.008289637952559301,
      "loss": 0.1521,
      "step": 2814
    },
    {
      "epoch": 3.51,
      "grad_norm": 0.009513959288597107,
      "learning_rate": 0.008285476487723679,
      "loss": 0.0236,
      "step": 2815
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.034543413668870926,
      "learning_rate": 0.008281315022888057,
      "loss": 0.2056,
      "step": 2816
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.01857166737318039,
      "learning_rate": 0.008277153558052435,
      "loss": 0.1086,
      "step": 2817
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.00016589948791079223,
      "learning_rate": 0.008272992093216813,
      "loss": 0.0002,
      "step": 2818
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.008429229259490967,
      "learning_rate": 0.00826883062838119,
      "loss": 0.0137,
      "step": 2819
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.03843096271157265,
      "learning_rate": 0.008264669163545568,
      "loss": 0.201,
      "step": 2820
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.019074315205216408,
      "learning_rate": 0.008260507698709946,
      "loss": 0.168,
      "step": 2821
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.024920795112848282,
      "learning_rate": 0.008256346233874324,
      "loss": 0.1163,
      "step": 2822
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.023272981867194176,
      "learning_rate": 0.008252184769038702,
      "loss": 0.2815,
      "step": 2823
    },
    {
      "epoch": 3.53,
      "grad_norm": 0.025408755987882614,
      "learning_rate": 0.00824802330420308,
      "loss": 0.1394,
      "step": 2824
    },
    {
      "epoch": 3.53,
      "grad_norm": 0.02214597538113594,
      "learning_rate": 0.008243861839367457,
      "loss": 0.1016,
      "step": 2825
    },
    {
      "epoch": 3.53,
      "grad_norm": 0.02028495818376541,
      "learning_rate": 0.008239700374531835,
      "loss": 0.0779,
      "step": 2826
    },
    {
      "epoch": 3.53,
      "grad_norm": 0.02371542900800705,
      "learning_rate": 0.008235538909696213,
      "loss": 0.2191,
      "step": 2827
    },
    {
      "epoch": 3.53,
      "grad_norm": 0.006602564360946417,
      "learning_rate": 0.00823137744486059,
      "loss": 0.021,
      "step": 2828
    },
    {
      "epoch": 3.53,
      "grad_norm": 0.019826531410217285,
      "learning_rate": 0.008227215980024969,
      "loss": 0.0787,
      "step": 2829
    },
    {
      "epoch": 3.53,
      "grad_norm": 0.01490735448896885,
      "learning_rate": 0.008223054515189347,
      "loss": 0.0588,
      "step": 2830
    },
    {
      "epoch": 3.53,
      "grad_norm": 0.0332588329911232,
      "learning_rate": 0.008218893050353724,
      "loss": 0.1932,
      "step": 2831
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.03228091821074486,
      "learning_rate": 0.008214731585518102,
      "loss": 0.4561,
      "step": 2832
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.026773624122142792,
      "learning_rate": 0.00821057012068248,
      "loss": 0.2612,
      "step": 2833
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.020364491268992424,
      "learning_rate": 0.008206408655846858,
      "loss": 0.0962,
      "step": 2834
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.012533203698694706,
      "learning_rate": 0.008202247191011236,
      "loss": 0.0309,
      "step": 2835
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.012927727773785591,
      "learning_rate": 0.008198085726175613,
      "loss": 0.1121,
      "step": 2836
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.021271774545311928,
      "learning_rate": 0.008193924261339993,
      "loss": 0.1141,
      "step": 2837
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.032415468245744705,
      "learning_rate": 0.00818976279650437,
      "loss": 0.2832,
      "step": 2838
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.025405125692486763,
      "learning_rate": 0.008185601331668747,
      "loss": 0.2085,
      "step": 2839
    },
    {
      "epoch": 3.55,
      "grad_norm": 0.018103789538145065,
      "learning_rate": 0.008181439866833127,
      "loss": 0.1115,
      "step": 2840
    },
    {
      "epoch": 3.55,
      "grad_norm": 0.02968664839863777,
      "learning_rate": 0.008177278401997503,
      "loss": 0.0839,
      "step": 2841
    },
    {
      "epoch": 3.55,
      "grad_norm": 0.016489094123244286,
      "learning_rate": 0.00817311693716188,
      "loss": 0.1891,
      "step": 2842
    },
    {
      "epoch": 3.55,
      "grad_norm": 0.025647612288594246,
      "learning_rate": 0.00816895547232626,
      "loss": 0.1805,
      "step": 2843
    },
    {
      "epoch": 3.55,
      "grad_norm": 0.025426466017961502,
      "learning_rate": 0.008164794007490636,
      "loss": 0.1652,
      "step": 2844
    },
    {
      "epoch": 3.55,
      "grad_norm": 0.02201475203037262,
      "learning_rate": 0.008160632542655014,
      "loss": 0.1302,
      "step": 2845
    },
    {
      "epoch": 3.55,
      "grad_norm": 0.01287840399891138,
      "learning_rate": 0.008156471077819393,
      "loss": 0.0512,
      "step": 2846
    },
    {
      "epoch": 3.55,
      "grad_norm": 0.009715926833450794,
      "learning_rate": 0.00815230961298377,
      "loss": 0.0258,
      "step": 2847
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.015514995902776718,
      "learning_rate": 0.008148148148148147,
      "loss": 0.0919,
      "step": 2848
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.015241079963743687,
      "learning_rate": 0.008143986683312527,
      "loss": 0.2172,
      "step": 2849
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.014279845170676708,
      "learning_rate": 0.008139825218476903,
      "loss": 0.0704,
      "step": 2850
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.015237690880894661,
      "learning_rate": 0.008135663753641281,
      "loss": 0.0793,
      "step": 2851
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.028936734423041344,
      "learning_rate": 0.00813150228880566,
      "loss": 0.2905,
      "step": 2852
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.023737533017992973,
      "learning_rate": 0.008127340823970038,
      "loss": 0.0847,
      "step": 2853
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.0035955675411969423,
      "learning_rate": 0.008123179359134414,
      "loss": 0.0025,
      "step": 2854
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.03607596457004547,
      "learning_rate": 0.008119017894298794,
      "loss": 0.4033,
      "step": 2855
    },
    {
      "epoch": 3.57,
      "grad_norm": 0.02004390023648739,
      "learning_rate": 0.008114856429463172,
      "loss": 0.1132,
      "step": 2856
    },
    {
      "epoch": 3.57,
      "grad_norm": 0.01951761171221733,
      "learning_rate": 0.008110694964627548,
      "loss": 0.1581,
      "step": 2857
    },
    {
      "epoch": 3.57,
      "grad_norm": 0.012490807101130486,
      "learning_rate": 0.008106533499791927,
      "loss": 0.0641,
      "step": 2858
    },
    {
      "epoch": 3.57,
      "grad_norm": 0.029022851958870888,
      "learning_rate": 0.008102372034956305,
      "loss": 0.3318,
      "step": 2859
    },
    {
      "epoch": 3.57,
      "grad_norm": 0.0358114130795002,
      "learning_rate": 0.008098210570120683,
      "loss": 0.3679,
      "step": 2860
    },
    {
      "epoch": 3.57,
      "grad_norm": 0.020903335884213448,
      "learning_rate": 0.008094049105285061,
      "loss": 0.0691,
      "step": 2861
    },
    {
      "epoch": 3.57,
      "grad_norm": 0.019091518595814705,
      "learning_rate": 0.008089887640449439,
      "loss": 0.1089,
      "step": 2862
    },
    {
      "epoch": 3.57,
      "grad_norm": 0.01449755672365427,
      "learning_rate": 0.008085726175613817,
      "loss": 0.0421,
      "step": 2863
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.024066466838121414,
      "learning_rate": 0.008081564710778194,
      "loss": 0.1471,
      "step": 2864
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.02017846144735813,
      "learning_rate": 0.008077403245942572,
      "loss": 0.1136,
      "step": 2865
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.025953268632292747,
      "learning_rate": 0.00807324178110695,
      "loss": 0.2815,
      "step": 2866
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.023057933896780014,
      "learning_rate": 0.008069080316271328,
      "loss": 0.1669,
      "step": 2867
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.04632827267050743,
      "learning_rate": 0.008064918851435706,
      "loss": 0.2556,
      "step": 2868
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.027906233444809914,
      "learning_rate": 0.008060757386600084,
      "loss": 0.2368,
      "step": 2869
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.019089100882411003,
      "learning_rate": 0.008056595921764461,
      "loss": 0.091,
      "step": 2870
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.023303808644413948,
      "learning_rate": 0.00805243445692884,
      "loss": 0.2231,
      "step": 2871
    },
    {
      "epoch": 3.59,
      "grad_norm": 0.024863872677087784,
      "learning_rate": 0.008048272992093217,
      "loss": 0.2091,
      "step": 2872
    },
    {
      "epoch": 3.59,
      "grad_norm": 0.025420060381293297,
      "learning_rate": 0.008044111527257595,
      "loss": 0.1648,
      "step": 2873
    },
    {
      "epoch": 3.59,
      "grad_norm": 0.010958986356854439,
      "learning_rate": 0.008039950062421973,
      "loss": 0.0033,
      "step": 2874
    },
    {
      "epoch": 3.59,
      "grad_norm": 0.01755262352526188,
      "learning_rate": 0.00803578859758635,
      "loss": 0.0793,
      "step": 2875
    },
    {
      "epoch": 3.59,
      "grad_norm": 0.028063878417015076,
      "learning_rate": 0.008031627132750728,
      "loss": 0.1924,
      "step": 2876
    },
    {
      "epoch": 3.59,
      "grad_norm": 0.017500683665275574,
      "learning_rate": 0.008027465667915106,
      "loss": 0.1274,
      "step": 2877
    },
    {
      "epoch": 3.59,
      "grad_norm": 0.018681222572922707,
      "learning_rate": 0.008023304203079484,
      "loss": 0.0958,
      "step": 2878
    },
    {
      "epoch": 3.59,
      "grad_norm": 0.02492377534508705,
      "learning_rate": 0.008019142738243862,
      "loss": 0.2996,
      "step": 2879
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.027441583573818207,
      "learning_rate": 0.00801498127340824,
      "loss": 0.2317,
      "step": 2880
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.01634051837027073,
      "learning_rate": 0.008010819808572617,
      "loss": 0.1064,
      "step": 2881
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.03431393578648567,
      "learning_rate": 0.008006658343736995,
      "loss": 0.1964,
      "step": 2882
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.018541410565376282,
      "learning_rate": 0.008002496878901373,
      "loss": 0.0847,
      "step": 2883
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.015149746090173721,
      "learning_rate": 0.007998335414065751,
      "loss": 0.0976,
      "step": 2884
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.01817130483686924,
      "learning_rate": 0.007994173949230129,
      "loss": 0.1816,
      "step": 2885
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.031041434034705162,
      "learning_rate": 0.007990012484394507,
      "loss": 0.3452,
      "step": 2886
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.023183265700936317,
      "learning_rate": 0.007985851019558884,
      "loss": 0.2091,
      "step": 2887
    },
    {
      "epoch": 3.61,
      "grad_norm": 0.028213437646627426,
      "learning_rate": 0.007981689554723262,
      "loss": 0.1683,
      "step": 2888
    },
    {
      "epoch": 3.61,
      "grad_norm": 0.01660030148923397,
      "learning_rate": 0.00797752808988764,
      "loss": 0.1001,
      "step": 2889
    },
    {
      "epoch": 3.61,
      "grad_norm": 0.016511719673871994,
      "learning_rate": 0.007973366625052018,
      "loss": 0.1028,
      "step": 2890
    },
    {
      "epoch": 3.61,
      "grad_norm": 0.016492491587996483,
      "learning_rate": 0.007969205160216396,
      "loss": 0.059,
      "step": 2891
    },
    {
      "epoch": 3.61,
      "grad_norm": 0.025955669581890106,
      "learning_rate": 0.007965043695380774,
      "loss": 0.2136,
      "step": 2892
    },
    {
      "epoch": 3.61,
      "grad_norm": 0.014061951078474522,
      "learning_rate": 0.007960882230545151,
      "loss": 0.0352,
      "step": 2893
    },
    {
      "epoch": 3.61,
      "grad_norm": 0.011816045269370079,
      "learning_rate": 0.00795672076570953,
      "loss": 0.027,
      "step": 2894
    },
    {
      "epoch": 3.61,
      "grad_norm": 0.018381977453827858,
      "learning_rate": 0.007952559300873909,
      "loss": 0.0786,
      "step": 2895
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.033497877418994904,
      "learning_rate": 0.007948397836038287,
      "loss": 0.2043,
      "step": 2896
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.028511477634310722,
      "learning_rate": 0.007944236371202663,
      "loss": 0.2161,
      "step": 2897
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.022279182448983192,
      "learning_rate": 0.007940074906367042,
      "loss": 0.1348,
      "step": 2898
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.0245820339769125,
      "learning_rate": 0.00793591344153142,
      "loss": 0.1904,
      "step": 2899
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.025555038824677467,
      "learning_rate": 0.007931751976695796,
      "loss": 0.1876,
      "step": 2900
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.02030557207763195,
      "learning_rate": 0.007927590511860176,
      "loss": 0.1263,
      "step": 2901
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.02302958443760872,
      "learning_rate": 0.007923429047024554,
      "loss": 0.1714,
      "step": 2902
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.00912085734307766,
      "learning_rate": 0.00791926758218893,
      "loss": 0.014,
      "step": 2903
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.029544631019234657,
      "learning_rate": 0.00791510611735331,
      "loss": 0.0526,
      "step": 2904
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.03651392087340355,
      "learning_rate": 0.007910944652517687,
      "loss": 0.2181,
      "step": 2905
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.03046834096312523,
      "learning_rate": 0.007906783187682063,
      "loss": 0.1429,
      "step": 2906
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.025629596784710884,
      "learning_rate": 0.007902621722846443,
      "loss": 0.26,
      "step": 2907
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.0006943024345673621,
      "learning_rate": 0.00789846025801082,
      "loss": 0.0005,
      "step": 2908
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.013275512494146824,
      "learning_rate": 0.007894298793175197,
      "loss": 0.0617,
      "step": 2909
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.033038534224033356,
      "learning_rate": 0.007890137328339576,
      "loss": 0.2273,
      "step": 2910
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.033113837242126465,
      "learning_rate": 0.007885975863503954,
      "loss": 0.2267,
      "step": 2911
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.014370814897119999,
      "learning_rate": 0.007881814398668332,
      "loss": 0.0732,
      "step": 2912
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.029394587501883507,
      "learning_rate": 0.00787765293383271,
      "loss": 0.034,
      "step": 2913
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.02006080374121666,
      "learning_rate": 0.007873491468997088,
      "loss": 0.1353,
      "step": 2914
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.025117773562669754,
      "learning_rate": 0.007869330004161465,
      "loss": 0.1582,
      "step": 2915
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.016948048025369644,
      "learning_rate": 0.007865168539325843,
      "loss": 0.0676,
      "step": 2916
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.033386897295713425,
      "learning_rate": 0.007861007074490221,
      "loss": 0.291,
      "step": 2917
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.026908012107014656,
      "learning_rate": 0.007856845609654599,
      "loss": 0.2566,
      "step": 2918
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.013917958363890648,
      "learning_rate": 0.007852684144818977,
      "loss": 0.0826,
      "step": 2919
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.00030268114642240107,
      "learning_rate": 0.007848522679983354,
      "loss": 0.0003,
      "step": 2920
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.01861095055937767,
      "learning_rate": 0.007844361215147732,
      "loss": 0.0806,
      "step": 2921
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.030343975871801376,
      "learning_rate": 0.00784019975031211,
      "loss": 0.0534,
      "step": 2922
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.02371583878993988,
      "learning_rate": 0.007836038285476488,
      "loss": 0.2222,
      "step": 2923
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.013391751796007156,
      "learning_rate": 0.007831876820640866,
      "loss": 0.1032,
      "step": 2924
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.023683005943894386,
      "learning_rate": 0.007827715355805244,
      "loss": 0.1208,
      "step": 2925
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.02255955897271633,
      "learning_rate": 0.007823553890969621,
      "loss": 0.1737,
      "step": 2926
    },
    {
      "epoch": 3.65,
      "grad_norm": 0.03581146150827408,
      "learning_rate": 0.007819392426134,
      "loss": 0.2815,
      "step": 2927
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.021406501531600952,
      "learning_rate": 0.007815230961298377,
      "loss": 0.1715,
      "step": 2928
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.02177511714398861,
      "learning_rate": 0.007811069496462755,
      "loss": 0.2352,
      "step": 2929
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.022804584354162216,
      "learning_rate": 0.007806908031627133,
      "loss": 0.2192,
      "step": 2930
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.028632214292883873,
      "learning_rate": 0.007802746566791511,
      "loss": 0.26,
      "step": 2931
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.017460091039538383,
      "learning_rate": 0.007798585101955888,
      "loss": 0.0541,
      "step": 2932
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.02973257005214691,
      "learning_rate": 0.007794423637120267,
      "loss": 0.2976,
      "step": 2933
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.026219947263598442,
      "learning_rate": 0.007790262172284645,
      "loss": 0.2258,
      "step": 2934
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.02747885324060917,
      "learning_rate": 0.007786100707449022,
      "loss": 0.1566,
      "step": 2935
    },
    {
      "epoch": 3.67,
      "grad_norm": 0.027061009779572487,
      "learning_rate": 0.007781939242613401,
      "loss": 0.2321,
      "step": 2936
    },
    {
      "epoch": 3.67,
      "grad_norm": 0.01936732418835163,
      "learning_rate": 0.007777777777777778,
      "loss": 0.103,
      "step": 2937
    },
    {
      "epoch": 3.67,
      "grad_norm": 0.017103061079978943,
      "learning_rate": 0.007773616312942155,
      "loss": 0.0334,
      "step": 2938
    },
    {
      "epoch": 3.67,
      "grad_norm": 0.015116296708583832,
      "learning_rate": 0.007769454848106534,
      "loss": 0.0451,
      "step": 2939
    },
    {
      "epoch": 3.67,
      "grad_norm": 0.025622528046369553,
      "learning_rate": 0.007765293383270912,
      "loss": 0.1823,
      "step": 2940
    },
    {
      "epoch": 3.67,
      "grad_norm": 0.01664191111922264,
      "learning_rate": 0.007761131918435289,
      "loss": 0.0518,
      "step": 2941
    },
    {
      "epoch": 3.67,
      "grad_norm": 0.029351502656936646,
      "learning_rate": 0.0077569704535996676,
      "loss": 0.4016,
      "step": 2942
    },
    {
      "epoch": 3.67,
      "grad_norm": 0.007682634983211756,
      "learning_rate": 0.007752808988764045,
      "loss": 0.0273,
      "step": 2943
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.03153334558010101,
      "learning_rate": 0.007748647523928422,
      "loss": 0.1512,
      "step": 2944
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.037669915705919266,
      "learning_rate": 0.007744486059092801,
      "loss": 0.2499,
      "step": 2945
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.019807269796729088,
      "learning_rate": 0.007740324594257179,
      "loss": 0.1349,
      "step": 2946
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.017468655481934547,
      "learning_rate": 0.007736163129421556,
      "loss": 0.1896,
      "step": 2947
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.019637873396277428,
      "learning_rate": 0.0077320016645859345,
      "loss": 0.1392,
      "step": 2948
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.006968246307224035,
      "learning_rate": 0.007727840199750312,
      "loss": 0.0138,
      "step": 2949
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.02038305066525936,
      "learning_rate": 0.00772367873491469,
      "loss": 0.1245,
      "step": 2950
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.019875384867191315,
      "learning_rate": 0.007719517270079068,
      "loss": 0.0384,
      "step": 2951
    },
    {
      "epoch": 3.69,
      "grad_norm": 0.023372404277324677,
      "learning_rate": 0.007715355805243446,
      "loss": 0.1886,
      "step": 2952
    },
    {
      "epoch": 3.69,
      "grad_norm": 0.02681809850037098,
      "learning_rate": 0.007711194340407824,
      "loss": 0.3535,
      "step": 2953
    },
    {
      "epoch": 3.69,
      "grad_norm": 0.014951186254620552,
      "learning_rate": 0.0077070328755722015,
      "loss": 0.0701,
      "step": 2954
    },
    {
      "epoch": 3.69,
      "grad_norm": 0.02695605158805847,
      "learning_rate": 0.007702871410736579,
      "loss": 0.2754,
      "step": 2955
    },
    {
      "epoch": 3.69,
      "grad_norm": 0.026725294068455696,
      "learning_rate": 0.007698709945900957,
      "loss": 0.197,
      "step": 2956
    },
    {
      "epoch": 3.69,
      "grad_norm": 0.03142290189862251,
      "learning_rate": 0.007694548481065336,
      "loss": 0.1603,
      "step": 2957
    },
    {
      "epoch": 3.69,
      "grad_norm": 0.01512101385742426,
      "learning_rate": 0.007690387016229713,
      "loss": 0.0354,
      "step": 2958
    },
    {
      "epoch": 3.69,
      "grad_norm": 0.015585810877382755,
      "learning_rate": 0.007686225551394091,
      "loss": 0.0938,
      "step": 2959
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.023874683305621147,
      "learning_rate": 0.007682064086558469,
      "loss": 0.1256,
      "step": 2960
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.0201125405728817,
      "learning_rate": 0.007677902621722846,
      "loss": 0.0999,
      "step": 2961
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.009769189171493053,
      "learning_rate": 0.007673741156887224,
      "loss": 0.0179,
      "step": 2962
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.026588398963212967,
      "learning_rate": 0.007669579692051603,
      "loss": 0.1229,
      "step": 2963
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.02094404585659504,
      "learning_rate": 0.00766541822721598,
      "loss": 0.1231,
      "step": 2964
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.016776129603385925,
      "learning_rate": 0.007661256762380358,
      "loss": 0.2255,
      "step": 2965
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.015455693006515503,
      "learning_rate": 0.007657095297544736,
      "loss": 0.0887,
      "step": 2966
    },
    {
      "epoch": 3.7,
      "grad_norm": 0.020182836800813675,
      "learning_rate": 0.007652933832709113,
      "loss": 0.4348,
      "step": 2967
    },
    {
      "epoch": 3.71,
      "grad_norm": 0.01211516559123993,
      "learning_rate": 0.007648772367873492,
      "loss": 0.03,
      "step": 2968
    },
    {
      "epoch": 3.71,
      "grad_norm": 0.01977180875837803,
      "learning_rate": 0.00764461090303787,
      "loss": 0.081,
      "step": 2969
    },
    {
      "epoch": 3.71,
      "grad_norm": 0.01216968335211277,
      "learning_rate": 0.007640449438202247,
      "loss": 0.037,
      "step": 2970
    },
    {
      "epoch": 3.71,
      "grad_norm": 0.008068501017987728,
      "learning_rate": 0.007636287973366625,
      "loss": 0.0179,
      "step": 2971
    },
    {
      "epoch": 3.71,
      "grad_norm": 0.021039804443717003,
      "learning_rate": 0.007632126508531003,
      "loss": 0.1398,
      "step": 2972
    },
    {
      "epoch": 3.71,
      "grad_norm": 0.029314367100596428,
      "learning_rate": 0.007627965043695381,
      "loss": 0.2776,
      "step": 2973
    },
    {
      "epoch": 3.71,
      "grad_norm": 0.017479125410318375,
      "learning_rate": 0.007623803578859759,
      "loss": 0.1154,
      "step": 2974
    },
    {
      "epoch": 3.71,
      "grad_norm": 0.01827158033847809,
      "learning_rate": 0.007619642114024137,
      "loss": 0.0933,
      "step": 2975
    },
    {
      "epoch": 3.72,
      "grad_norm": 0.033552754670381546,
      "learning_rate": 0.007615480649188515,
      "loss": 0.2115,
      "step": 2976
    },
    {
      "epoch": 3.72,
      "grad_norm": 0.0347176231443882,
      "learning_rate": 0.007611319184352893,
      "loss": 0.5308,
      "step": 2977
    },
    {
      "epoch": 3.72,
      "grad_norm": 0.024414630606770515,
      "learning_rate": 0.00760715771951727,
      "loss": 0.1755,
      "step": 2978
    },
    {
      "epoch": 3.72,
      "grad_norm": 0.019000090658664703,
      "learning_rate": 0.007602996254681648,
      "loss": 0.0998,
      "step": 2979
    },
    {
      "epoch": 3.72,
      "grad_norm": 0.021677756682038307,
      "learning_rate": 0.007598834789846027,
      "loss": 0.1442,
      "step": 2980
    },
    {
      "epoch": 3.72,
      "grad_norm": 0.02147234044969082,
      "learning_rate": 0.007594673325010404,
      "loss": 0.1407,
      "step": 2981
    },
    {
      "epoch": 3.72,
      "grad_norm": 0.023255042731761932,
      "learning_rate": 0.0075905118601747815,
      "loss": 0.118,
      "step": 2982
    },
    {
      "epoch": 3.72,
      "grad_norm": 0.0236919317394495,
      "learning_rate": 0.00758635039533916,
      "loss": 0.1122,
      "step": 2983
    },
    {
      "epoch": 3.73,
      "grad_norm": 0.03611147031188011,
      "learning_rate": 0.007582188930503537,
      "loss": 0.3738,
      "step": 2984
    },
    {
      "epoch": 3.73,
      "grad_norm": 0.018755530938506126,
      "learning_rate": 0.007578027465667915,
      "loss": 0.2822,
      "step": 2985
    },
    {
      "epoch": 3.73,
      "grad_norm": 0.01906651258468628,
      "learning_rate": 0.007573866000832294,
      "loss": 0.1458,
      "step": 2986
    },
    {
      "epoch": 3.73,
      "grad_norm": 0.02651076950132847,
      "learning_rate": 0.007569704535996671,
      "loss": 0.2179,
      "step": 2987
    },
    {
      "epoch": 3.73,
      "grad_norm": 0.020992396399378777,
      "learning_rate": 0.0075655430711610485,
      "loss": 0.1707,
      "step": 2988
    },
    {
      "epoch": 3.73,
      "grad_norm": 0.029139535501599312,
      "learning_rate": 0.007561381606325427,
      "loss": 0.2156,
      "step": 2989
    },
    {
      "epoch": 3.73,
      "grad_norm": 0.017534378916025162,
      "learning_rate": 0.007557220141489804,
      "loss": 0.1516,
      "step": 2990
    },
    {
      "epoch": 3.73,
      "grad_norm": 0.02550153061747551,
      "learning_rate": 0.007553058676654182,
      "loss": 0.1779,
      "step": 2991
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.02612401358783245,
      "learning_rate": 0.007548897211818561,
      "loss": 0.2517,
      "step": 2992
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.023735718801617622,
      "learning_rate": 0.0075447357469829385,
      "loss": 0.2651,
      "step": 2993
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.02023289166390896,
      "learning_rate": 0.0075405742821473155,
      "loss": 0.0883,
      "step": 2994
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.02286474034190178,
      "learning_rate": 0.007536412817311694,
      "loss": 0.119,
      "step": 2995
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.022064751014113426,
      "learning_rate": 0.007532251352476072,
      "loss": 0.2444,
      "step": 2996
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.00025400446611456573,
      "learning_rate": 0.007528089887640449,
      "loss": 0.0003,
      "step": 2997
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.016671020537614822,
      "learning_rate": 0.007523928422804828,
      "loss": 0.1461,
      "step": 2998
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.018711725249886513,
      "learning_rate": 0.0075197669579692055,
      "loss": 0.1328,
      "step": 2999
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.025530120357871056,
      "learning_rate": 0.0075156054931335824,
      "loss": 0.2722,
      "step": 3000
    },
    {
      "epoch": 3.75,
      "eval_loss": 0.2462158203125,
      "eval_runtime": 182.8866,
      "eval_samples_per_second": 1.099,
      "eval_steps_per_second": 0.552,
      "step": 3000
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.020947571843862534,
      "learning_rate": 0.007511444028297961,
      "loss": 0.1248,
      "step": 3001
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.01890726014971733,
      "learning_rate": 0.007507282563462339,
      "loss": 0.1523,
      "step": 3002
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.022786658257246017,
      "learning_rate": 0.007503121098626716,
      "loss": 0.1268,
      "step": 3003
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.0583333857357502,
      "learning_rate": 0.007498959633791095,
      "loss": 0.7959,
      "step": 3004
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.016722653061151505,
      "learning_rate": 0.0074947981689554724,
      "loss": 0.1008,
      "step": 3005
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.011340693570673466,
      "learning_rate": 0.007490636704119851,
      "loss": 0.0293,
      "step": 3006
    },
    {
      "epoch": 3.75,
      "grad_norm": 0.021065188571810722,
      "learning_rate": 0.007486475239284228,
      "loss": 0.1074,
      "step": 3007
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.028872458264231682,
      "learning_rate": 0.007482313774448606,
      "loss": 0.3081,
      "step": 3008
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.02161755971610546,
      "learning_rate": 0.007478152309612985,
      "loss": 0.0767,
      "step": 3009
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.02952500991523266,
      "learning_rate": 0.007473990844777362,
      "loss": 0.3723,
      "step": 3010
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.01738063246011734,
      "learning_rate": 0.007469829379941739,
      "loss": 0.064,
      "step": 3011
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.015582648105919361,
      "learning_rate": 0.007465667915106118,
      "loss": 0.0596,
      "step": 3012
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.021328188478946686,
      "learning_rate": 0.007461506450270495,
      "loss": 0.1121,
      "step": 3013
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.011751336045563221,
      "learning_rate": 0.007457344985434873,
      "loss": 0.0476,
      "step": 3014
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.02685796283185482,
      "learning_rate": 0.007453183520599252,
      "loss": 0.1816,
      "step": 3015
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.026793790981173515,
      "learning_rate": 0.007449022055763629,
      "loss": 0.2764,
      "step": 3016
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.028668126091361046,
      "learning_rate": 0.007444860590928006,
      "loss": 0.1007,
      "step": 3017
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.020044362172484398,
      "learning_rate": 0.007440699126092385,
      "loss": 0.0553,
      "step": 3018
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.026326991617679596,
      "learning_rate": 0.007436537661256763,
      "loss": 0.1898,
      "step": 3019
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.02025780640542507,
      "learning_rate": 0.00743237619642114,
      "loss": 0.0784,
      "step": 3020
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.025871340185403824,
      "learning_rate": 0.0074282147315855186,
      "loss": 0.1979,
      "step": 3021
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.023961573839187622,
      "learning_rate": 0.007424053266749896,
      "loss": 0.3052,
      "step": 3022
    },
    {
      "epoch": 3.77,
      "grad_norm": 0.027341295033693314,
      "learning_rate": 0.007419891801914273,
      "loss": 0.2443,
      "step": 3023
    },
    {
      "epoch": 3.78,
      "grad_norm": 0.018532035872340202,
      "learning_rate": 0.007415730337078652,
      "loss": 0.0772,
      "step": 3024
    },
    {
      "epoch": 3.78,
      "grad_norm": 0.0005704626091755927,
      "learning_rate": 0.00741156887224303,
      "loss": 0.0004,
      "step": 3025
    },
    {
      "epoch": 3.78,
      "grad_norm": 0.01297977939248085,
      "learning_rate": 0.007407407407407407,
      "loss": 0.0556,
      "step": 3026
    },
    {
      "epoch": 3.78,
      "grad_norm": 0.002036874881014228,
      "learning_rate": 0.0074032459425717855,
      "loss": 0.0009,
      "step": 3027
    },
    {
      "epoch": 3.78,
      "grad_norm": 0.02013500966131687,
      "learning_rate": 0.007399084477736163,
      "loss": 0.1214,
      "step": 3028
    },
    {
      "epoch": 3.78,
      "grad_norm": 0.022396450862288475,
      "learning_rate": 0.00739492301290054,
      "loss": 0.2158,
      "step": 3029
    },
    {
      "epoch": 3.78,
      "grad_norm": 0.01804165542125702,
      "learning_rate": 0.007390761548064919,
      "loss": 0.0401,
      "step": 3030
    },
    {
      "epoch": 3.78,
      "grad_norm": 0.022892644628882408,
      "learning_rate": 0.007386600083229297,
      "loss": 0.292,
      "step": 3031
    },
    {
      "epoch": 3.79,
      "grad_norm": 0.009431442245841026,
      "learning_rate": 0.007382438618393675,
      "loss": 0.0143,
      "step": 3032
    },
    {
      "epoch": 3.79,
      "grad_norm": 0.02145378477871418,
      "learning_rate": 0.0073782771535580525,
      "loss": 0.0817,
      "step": 3033
    },
    {
      "epoch": 3.79,
      "grad_norm": 0.025713348761200905,
      "learning_rate": 0.00737411568872243,
      "loss": 0.3118,
      "step": 3034
    },
    {
      "epoch": 3.79,
      "grad_norm": 0.02620074898004532,
      "learning_rate": 0.007369954223886808,
      "loss": 0.1189,
      "step": 3035
    },
    {
      "epoch": 3.79,
      "grad_norm": 0.023508712649345398,
      "learning_rate": 0.007365792759051186,
      "loss": 0.1691,
      "step": 3036
    },
    {
      "epoch": 3.79,
      "grad_norm": 0.030360473319888115,
      "learning_rate": 0.007361631294215564,
      "loss": 0.1262,
      "step": 3037
    },
    {
      "epoch": 3.79,
      "grad_norm": 0.027496160939335823,
      "learning_rate": 0.007357469829379942,
      "loss": 0.2114,
      "step": 3038
    },
    {
      "epoch": 3.79,
      "grad_norm": 0.02078140899538994,
      "learning_rate": 0.00735330836454432,
      "loss": 0.1248,
      "step": 3039
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.03530541807413101,
      "learning_rate": 0.007349146899708697,
      "loss": 0.2664,
      "step": 3040
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.020140137523412704,
      "learning_rate": 0.007344985434873076,
      "loss": 0.1497,
      "step": 3041
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.02946803718805313,
      "learning_rate": 0.007340823970037454,
      "loss": 0.1616,
      "step": 3042
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.023613130673766136,
      "learning_rate": 0.007336662505201831,
      "loss": 0.1611,
      "step": 3043
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.022040745243430138,
      "learning_rate": 0.0073325010403662095,
      "loss": 0.2239,
      "step": 3044
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.04490484297275543,
      "learning_rate": 0.007328339575530587,
      "loss": 0.1456,
      "step": 3045
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.027208801358938217,
      "learning_rate": 0.007324178110694964,
      "loss": 0.3367,
      "step": 3046
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.02066088281571865,
      "learning_rate": 0.007320016645859343,
      "loss": 0.176,
      "step": 3047
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.027409188449382782,
      "learning_rate": 0.007315855181023721,
      "loss": 0.2186,
      "step": 3048
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.02638986147940159,
      "learning_rate": 0.007311693716188098,
      "loss": 0.2927,
      "step": 3049
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.018016425892710686,
      "learning_rate": 0.007307532251352476,
      "loss": 0.09,
      "step": 3050
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.01913651078939438,
      "learning_rate": 0.007303370786516854,
      "loss": 0.131,
      "step": 3051
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.015474367886781693,
      "learning_rate": 0.007299209321681231,
      "loss": 0.0883,
      "step": 3052
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.018864665180444717,
      "learning_rate": 0.00729504785684561,
      "loss": 0.1571,
      "step": 3053
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.02444211207330227,
      "learning_rate": 0.007290886392009988,
      "loss": 0.3013,
      "step": 3054
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.023340055719017982,
      "learning_rate": 0.0072867249271743656,
      "loss": 0.1031,
      "step": 3055
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.013364342041313648,
      "learning_rate": 0.007282563462338743,
      "loss": 0.0858,
      "step": 3056
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.024733955040574074,
      "learning_rate": 0.007278401997503121,
      "loss": 0.1766,
      "step": 3057
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.013867567293345928,
      "learning_rate": 0.007274240532667499,
      "loss": 0.0542,
      "step": 3058
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.023446878418326378,
      "learning_rate": 0.007270079067831878,
      "loss": 0.207,
      "step": 3059
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.02850128524005413,
      "learning_rate": 0.007265917602996255,
      "loss": 0.2493,
      "step": 3060
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.028601694852113724,
      "learning_rate": 0.0072617561381606325,
      "loss": 0.188,
      "step": 3061
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.03098677657544613,
      "learning_rate": 0.007257594673325011,
      "loss": 0.4084,
      "step": 3062
    },
    {
      "epoch": 3.82,
      "grad_norm": 0.03200793266296387,
      "learning_rate": 0.007253433208489388,
      "loss": 0.1763,
      "step": 3063
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.03167019784450531,
      "learning_rate": 0.007249271743653766,
      "loss": 0.0177,
      "step": 3064
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.005703413859009743,
      "learning_rate": 0.007245110278818145,
      "loss": 0.0085,
      "step": 3065
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.02202814258635044,
      "learning_rate": 0.007240948813982522,
      "loss": 0.1158,
      "step": 3066
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.016733180731534958,
      "learning_rate": 0.0072367873491468995,
      "loss": 0.0786,
      "step": 3067
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.00810986664146185,
      "learning_rate": 0.007232625884311278,
      "loss": 0.0271,
      "step": 3068
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.00518048694357276,
      "learning_rate": 0.007228464419475655,
      "loss": 0.0123,
      "step": 3069
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.024778764694929123,
      "learning_rate": 0.007224302954640033,
      "loss": 0.193,
      "step": 3070
    },
    {
      "epoch": 3.83,
      "grad_norm": 0.0196138396859169,
      "learning_rate": 0.007220141489804412,
      "loss": 0.1331,
      "step": 3071
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.023671511560678482,
      "learning_rate": 0.007215980024968789,
      "loss": 0.187,
      "step": 3072
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.029034825041890144,
      "learning_rate": 0.0072118185601331665,
      "loss": 0.3882,
      "step": 3073
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.030943110585212708,
      "learning_rate": 0.007207657095297545,
      "loss": 0.2549,
      "step": 3074
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.03306640684604645,
      "learning_rate": 0.007203495630461923,
      "loss": 0.5493,
      "step": 3075
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.026126757264137268,
      "learning_rate": 0.0071993341656263,
      "loss": 0.2751,
      "step": 3076
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.008976618759334087,
      "learning_rate": 0.007195172700790679,
      "loss": 0.0259,
      "step": 3077
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.021177126094698906,
      "learning_rate": 0.0071910112359550565,
      "loss": 0.1186,
      "step": 3078
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.01952754519879818,
      "learning_rate": 0.007186849771119434,
      "loss": 0.1576,
      "step": 3079
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.020707396790385246,
      "learning_rate": 0.007182688306283812,
      "loss": 0.1112,
      "step": 3080
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.02493567205965519,
      "learning_rate": 0.00717852684144819,
      "loss": 0.1532,
      "step": 3081
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.024297943338751793,
      "learning_rate": 0.007174365376612569,
      "loss": 0.2434,
      "step": 3082
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.020542874932289124,
      "learning_rate": 0.007170203911776946,
      "loss": 0.1375,
      "step": 3083
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.021203404292464256,
      "learning_rate": 0.007166042446941323,
      "loss": 0.2478,
      "step": 3084
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.024955149739980698,
      "learning_rate": 0.007161880982105702,
      "loss": 0.2058,
      "step": 3085
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.02965738996863365,
      "learning_rate": 0.007157719517270079,
      "loss": 0.4863,
      "step": 3086
    },
    {
      "epoch": 3.85,
      "grad_norm": 0.020452015101909637,
      "learning_rate": 0.007153558052434457,
      "loss": 0.2,
      "step": 3087
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.0027284652460366488,
      "learning_rate": 0.007149396587598836,
      "loss": 0.0037,
      "step": 3088
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.04053844138979912,
      "learning_rate": 0.007145235122763213,
      "loss": 0.4353,
      "step": 3089
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.0267835333943367,
      "learning_rate": 0.00714107365792759,
      "loss": 0.2319,
      "step": 3090
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.015188666060566902,
      "learning_rate": 0.007136912193091969,
      "loss": 0.0703,
      "step": 3091
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.025898592546582222,
      "learning_rate": 0.007132750728256346,
      "loss": 0.2754,
      "step": 3092
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.014127141796052456,
      "learning_rate": 0.007128589263420724,
      "loss": 0.0592,
      "step": 3093
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.02833077311515808,
      "learning_rate": 0.007124427798585103,
      "loss": 0.1788,
      "step": 3094
    },
    {
      "epoch": 3.86,
      "grad_norm": 0.033005241304636,
      "learning_rate": 0.0071202663337494795,
      "loss": 0.2078,
      "step": 3095
    },
    {
      "epoch": 3.87,
      "grad_norm": 0.02315322682261467,
      "learning_rate": 0.007116104868913857,
      "loss": 0.153,
      "step": 3096
    },
    {
      "epoch": 3.87,
      "grad_norm": 0.03724099323153496,
      "learning_rate": 0.007111943404078236,
      "loss": 0.1346,
      "step": 3097
    },
    {
      "epoch": 3.87,
      "grad_norm": 0.03497486561536789,
      "learning_rate": 0.007107781939242614,
      "loss": 0.3567,
      "step": 3098
    },
    {
      "epoch": 3.87,
      "grad_norm": 0.037775419652462006,
      "learning_rate": 0.007103620474406991,
      "loss": 0.3452,
      "step": 3099
    },
    {
      "epoch": 3.87,
      "grad_norm": 0.03612029552459717,
      "learning_rate": 0.0070994590095713695,
      "loss": 0.3447,
      "step": 3100
    },
    {
      "epoch": 3.87,
      "grad_norm": 0.022234981879591942,
      "learning_rate": 0.007095297544735747,
      "loss": 0.1071,
      "step": 3101
    },
    {
      "epoch": 3.87,
      "grad_norm": 0.01839832402765751,
      "learning_rate": 0.007091136079900124,
      "loss": 0.0451,
      "step": 3102
    },
    {
      "epoch": 3.87,
      "grad_norm": 0.01858435571193695,
      "learning_rate": 0.007086974615064503,
      "loss": 0.1479,
      "step": 3103
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.010384575463831425,
      "learning_rate": 0.007082813150228881,
      "loss": 0.0309,
      "step": 3104
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.02845243737101555,
      "learning_rate": 0.007078651685393258,
      "loss": 0.3721,
      "step": 3105
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.019550025463104248,
      "learning_rate": 0.0070744902205576365,
      "loss": 0.106,
      "step": 3106
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.018621359020471573,
      "learning_rate": 0.007070328755722014,
      "loss": 0.1614,
      "step": 3107
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.021772457286715508,
      "learning_rate": 0.007066167290886391,
      "loss": 0.1377,
      "step": 3108
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.026966698467731476,
      "learning_rate": 0.00706200582605077,
      "loss": 0.1309,
      "step": 3109
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.013428350910544395,
      "learning_rate": 0.007057844361215148,
      "loss": 0.0213,
      "step": 3110
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.03283189237117767,
      "learning_rate": 0.007053682896379525,
      "loss": 0.3965,
      "step": 3111
    },
    {
      "epoch": 3.89,
      "grad_norm": 0.024382373318076134,
      "learning_rate": 0.0070495214315439035,
      "loss": 0.3047,
      "step": 3112
    },
    {
      "epoch": 3.89,
      "grad_norm": 0.00023218453861773014,
      "learning_rate": 0.007045359966708281,
      "loss": 0.0003,
      "step": 3113
    },
    {
      "epoch": 3.89,
      "grad_norm": 0.009318538010120392,
      "learning_rate": 0.00704119850187266,
      "loss": 0.022,
      "step": 3114
    },
    {
      "epoch": 3.89,
      "grad_norm": 0.022718679159879684,
      "learning_rate": 0.007037037037037037,
      "loss": 0.25,
      "step": 3115
    },
    {
      "epoch": 3.89,
      "grad_norm": 0.02347397617995739,
      "learning_rate": 0.007032875572201415,
      "loss": 0.281,
      "step": 3116
    },
    {
      "epoch": 3.89,
      "grad_norm": 0.018910013139247894,
      "learning_rate": 0.0070287141073657935,
      "loss": 0.1227,
      "step": 3117
    },
    {
      "epoch": 3.89,
      "grad_norm": 0.022751038894057274,
      "learning_rate": 0.007024552642530171,
      "loss": 0.3569,
      "step": 3118
    },
    {
      "epoch": 3.89,
      "grad_norm": 0.026040365919470787,
      "learning_rate": 0.007020391177694548,
      "loss": 0.1857,
      "step": 3119
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.023367663845419884,
      "learning_rate": 0.007016229712858927,
      "loss": 0.2141,
      "step": 3120
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.014198984019458294,
      "learning_rate": 0.007012068248023305,
      "loss": 0.0548,
      "step": 3121
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.021048517897725105,
      "learning_rate": 0.007007906783187682,
      "loss": 0.156,
      "step": 3122
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.008651030249893665,
      "learning_rate": 0.0070037453183520604,
      "loss": 0.0239,
      "step": 3123
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.01700453832745552,
      "learning_rate": 0.006999583853516438,
      "loss": 0.1365,
      "step": 3124
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.024061331525444984,
      "learning_rate": 0.006995422388680815,
      "loss": 0.0764,
      "step": 3125
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.029899390414357185,
      "learning_rate": 0.006991260923845194,
      "loss": 0.3333,
      "step": 3126
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.023408524692058563,
      "learning_rate": 0.006987099459009572,
      "loss": 0.1888,
      "step": 3127
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.019571298733353615,
      "learning_rate": 0.006982937994173949,
      "loss": 0.1381,
      "step": 3128
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.009337930008769035,
      "learning_rate": 0.006978776529338327,
      "loss": 0.0198,
      "step": 3129
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.011069224216043949,
      "learning_rate": 0.006974615064502705,
      "loss": 0.041,
      "step": 3130
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.017318015918135643,
      "learning_rate": 0.006970453599667082,
      "loss": 0.1295,
      "step": 3131
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.011772510595619678,
      "learning_rate": 0.006966292134831461,
      "loss": 0.0381,
      "step": 3132
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.035798706114292145,
      "learning_rate": 0.006962130669995839,
      "loss": 0.1714,
      "step": 3133
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.019874753430485725,
      "learning_rate": 0.0069579692051602166,
      "loss": 0.0897,
      "step": 3134
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.03264801576733589,
      "learning_rate": 0.006953807740324594,
      "loss": 0.4133,
      "step": 3135
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.021489474922418594,
      "learning_rate": 0.006949646275488972,
      "loss": 0.2581,
      "step": 3136
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.010918905958533287,
      "learning_rate": 0.00694548481065335,
      "loss": 0.0371,
      "step": 3137
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.025630051270127296,
      "learning_rate": 0.006941323345817728,
      "loss": 0.27,
      "step": 3138
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.015494813211262226,
      "learning_rate": 0.006937161880982106,
      "loss": 0.095,
      "step": 3139
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.0053593493066728115,
      "learning_rate": 0.0069330004161464835,
      "loss": 0.0131,
      "step": 3140
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.031508900225162506,
      "learning_rate": 0.006928838951310862,
      "loss": 0.2673,
      "step": 3141
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.030546873807907104,
      "learning_rate": 0.006924677486475239,
      "loss": 0.2791,
      "step": 3142
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.029330436140298843,
      "learning_rate": 0.006920516021639617,
      "loss": 0.3242,
      "step": 3143
    },
    {
      "epoch": 3.93,
      "grad_norm": 0.020741965621709824,
      "learning_rate": 0.006916354556803996,
      "loss": 0.1697,
      "step": 3144
    },
    {
      "epoch": 3.93,
      "grad_norm": 0.016712144017219543,
      "learning_rate": 0.006912193091968373,
      "loss": 0.1415,
      "step": 3145
    },
    {
      "epoch": 3.93,
      "grad_norm": 0.030862610787153244,
      "learning_rate": 0.0069080316271327505,
      "loss": 0.3496,
      "step": 3146
    },
    {
      "epoch": 3.93,
      "grad_norm": 0.030430609360337257,
      "learning_rate": 0.006903870162297129,
      "loss": 0.3838,
      "step": 3147
    },
    {
      "epoch": 3.93,
      "grad_norm": 0.031187260523438454,
      "learning_rate": 0.006899708697461506,
      "loss": 0.1478,
      "step": 3148
    },
    {
      "epoch": 3.93,
      "grad_norm": 0.017005177214741707,
      "learning_rate": 0.006895547232625884,
      "loss": 0.0991,
      "step": 3149
    },
    {
      "epoch": 3.93,
      "grad_norm": 0.039256274700164795,
      "learning_rate": 0.006891385767790263,
      "loss": 0.6948,
      "step": 3150
    },
    {
      "epoch": 3.93,
      "grad_norm": 0.025417815893888474,
      "learning_rate": 0.00688722430295464,
      "loss": 0.0659,
      "step": 3151
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.0008107554749585688,
      "learning_rate": 0.006883062838119018,
      "loss": 0.0005,
      "step": 3152
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.01786183752119541,
      "learning_rate": 0.006878901373283396,
      "loss": 0.0909,
      "step": 3153
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.012951834127306938,
      "learning_rate": 0.006874739908447773,
      "loss": 0.0345,
      "step": 3154
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.014469447545707226,
      "learning_rate": 0.006870578443612152,
      "loss": 0.0888,
      "step": 3155
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.029258428141474724,
      "learning_rate": 0.00686641697877653,
      "loss": 0.2974,
      "step": 3156
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.020744778215885162,
      "learning_rate": 0.0068622555139409075,
      "loss": 0.1329,
      "step": 3157
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.04466787725687027,
      "learning_rate": 0.006858094049105285,
      "loss": 0.3997,
      "step": 3158
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.023046191781759262,
      "learning_rate": 0.006853932584269663,
      "loss": 0.2573,
      "step": 3159
    },
    {
      "epoch": 3.95,
      "grad_norm": 0.011420192196965218,
      "learning_rate": 0.006849771119434041,
      "loss": 0.0412,
      "step": 3160
    },
    {
      "epoch": 3.95,
      "grad_norm": 0.020093217492103577,
      "learning_rate": 0.00684560965459842,
      "loss": 0.1344,
      "step": 3161
    },
    {
      "epoch": 3.95,
      "grad_norm": 0.025633225217461586,
      "learning_rate": 0.006841448189762797,
      "loss": 0.2013,
      "step": 3162
    },
    {
      "epoch": 3.95,
      "grad_norm": 0.023218385875225067,
      "learning_rate": 0.006837286724927174,
      "loss": 0.2031,
      "step": 3163
    },
    {
      "epoch": 3.95,
      "grad_norm": 0.03187666833400726,
      "learning_rate": 0.006833125260091553,
      "loss": 0.2036,
      "step": 3164
    },
    {
      "epoch": 3.95,
      "grad_norm": 0.03729914128780365,
      "learning_rate": 0.00682896379525593,
      "loss": 0.2316,
      "step": 3165
    },
    {
      "epoch": 3.95,
      "grad_norm": 0.03990916535258293,
      "learning_rate": 0.006824802330420308,
      "loss": 0.3616,
      "step": 3166
    },
    {
      "epoch": 3.95,
      "grad_norm": 0.01792808435857296,
      "learning_rate": 0.006820640865584687,
      "loss": 0.1522,
      "step": 3167
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.021452004089951515,
      "learning_rate": 0.0068164794007490636,
      "loss": 0.1918,
      "step": 3168
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.02787955477833748,
      "learning_rate": 0.006812317935913441,
      "loss": 0.3428,
      "step": 3169
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.02352243661880493,
      "learning_rate": 0.00680815647107782,
      "loss": 0.2502,
      "step": 3170
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.014626340940594673,
      "learning_rate": 0.006803995006242197,
      "loss": 0.0467,
      "step": 3171
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.007195156998932362,
      "learning_rate": 0.006799833541406575,
      "loss": 0.0138,
      "step": 3172
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.018378831446170807,
      "learning_rate": 0.006795672076570954,
      "loss": 0.1642,
      "step": 3173
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.021983632817864418,
      "learning_rate": 0.0067915106117353305,
      "loss": 0.1533,
      "step": 3174
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.025171874091029167,
      "learning_rate": 0.006787349146899708,
      "loss": 0.2421,
      "step": 3175
    },
    {
      "epoch": 3.97,
      "grad_norm": 0.018183685839176178,
      "learning_rate": 0.006783187682064087,
      "loss": 0.1034,
      "step": 3176
    },
    {
      "epoch": 3.97,
      "grad_norm": 0.028709620237350464,
      "learning_rate": 0.006779026217228465,
      "loss": 0.1949,
      "step": 3177
    },
    {
      "epoch": 3.97,
      "grad_norm": 0.008070266805589199,
      "learning_rate": 0.006774864752392842,
      "loss": 0.0164,
      "step": 3178
    },
    {
      "epoch": 3.97,
      "grad_norm": 0.0234394334256649,
      "learning_rate": 0.0067707032875572205,
      "loss": 0.1415,
      "step": 3179
    },
    {
      "epoch": 3.97,
      "grad_norm": 0.037419483065605164,
      "learning_rate": 0.006766541822721598,
      "loss": 0.4517,
      "step": 3180
    },
    {
      "epoch": 3.97,
      "grad_norm": 0.037680525332689285,
      "learning_rate": 0.006762380357885975,
      "loss": 0.2832,
      "step": 3181
    },
    {
      "epoch": 3.97,
      "grad_norm": 0.025812197476625443,
      "learning_rate": 0.006758218893050354,
      "loss": 0.218,
      "step": 3182
    },
    {
      "epoch": 3.97,
      "grad_norm": 0.01977810449898243,
      "learning_rate": 0.006754057428214732,
      "loss": 0.1324,
      "step": 3183
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.019650937989354134,
      "learning_rate": 0.006749895963379109,
      "loss": 0.1404,
      "step": 3184
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.019773542881011963,
      "learning_rate": 0.0067457344985434875,
      "loss": 0.1039,
      "step": 3185
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.017368406057357788,
      "learning_rate": 0.006741573033707865,
      "loss": 0.1566,
      "step": 3186
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.031169690191745758,
      "learning_rate": 0.006737411568872244,
      "loss": 0.248,
      "step": 3187
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.02020709216594696,
      "learning_rate": 0.006733250104036621,
      "loss": 0.1241,
      "step": 3188
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.022567931562662125,
      "learning_rate": 0.006729088639200999,
      "loss": 0.3508,
      "step": 3189
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.034284692257642746,
      "learning_rate": 0.0067249271743653775,
      "loss": 0.2318,
      "step": 3190
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.01628643274307251,
      "learning_rate": 0.0067207657095297545,
      "loss": 0.0276,
      "step": 3191
    },
    {
      "epoch": 3.99,
      "grad_norm": 0.025585055351257324,
      "learning_rate": 0.006716604244694132,
      "loss": 0.222,
      "step": 3192
    },
    {
      "epoch": 3.99,
      "grad_norm": 0.0290556401014328,
      "learning_rate": 0.006712442779858511,
      "loss": 0.1906,
      "step": 3193
    },
    {
      "epoch": 3.99,
      "grad_norm": 0.029463382437825203,
      "learning_rate": 0.006708281315022888,
      "loss": 0.0906,
      "step": 3194
    },
    {
      "epoch": 3.99,
      "grad_norm": 0.016788741573691368,
      "learning_rate": 0.006704119850187266,
      "loss": 0.0856,
      "step": 3195
    },
    {
      "epoch": 3.99,
      "grad_norm": 0.03226935490965843,
      "learning_rate": 0.0066999583853516445,
      "loss": 0.1686,
      "step": 3196
    },
    {
      "epoch": 3.99,
      "grad_norm": 0.02976124733686447,
      "learning_rate": 0.006695796920516021,
      "loss": 0.245,
      "step": 3197
    },
    {
      "epoch": 3.99,
      "grad_norm": 0.019141269847750664,
      "learning_rate": 0.006691635455680399,
      "loss": 0.0643,
      "step": 3198
    },
    {
      "epoch": 3.99,
      "grad_norm": 0.02936636283993721,
      "learning_rate": 0.006687473990844778,
      "loss": 0.2529,
      "step": 3199
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.024093130603432655,
      "learning_rate": 0.006683312526009156,
      "loss": 0.2216,
      "step": 3200
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.024242378771305084,
      "learning_rate": 0.006679151061173533,
      "loss": 0.1311,
      "step": 3201
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.02634851075708866,
      "learning_rate": 0.0066749895963379114,
      "loss": 0.1254,
      "step": 3202
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.013197011314332485,
      "learning_rate": 0.006670828131502289,
      "loss": 0.0787,
      "step": 3203
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.03234105184674263,
      "learning_rate": 0.006666666666666666,
      "loss": 0.2852,
      "step": 3204
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.021941399201750755,
      "learning_rate": 0.006662505201831045,
      "loss": 0.1631,
      "step": 3205
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.02630777470767498,
      "learning_rate": 0.006658343736995423,
      "loss": 0.292,
      "step": 3206
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.017874691635370255,
      "learning_rate": 0.0066541822721598,
      "loss": 0.0767,
      "step": 3207
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.004420131910592318,
      "learning_rate": 0.006650020807324178,
      "loss": 0.0044,
      "step": 3208
    },
    {
      "epoch": 4.01,
      "grad_norm": 0.017325442284345627,
      "learning_rate": 0.006645859342488556,
      "loss": 0.0721,
      "step": 3209
    },
    {
      "epoch": 4.01,
      "grad_norm": 0.006358161102980375,
      "learning_rate": 0.006641697877652933,
      "loss": 0.0152,
      "step": 3210
    },
    {
      "epoch": 4.01,
      "grad_norm": 0.02214854769408703,
      "learning_rate": 0.006637536412817312,
      "loss": 0.1071,
      "step": 3211
    },
    {
      "epoch": 4.01,
      "grad_norm": 0.011640191078186035,
      "learning_rate": 0.00663337494798169,
      "loss": 0.0665,
      "step": 3212
    },
    {
      "epoch": 4.01,
      "grad_norm": 0.024215683341026306,
      "learning_rate": 0.006629213483146067,
      "loss": 0.0923,
      "step": 3213
    },
    {
      "epoch": 4.01,
      "grad_norm": 0.014517012983560562,
      "learning_rate": 0.006625052018310445,
      "loss": 0.0476,
      "step": 3214
    },
    {
      "epoch": 4.01,
      "grad_norm": 0.012282261624932289,
      "learning_rate": 0.006620890553474823,
      "loss": 0.0375,
      "step": 3215
    },
    {
      "epoch": 4.01,
      "grad_norm": 0.024602321907877922,
      "learning_rate": 0.006616729088639201,
      "loss": 0.2013,
      "step": 3216
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.020435694605112076,
      "learning_rate": 0.006612567623803579,
      "loss": 0.0591,
      "step": 3217
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.02081003226339817,
      "learning_rate": 0.006608406158967957,
      "loss": 0.2983,
      "step": 3218
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.02937167137861252,
      "learning_rate": 0.0066042446941323345,
      "loss": 0.4016,
      "step": 3219
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.0130587387830019,
      "learning_rate": 0.006600083229296713,
      "loss": 0.0716,
      "step": 3220
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.027887457981705666,
      "learning_rate": 0.00659592176446109,
      "loss": 0.1722,
      "step": 3221
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.04698575660586357,
      "learning_rate": 0.006591760299625468,
      "loss": 0.1741,
      "step": 3222
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.030061809346079826,
      "learning_rate": 0.006587598834789847,
      "loss": 0.1713,
      "step": 3223
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.021268347278237343,
      "learning_rate": 0.006583437369954224,
      "loss": 0.042,
      "step": 3224
    },
    {
      "epoch": 4.03,
      "grad_norm": 0.021903926506638527,
      "learning_rate": 0.006579275905118602,
      "loss": 0.2343,
      "step": 3225
    },
    {
      "epoch": 4.03,
      "grad_norm": 0.015440460294485092,
      "learning_rate": 0.00657511444028298,
      "loss": 0.0546,
      "step": 3226
    },
    {
      "epoch": 4.03,
      "grad_norm": 0.02290337160229683,
      "learning_rate": 0.006570952975447357,
      "loss": 0.1071,
      "step": 3227
    },
    {
      "epoch": 4.03,
      "grad_norm": 0.02068062126636505,
      "learning_rate": 0.006566791510611736,
      "loss": 0.1592,
      "step": 3228
    },
    {
      "epoch": 4.03,
      "grad_norm": 0.018794743344187737,
      "learning_rate": 0.006562630045776114,
      "loss": 0.1573,
      "step": 3229
    },
    {
      "epoch": 4.03,
      "grad_norm": 0.01853705756366253,
      "learning_rate": 0.006558468580940491,
      "loss": 0.407,
      "step": 3230
    },
    {
      "epoch": 4.03,
      "grad_norm": 0.01895325630903244,
      "learning_rate": 0.006554307116104869,
      "loss": 0.134,
      "step": 3231
    },
    {
      "epoch": 4.03,
      "grad_norm": 0.0005669817910529673,
      "learning_rate": 0.006550145651269247,
      "loss": 0.0004,
      "step": 3232
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.01162019744515419,
      "learning_rate": 0.006545984186433624,
      "loss": 0.0493,
      "step": 3233
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.03185730054974556,
      "learning_rate": 0.006541822721598003,
      "loss": 0.1462,
      "step": 3234
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.019494086503982544,
      "learning_rate": 0.006537661256762381,
      "loss": 0.0623,
      "step": 3235
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.023658940568566322,
      "learning_rate": 0.0065334997919267584,
      "loss": 0.0972,
      "step": 3236
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.014140834100544453,
      "learning_rate": 0.006529338327091136,
      "loss": 0.1262,
      "step": 3237
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.024909451603889465,
      "learning_rate": 0.006525176862255514,
      "loss": 0.1639,
      "step": 3238
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.009545624256134033,
      "learning_rate": 0.006521015397419892,
      "loss": 0.0235,
      "step": 3239
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.0004267932381480932,
      "learning_rate": 0.00651685393258427,
      "loss": 0.0007,
      "step": 3240
    },
    {
      "epoch": 4.05,
      "grad_norm": 0.013555080629885197,
      "learning_rate": 0.006512692467748648,
      "loss": 0.0414,
      "step": 3241
    },
    {
      "epoch": 4.05,
      "grad_norm": 0.0009022252052091062,
      "learning_rate": 0.006508531002913025,
      "loss": 0.0006,
      "step": 3242
    },
    {
      "epoch": 4.05,
      "grad_norm": 0.024291105568408966,
      "learning_rate": 0.006504369538077404,
      "loss": 0.0743,
      "step": 3243
    },
    {
      "epoch": 4.05,
      "grad_norm": 0.029194748029112816,
      "learning_rate": 0.006500208073241781,
      "loss": 0.191,
      "step": 3244
    },
    {
      "epoch": 4.05,
      "grad_norm": 0.02670598216354847,
      "learning_rate": 0.006496046608406159,
      "loss": 0.1799,
      "step": 3245
    },
    {
      "epoch": 4.05,
      "grad_norm": 0.010593435727059841,
      "learning_rate": 0.006491885143570538,
      "loss": 0.02,
      "step": 3246
    },
    {
      "epoch": 4.05,
      "grad_norm": 0.007169611752033234,
      "learning_rate": 0.0064877236787349146,
      "loss": 0.0191,
      "step": 3247
    },
    {
      "epoch": 4.05,
      "grad_norm": 0.03247116878628731,
      "learning_rate": 0.006483562213899292,
      "loss": 0.3464,
      "step": 3248
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.012370582669973373,
      "learning_rate": 0.006479400749063671,
      "loss": 0.034,
      "step": 3249
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.026385920122265816,
      "learning_rate": 0.006475239284228048,
      "loss": 0.2717,
      "step": 3250
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.025602780282497406,
      "learning_rate": 0.006471077819392426,
      "loss": 0.446,
      "step": 3251
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.04161074757575989,
      "learning_rate": 0.0064669163545568046,
      "loss": 0.2837,
      "step": 3252
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.006104580592364073,
      "learning_rate": 0.0064627548897211815,
      "loss": 0.0088,
      "step": 3253
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.013676202856004238,
      "learning_rate": 0.006458593424885559,
      "loss": 0.0236,
      "step": 3254
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.023310096934437752,
      "learning_rate": 0.006454431960049938,
      "loss": 0.0582,
      "step": 3255
    },
    {
      "epoch": 4.06,
      "grad_norm": 0.027982037514448166,
      "learning_rate": 0.006450270495214315,
      "loss": 0.183,
      "step": 3256
    },
    {
      "epoch": 4.07,
      "grad_norm": 0.024568580090999603,
      "learning_rate": 0.006446109030378693,
      "loss": 0.1952,
      "step": 3257
    },
    {
      "epoch": 4.07,
      "grad_norm": 0.019673071801662445,
      "learning_rate": 0.0064419475655430715,
      "loss": 0.1873,
      "step": 3258
    },
    {
      "epoch": 4.07,
      "grad_norm": 0.016565708443522453,
      "learning_rate": 0.006437786100707449,
      "loss": 0.1026,
      "step": 3259
    },
    {
      "epoch": 4.07,
      "grad_norm": 0.007131272926926613,
      "learning_rate": 0.006433624635871827,
      "loss": 0.0125,
      "step": 3260
    },
    {
      "epoch": 4.07,
      "grad_norm": 0.016957296058535576,
      "learning_rate": 0.006429463171036205,
      "loss": 0.1241,
      "step": 3261
    },
    {
      "epoch": 4.07,
      "grad_norm": 0.02343575283885002,
      "learning_rate": 0.006425301706200583,
      "loss": 0.1473,
      "step": 3262
    },
    {
      "epoch": 4.07,
      "grad_norm": 0.020984496921300888,
      "learning_rate": 0.0064211402413649615,
      "loss": 0.0875,
      "step": 3263
    },
    {
      "epoch": 4.07,
      "grad_norm": 0.02505241334438324,
      "learning_rate": 0.0064169787765293385,
      "loss": 0.178,
      "step": 3264
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.01733914203941822,
      "learning_rate": 0.006412817311693716,
      "loss": 0.056,
      "step": 3265
    },
    {
      "epoch": 4.08,
      "grad_norm": 8.043835987336934e-05,
      "learning_rate": 0.006408655846858095,
      "loss": 0.0001,
      "step": 3266
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.018268082290887833,
      "learning_rate": 0.006404494382022472,
      "loss": 0.0931,
      "step": 3267
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.021662697196006775,
      "learning_rate": 0.00640033291718685,
      "loss": 0.118,
      "step": 3268
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.024803409352898598,
      "learning_rate": 0.0063961714523512285,
      "loss": 0.1036,
      "step": 3269
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.021998820826411247,
      "learning_rate": 0.0063920099875156055,
      "loss": 0.1434,
      "step": 3270
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.009643975645303726,
      "learning_rate": 0.006387848522679983,
      "loss": 0.0166,
      "step": 3271
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.02433268539607525,
      "learning_rate": 0.006383687057844362,
      "loss": 0.2593,
      "step": 3272
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.02106921747326851,
      "learning_rate": 0.006379525593008739,
      "loss": 0.0956,
      "step": 3273
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.02477318048477173,
      "learning_rate": 0.006375364128173117,
      "loss": 0.2449,
      "step": 3274
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.024040674790740013,
      "learning_rate": 0.0063712026633374955,
      "loss": 0.2407,
      "step": 3275
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.01989302597939968,
      "learning_rate": 0.006367041198501872,
      "loss": 0.0887,
      "step": 3276
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.018870124593377113,
      "learning_rate": 0.00636287973366625,
      "loss": 0.0856,
      "step": 3277
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.018999425694346428,
      "learning_rate": 0.006358718268830629,
      "loss": 0.2494,
      "step": 3278
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.023175759240984917,
      "learning_rate": 0.006354556803995007,
      "loss": 0.1401,
      "step": 3279
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.01650932803750038,
      "learning_rate": 0.006350395339159384,
      "loss": 0.1292,
      "step": 3280
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.026916589587926865,
      "learning_rate": 0.006346233874323762,
      "loss": 0.3027,
      "step": 3281
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.0005852818721905351,
      "learning_rate": 0.00634207240948814,
      "loss": 0.0003,
      "step": 3282
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.013947836123406887,
      "learning_rate": 0.006337910944652517,
      "loss": 0.0468,
      "step": 3283
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.0002941428974736482,
      "learning_rate": 0.006333749479816896,
      "loss": 0.0003,
      "step": 3284
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.03198280930519104,
      "learning_rate": 0.006329588014981274,
      "loss": 0.0844,
      "step": 3285
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.02399076148867607,
      "learning_rate": 0.006325426550145651,
      "loss": 0.1225,
      "step": 3286
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.0193774551153183,
      "learning_rate": 0.006321265085310029,
      "loss": 0.1556,
      "step": 3287
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.025578616186976433,
      "learning_rate": 0.006317103620474407,
      "loss": 0.1683,
      "step": 3288
    },
    {
      "epoch": 4.11,
      "grad_norm": 0.0343082956969738,
      "learning_rate": 0.006312942155638784,
      "loss": 0.366,
      "step": 3289
    },
    {
      "epoch": 4.11,
      "grad_norm": 0.018076349049806595,
      "learning_rate": 0.006308780690803163,
      "loss": 0.1033,
      "step": 3290
    },
    {
      "epoch": 4.11,
      "grad_norm": 0.02373817004263401,
      "learning_rate": 0.006304619225967541,
      "loss": 0.1482,
      "step": 3291
    },
    {
      "epoch": 4.11,
      "grad_norm": 0.025266757234930992,
      "learning_rate": 0.006300457761131918,
      "loss": 0.0936,
      "step": 3292
    },
    {
      "epoch": 4.11,
      "grad_norm": 0.006867893505841494,
      "learning_rate": 0.006296296296296296,
      "loss": 0.0134,
      "step": 3293
    },
    {
      "epoch": 4.11,
      "grad_norm": 0.02000983990728855,
      "learning_rate": 0.006292134831460674,
      "loss": 0.0797,
      "step": 3294
    },
    {
      "epoch": 4.11,
      "grad_norm": 0.03798248991370201,
      "learning_rate": 0.006287973366625052,
      "loss": 0.2438,
      "step": 3295
    },
    {
      "epoch": 4.11,
      "grad_norm": 0.020125169306993484,
      "learning_rate": 0.00628381190178943,
      "loss": 0.0963,
      "step": 3296
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.021415559574961662,
      "learning_rate": 0.006279650436953808,
      "loss": 0.2073,
      "step": 3297
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.020984621718525887,
      "learning_rate": 0.006275488972118186,
      "loss": 0.0572,
      "step": 3298
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.00016954238526523113,
      "learning_rate": 0.006271327507282563,
      "loss": 0.0002,
      "step": 3299
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.000604549830313772,
      "learning_rate": 0.006267166042446941,
      "loss": 0.0004,
      "step": 3300
    },
    {
      "epoch": 4.12,
      "eval_loss": 0.2479248046875,
      "eval_runtime": 183.0453,
      "eval_samples_per_second": 1.098,
      "eval_steps_per_second": 0.552,
      "step": 3300
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.019276462495326996,
      "learning_rate": 0.00626300457761132,
      "loss": 0.1708,
      "step": 3301
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.03286515921354294,
      "learning_rate": 0.006258843112775698,
      "loss": 0.2512,
      "step": 3302
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.0236359890550375,
      "learning_rate": 0.006254681647940075,
      "loss": 0.2661,
      "step": 3303
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.022502709180116653,
      "learning_rate": 0.006250520183104453,
      "loss": 0.1515,
      "step": 3304
    },
    {
      "epoch": 4.13,
      "grad_norm": 0.015232613310217857,
      "learning_rate": 0.006246358718268831,
      "loss": 0.0394,
      "step": 3305
    },
    {
      "epoch": 4.13,
      "grad_norm": 0.023766756057739258,
      "learning_rate": 0.006242197253433208,
      "loss": 0.0938,
      "step": 3306
    },
    {
      "epoch": 4.13,
      "grad_norm": 0.02694154530763626,
      "learning_rate": 0.006238035788597587,
      "loss": 0.107,
      "step": 3307
    },
    {
      "epoch": 4.13,
      "grad_norm": 0.01917792297899723,
      "learning_rate": 0.006233874323761965,
      "loss": 0.0446,
      "step": 3308
    },
    {
      "epoch": 4.13,
      "grad_norm": 0.02235698699951172,
      "learning_rate": 0.006229712858926342,
      "loss": 0.0493,
      "step": 3309
    },
    {
      "epoch": 4.13,
      "grad_norm": 0.018061364069581032,
      "learning_rate": 0.00622555139409072,
      "loss": 0.0764,
      "step": 3310
    },
    {
      "epoch": 4.13,
      "grad_norm": 0.00827738642692566,
      "learning_rate": 0.006221389929255098,
      "loss": 0.0146,
      "step": 3311
    },
    {
      "epoch": 4.13,
      "grad_norm": 0.03203769773244858,
      "learning_rate": 0.006217228464419475,
      "loss": 0.238,
      "step": 3312
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.02865116111934185,
      "learning_rate": 0.006213066999583854,
      "loss": 0.093,
      "step": 3313
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.02602851390838623,
      "learning_rate": 0.006208905534748232,
      "loss": 0.1478,
      "step": 3314
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.01779230497777462,
      "learning_rate": 0.006204744069912609,
      "loss": 0.1013,
      "step": 3315
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.023592954501509666,
      "learning_rate": 0.006200582605076987,
      "loss": 0.302,
      "step": 3316
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.01934531331062317,
      "learning_rate": 0.006196421140241365,
      "loss": 0.0525,
      "step": 3317
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.030758116394281387,
      "learning_rate": 0.006192259675405743,
      "loss": 0.2815,
      "step": 3318
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.013597818091511726,
      "learning_rate": 0.006188098210570121,
      "loss": 0.0428,
      "step": 3319
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.03222590312361717,
      "learning_rate": 0.006183936745734499,
      "loss": 0.2448,
      "step": 3320
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.02048686519265175,
      "learning_rate": 0.006179775280898876,
      "loss": 0.1127,
      "step": 3321
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.035891979932785034,
      "learning_rate": 0.006175613816063255,
      "loss": 0.333,
      "step": 3322
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.03522548824548721,
      "learning_rate": 0.006171452351227632,
      "loss": 0.2976,
      "step": 3323
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.011165761388838291,
      "learning_rate": 0.00616729088639201,
      "loss": 0.0189,
      "step": 3324
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.026469213888049126,
      "learning_rate": 0.006163129421556389,
      "loss": 0.1628,
      "step": 3325
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.021535463631153107,
      "learning_rate": 0.0061589679567207655,
      "loss": 0.0322,
      "step": 3326
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.02551395259797573,
      "learning_rate": 0.006154806491885143,
      "loss": 0.0749,
      "step": 3327
    },
    {
      "epoch": 4.15,
      "grad_norm": 0.017076143994927406,
      "learning_rate": 0.006150645027049522,
      "loss": 0.147,
      "step": 3328
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.020524613559246063,
      "learning_rate": 0.006146483562213899,
      "loss": 0.0954,
      "step": 3329
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.0393730066716671,
      "learning_rate": 0.006142322097378277,
      "loss": 0.3625,
      "step": 3330
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.00012064012844348326,
      "learning_rate": 0.0061381606325426556,
      "loss": 0.0001,
      "step": 3331
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.020820271223783493,
      "learning_rate": 0.0061339991677070325,
      "loss": 0.0946,
      "step": 3332
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.027249393984675407,
      "learning_rate": 0.006129837702871411,
      "loss": 0.1857,
      "step": 3333
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.01905878260731697,
      "learning_rate": 0.006125676238035789,
      "loss": 0.0515,
      "step": 3334
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.02510533481836319,
      "learning_rate": 0.006121514773200166,
      "loss": 0.075,
      "step": 3335
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.027266591787338257,
      "learning_rate": 0.006117353308364545,
      "loss": 0.1313,
      "step": 3336
    },
    {
      "epoch": 4.17,
      "grad_norm": 0.029131578281521797,
      "learning_rate": 0.0061131918435289225,
      "loss": 0.1592,
      "step": 3337
    },
    {
      "epoch": 4.17,
      "grad_norm": 0.018239013850688934,
      "learning_rate": 0.0061090303786933,
      "loss": 0.054,
      "step": 3338
    },
    {
      "epoch": 4.17,
      "grad_norm": 0.010317107662558556,
      "learning_rate": 0.006104868913857678,
      "loss": 0.0368,
      "step": 3339
    },
    {
      "epoch": 4.17,
      "grad_norm": 0.008001415990293026,
      "learning_rate": 0.006100707449022056,
      "loss": 0.0131,
      "step": 3340
    },
    {
      "epoch": 4.17,
      "grad_norm": 0.01324660424143076,
      "learning_rate": 0.006096545984186434,
      "loss": 0.0504,
      "step": 3341
    },
    {
      "epoch": 4.17,
      "grad_norm": 0.018359825015068054,
      "learning_rate": 0.006092384519350812,
      "loss": 0.1102,
      "step": 3342
    },
    {
      "epoch": 4.17,
      "grad_norm": 0.023118067532777786,
      "learning_rate": 0.0060882230545151895,
      "loss": 0.2194,
      "step": 3343
    },
    {
      "epoch": 4.17,
      "grad_norm": 0.01795177347958088,
      "learning_rate": 0.006084061589679567,
      "loss": 0.0487,
      "step": 3344
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.018260639160871506,
      "learning_rate": 0.006079900124843946,
      "loss": 0.0722,
      "step": 3345
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.030028337612748146,
      "learning_rate": 0.006075738660008323,
      "loss": 0.1439,
      "step": 3346
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.031969886273145676,
      "learning_rate": 0.006071577195172701,
      "loss": 0.5024,
      "step": 3347
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.02236478589475155,
      "learning_rate": 0.0060674157303370795,
      "loss": 0.107,
      "step": 3348
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.025211647152900696,
      "learning_rate": 0.0060632542655014564,
      "loss": 0.2454,
      "step": 3349
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.040494516491889954,
      "learning_rate": 0.006059092800665834,
      "loss": 0.3435,
      "step": 3350
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.01940125599503517,
      "learning_rate": 0.006054931335830213,
      "loss": 0.0503,
      "step": 3351
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.004565530922263861,
      "learning_rate": 0.00605076987099459,
      "loss": 0.0057,
      "step": 3352
    },
    {
      "epoch": 4.19,
      "grad_norm": 0.027111908420920372,
      "learning_rate": 0.006046608406158968,
      "loss": 0.1171,
      "step": 3353
    },
    {
      "epoch": 4.19,
      "grad_norm": 0.009282763116061687,
      "learning_rate": 0.0060424469413233465,
      "loss": 0.0135,
      "step": 3354
    },
    {
      "epoch": 4.19,
      "grad_norm": 0.03726346790790558,
      "learning_rate": 0.006038285476487723,
      "loss": 0.1715,
      "step": 3355
    },
    {
      "epoch": 4.19,
      "grad_norm": 0.02438288927078247,
      "learning_rate": 0.006034124011652101,
      "loss": 0.2383,
      "step": 3356
    },
    {
      "epoch": 4.19,
      "grad_norm": 0.01725088432431221,
      "learning_rate": 0.00602996254681648,
      "loss": 0.1013,
      "step": 3357
    },
    {
      "epoch": 4.19,
      "grad_norm": 0.015628186985850334,
      "learning_rate": 0.006025801081980857,
      "loss": 0.0558,
      "step": 3358
    },
    {
      "epoch": 4.19,
      "grad_norm": 0.017118487507104874,
      "learning_rate": 0.006021639617145235,
      "loss": 0.1875,
      "step": 3359
    },
    {
      "epoch": 4.19,
      "grad_norm": 0.016280189156532288,
      "learning_rate": 0.006017478152309613,
      "loss": 0.0397,
      "step": 3360
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.02727035991847515,
      "learning_rate": 0.006013316687473991,
      "loss": 0.1868,
      "step": 3361
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.03314259648323059,
      "learning_rate": 0.006009155222638368,
      "loss": 0.2034,
      "step": 3362
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.028793422505259514,
      "learning_rate": 0.006004993757802747,
      "loss": 0.1238,
      "step": 3363
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.02383665181696415,
      "learning_rate": 0.006000832292967125,
      "loss": 0.112,
      "step": 3364
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.02869436703622341,
      "learning_rate": 0.005996670828131502,
      "loss": 0.3052,
      "step": 3365
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.0315740704536438,
      "learning_rate": 0.00599250936329588,
      "loss": 0.2803,
      "step": 3366
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.04353388771414757,
      "learning_rate": 0.005988347898460258,
      "loss": 0.3567,
      "step": 3367
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.029367942363023758,
      "learning_rate": 0.005984186433624637,
      "loss": 0.2441,
      "step": 3368
    },
    {
      "epoch": 4.21,
      "grad_norm": 0.021158600226044655,
      "learning_rate": 0.005980024968789014,
      "loss": 0.0841,
      "step": 3369
    },
    {
      "epoch": 4.21,
      "grad_norm": 0.02335706166923046,
      "learning_rate": 0.005975863503953392,
      "loss": 0.2571,
      "step": 3370
    },
    {
      "epoch": 4.21,
      "grad_norm": 0.019670283421874046,
      "learning_rate": 0.00597170203911777,
      "loss": 0.1609,
      "step": 3371
    },
    {
      "epoch": 4.21,
      "grad_norm": 0.015043710358440876,
      "learning_rate": 0.005967540574282147,
      "loss": 0.0598,
      "step": 3372
    },
    {
      "epoch": 4.21,
      "grad_norm": 0.028682297095656395,
      "learning_rate": 0.005963379109446525,
      "loss": 0.2111,
      "step": 3373
    },
    {
      "epoch": 4.21,
      "grad_norm": 0.013406490907073021,
      "learning_rate": 0.005959217644610904,
      "loss": 0.0526,
      "step": 3374
    },
    {
      "epoch": 4.21,
      "grad_norm": 0.028992749750614166,
      "learning_rate": 0.005955056179775281,
      "loss": 0.1351,
      "step": 3375
    },
    {
      "epoch": 4.21,
      "grad_norm": 0.013578087091445923,
      "learning_rate": 0.005950894714939659,
      "loss": 0.0476,
      "step": 3376
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.041046977043151855,
      "learning_rate": 0.005946733250104037,
      "loss": 0.2676,
      "step": 3377
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.022814931347966194,
      "learning_rate": 0.005942571785268414,
      "loss": 0.1427,
      "step": 3378
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.04454979673027992,
      "learning_rate": 0.005938410320432792,
      "loss": 0.2856,
      "step": 3379
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.011750491335988045,
      "learning_rate": 0.005934248855597171,
      "loss": 0.0275,
      "step": 3380
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.021018803119659424,
      "learning_rate": 0.005930087390761549,
      "loss": 0.1181,
      "step": 3381
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.013671785593032837,
      "learning_rate": 0.005925925925925926,
      "loss": 0.1155,
      "step": 3382
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.03362062945961952,
      "learning_rate": 0.005921764461090304,
      "loss": 0.2419,
      "step": 3383
    },
    {
      "epoch": 4.22,
      "grad_norm": 0.017696168273687363,
      "learning_rate": 0.005917602996254682,
      "loss": 0.1422,
      "step": 3384
    },
    {
      "epoch": 4.23,
      "grad_norm": 0.026200709864497185,
      "learning_rate": 0.005913441531419059,
      "loss": 0.1833,
      "step": 3385
    },
    {
      "epoch": 4.23,
      "grad_norm": 0.031593892723321915,
      "learning_rate": 0.005909280066583438,
      "loss": 0.1469,
      "step": 3386
    },
    {
      "epoch": 4.23,
      "grad_norm": 0.02802674099802971,
      "learning_rate": 0.005905118601747816,
      "loss": 0.3721,
      "step": 3387
    },
    {
      "epoch": 4.23,
      "grad_norm": 0.028142092749476433,
      "learning_rate": 0.005900957136912193,
      "loss": 0.1233,
      "step": 3388
    },
    {
      "epoch": 4.23,
      "grad_norm": 0.022221090272068977,
      "learning_rate": 0.005896795672076571,
      "loss": 0.115,
      "step": 3389
    },
    {
      "epoch": 4.23,
      "grad_norm": 0.0267118401825428,
      "learning_rate": 0.005892634207240949,
      "loss": 0.1748,
      "step": 3390
    },
    {
      "epoch": 4.23,
      "grad_norm": 0.023342786356806755,
      "learning_rate": 0.005888472742405326,
      "loss": 0.0894,
      "step": 3391
    },
    {
      "epoch": 4.23,
      "grad_norm": 0.01994529739022255,
      "learning_rate": 0.005884311277569705,
      "loss": 0.1019,
      "step": 3392
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.029589630663394928,
      "learning_rate": 0.005880149812734083,
      "loss": 0.1449,
      "step": 3393
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.03192434459924698,
      "learning_rate": 0.0058759883478984596,
      "loss": 0.27,
      "step": 3394
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.02297034114599228,
      "learning_rate": 0.005871826883062838,
      "loss": 0.3098,
      "step": 3395
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.01847153902053833,
      "learning_rate": 0.005867665418227216,
      "loss": 0.1003,
      "step": 3396
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.0049329400062561035,
      "learning_rate": 0.005863503953391594,
      "loss": 0.0064,
      "step": 3397
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.008189541287720203,
      "learning_rate": 0.005859342488555972,
      "loss": 0.0169,
      "step": 3398
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.023352231830358505,
      "learning_rate": 0.00585518102372035,
      "loss": 0.1639,
      "step": 3399
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.0448044091463089,
      "learning_rate": 0.005851019558884727,
      "loss": 0.2529,
      "step": 3400
    },
    {
      "epoch": 4.25,
      "grad_norm": 0.02753887139260769,
      "learning_rate": 0.005846858094049105,
      "loss": 0.1941,
      "step": 3401
    },
    {
      "epoch": 4.25,
      "grad_norm": 0.019078120589256287,
      "learning_rate": 0.005842696629213483,
      "loss": 0.1024,
      "step": 3402
    },
    {
      "epoch": 4.25,
      "grad_norm": 0.02293316274881363,
      "learning_rate": 0.005838535164377861,
      "loss": 0.1209,
      "step": 3403
    },
    {
      "epoch": 4.25,
      "grad_norm": 0.017980247735977173,
      "learning_rate": 0.00583437369954224,
      "loss": 0.1171,
      "step": 3404
    },
    {
      "epoch": 4.25,
      "grad_norm": 0.009146086871623993,
      "learning_rate": 0.0058302122347066165,
      "loss": 0.0154,
      "step": 3405
    },
    {
      "epoch": 4.25,
      "grad_norm": 0.03178134560585022,
      "learning_rate": 0.005826050769870995,
      "loss": 0.1729,
      "step": 3406
    },
    {
      "epoch": 4.25,
      "grad_norm": 0.018179964274168015,
      "learning_rate": 0.005821889305035373,
      "loss": 0.1069,
      "step": 3407
    },
    {
      "epoch": 4.25,
      "grad_norm": 0.02726644277572632,
      "learning_rate": 0.00581772784019975,
      "loss": 0.3289,
      "step": 3408
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.03320320323109627,
      "learning_rate": 0.005813566375364129,
      "loss": 0.3237,
      "step": 3409
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.026240495964884758,
      "learning_rate": 0.0058094049105285065,
      "loss": 0.0648,
      "step": 3410
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.020207758992910385,
      "learning_rate": 0.0058052434456928835,
      "loss": 0.0637,
      "step": 3411
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.019345896318554878,
      "learning_rate": 0.005801081980857262,
      "loss": 0.0465,
      "step": 3412
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.021564001217484474,
      "learning_rate": 0.00579692051602164,
      "loss": 0.1444,
      "step": 3413
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.02322378382086754,
      "learning_rate": 0.005792759051186017,
      "loss": 0.0908,
      "step": 3414
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.02645736001431942,
      "learning_rate": 0.005788597586350396,
      "loss": 0.0737,
      "step": 3415
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.02952725626528263,
      "learning_rate": 0.0057844361215147735,
      "loss": 0.2478,
      "step": 3416
    },
    {
      "epoch": 4.27,
      "grad_norm": 0.016413748264312744,
      "learning_rate": 0.0057802746566791505,
      "loss": 0.0425,
      "step": 3417
    },
    {
      "epoch": 4.27,
      "grad_norm": 0.017983453348279,
      "learning_rate": 0.005776113191843529,
      "loss": 0.0612,
      "step": 3418
    },
    {
      "epoch": 4.27,
      "grad_norm": 0.023251233622431755,
      "learning_rate": 0.005771951727007907,
      "loss": 0.1373,
      "step": 3419
    },
    {
      "epoch": 4.27,
      "grad_norm": 0.027456175535917282,
      "learning_rate": 0.005767790262172285,
      "loss": 0.0615,
      "step": 3420
    },
    {
      "epoch": 4.27,
      "grad_norm": 0.027367951348423958,
      "learning_rate": 0.005763628797336663,
      "loss": 0.1998,
      "step": 3421
    },
    {
      "epoch": 4.27,
      "grad_norm": 0.01007504016160965,
      "learning_rate": 0.0057594673325010405,
      "loss": 0.0142,
      "step": 3422
    },
    {
      "epoch": 4.27,
      "grad_norm": 0.01637990027666092,
      "learning_rate": 0.005755305867665418,
      "loss": 0.07,
      "step": 3423
    },
    {
      "epoch": 4.27,
      "grad_norm": 0.016312522813677788,
      "learning_rate": 0.005751144402829796,
      "loss": 0.1364,
      "step": 3424
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.017119845375418663,
      "learning_rate": 0.005746982937994174,
      "loss": 0.0714,
      "step": 3425
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.027216577902436256,
      "learning_rate": 0.005742821473158552,
      "loss": 0.1426,
      "step": 3426
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.032351989299058914,
      "learning_rate": 0.0057386600083229305,
      "loss": 0.2224,
      "step": 3427
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.018840022385120392,
      "learning_rate": 0.0057344985434873074,
      "loss": 0.0674,
      "step": 3428
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.034731414169073105,
      "learning_rate": 0.005730337078651685,
      "loss": 0.2991,
      "step": 3429
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.02531472221016884,
      "learning_rate": 0.005726175613816064,
      "loss": 0.0507,
      "step": 3430
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.019280260428786278,
      "learning_rate": 0.005722014148980441,
      "loss": 0.1156,
      "step": 3431
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.02632824331521988,
      "learning_rate": 0.005717852684144819,
      "loss": 0.161,
      "step": 3432
    },
    {
      "epoch": 4.29,
      "grad_norm": 0.022339949384331703,
      "learning_rate": 0.0057136912193091974,
      "loss": 0.1003,
      "step": 3433
    },
    {
      "epoch": 4.29,
      "grad_norm": 0.03037218190729618,
      "learning_rate": 0.005709529754473574,
      "loss": 0.2048,
      "step": 3434
    },
    {
      "epoch": 4.29,
      "grad_norm": 0.03426725044846535,
      "learning_rate": 0.005705368289637952,
      "loss": 0.303,
      "step": 3435
    },
    {
      "epoch": 4.29,
      "grad_norm": 0.019214633852243423,
      "learning_rate": 0.005701206824802331,
      "loss": 0.0531,
      "step": 3436
    },
    {
      "epoch": 4.29,
      "grad_norm": 0.03365178033709526,
      "learning_rate": 0.005697045359966708,
      "loss": 0.2198,
      "step": 3437
    },
    {
      "epoch": 4.29,
      "grad_norm": 0.010867352597415447,
      "learning_rate": 0.005692883895131086,
      "loss": 0.0268,
      "step": 3438
    },
    {
      "epoch": 4.29,
      "grad_norm": 0.032669294625520706,
      "learning_rate": 0.005688722430295464,
      "loss": 0.2308,
      "step": 3439
    },
    {
      "epoch": 4.29,
      "grad_norm": 0.024440858513116837,
      "learning_rate": 0.005684560965459841,
      "loss": 0.1394,
      "step": 3440
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.023663341999053955,
      "learning_rate": 0.00568039950062422,
      "loss": 0.1293,
      "step": 3441
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.02108931913971901,
      "learning_rate": 0.005676238035788598,
      "loss": 0.1866,
      "step": 3442
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.02195298857986927,
      "learning_rate": 0.005672076570952976,
      "loss": 0.0729,
      "step": 3443
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.018710769712924957,
      "learning_rate": 0.0056679151061173536,
      "loss": 0.1146,
      "step": 3444
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.04142192006111145,
      "learning_rate": 0.005663753641281731,
      "loss": 0.1909,
      "step": 3445
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.025820117443799973,
      "learning_rate": 0.005659592176446109,
      "loss": 0.1204,
      "step": 3446
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.022746719419956207,
      "learning_rate": 0.005655430711610488,
      "loss": 0.1025,
      "step": 3447
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.02388349547982216,
      "learning_rate": 0.005651269246774865,
      "loss": 0.1212,
      "step": 3448
    },
    {
      "epoch": 4.31,
      "grad_norm": 0.024757307022809982,
      "learning_rate": 0.005647107781939243,
      "loss": 0.1071,
      "step": 3449
    },
    {
      "epoch": 4.31,
      "grad_norm": 0.020886199548840523,
      "learning_rate": 0.005642946317103621,
      "loss": 0.1359,
      "step": 3450
    },
    {
      "epoch": 4.31,
      "grad_norm": 0.01958843506872654,
      "learning_rate": 0.005638784852267998,
      "loss": 0.0731,
      "step": 3451
    },
    {
      "epoch": 4.31,
      "grad_norm": 0.022204039618372917,
      "learning_rate": 0.005634623387432376,
      "loss": 0.0809,
      "step": 3452
    },
    {
      "epoch": 4.31,
      "grad_norm": 0.024871695786714554,
      "learning_rate": 0.005630461922596755,
      "loss": 0.1178,
      "step": 3453
    },
    {
      "epoch": 4.31,
      "grad_norm": 0.033088523894548416,
      "learning_rate": 0.005626300457761132,
      "loss": 0.0796,
      "step": 3454
    },
    {
      "epoch": 4.31,
      "grad_norm": 0.01599760726094246,
      "learning_rate": 0.00562213899292551,
      "loss": 0.0563,
      "step": 3455
    },
    {
      "epoch": 4.31,
      "grad_norm": 0.0019192753825336695,
      "learning_rate": 0.005617977528089888,
      "loss": 0.001,
      "step": 3456
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.010592589154839516,
      "learning_rate": 0.005613816063254265,
      "loss": 0.0271,
      "step": 3457
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.028501363471150398,
      "learning_rate": 0.005609654598418643,
      "loss": 0.1766,
      "step": 3458
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.032245319336652756,
      "learning_rate": 0.005605493133583022,
      "loss": 0.2039,
      "step": 3459
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.020711392164230347,
      "learning_rate": 0.005601331668747399,
      "loss": 0.075,
      "step": 3460
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.0596572645008564,
      "learning_rate": 0.005597170203911777,
      "loss": 0.6577,
      "step": 3461
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.031157705932855606,
      "learning_rate": 0.005593008739076155,
      "loss": 0.2839,
      "step": 3462
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.026977529749274254,
      "learning_rate": 0.005588847274240533,
      "loss": 0.3015,
      "step": 3463
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.0269811749458313,
      "learning_rate": 0.00558468580940491,
      "loss": 0.301,
      "step": 3464
    },
    {
      "epoch": 4.33,
      "grad_norm": 0.025596175342798233,
      "learning_rate": 0.005580524344569289,
      "loss": 0.248,
      "step": 3465
    },
    {
      "epoch": 4.33,
      "grad_norm": 0.008379586972296238,
      "learning_rate": 0.005576362879733667,
      "loss": 0.0266,
      "step": 3466
    },
    {
      "epoch": 4.33,
      "grad_norm": 0.021019775420427322,
      "learning_rate": 0.005572201414898044,
      "loss": 0.0512,
      "step": 3467
    },
    {
      "epoch": 4.33,
      "grad_norm": 0.024421600624918938,
      "learning_rate": 0.005568039950062422,
      "loss": 0.1831,
      "step": 3468
    },
    {
      "epoch": 4.33,
      "grad_norm": 0.03155336529016495,
      "learning_rate": 0.0055638784852268,
      "loss": 0.1106,
      "step": 3469
    },
    {
      "epoch": 4.33,
      "grad_norm": 0.02199382707476616,
      "learning_rate": 0.005559717020391177,
      "loss": 0.0847,
      "step": 3470
    },
    {
      "epoch": 4.33,
      "grad_norm": 0.01712925359606743,
      "learning_rate": 0.005555555555555556,
      "loss": 0.0276,
      "step": 3471
    },
    {
      "epoch": 4.33,
      "grad_norm": 0.02723831683397293,
      "learning_rate": 0.005551394090719934,
      "loss": 0.2238,
      "step": 3472
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.019958371296525,
      "learning_rate": 0.0055472326258843106,
      "loss": 0.0653,
      "step": 3473
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.025973889976739883,
      "learning_rate": 0.005543071161048689,
      "loss": 0.1226,
      "step": 3474
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.02463850937783718,
      "learning_rate": 0.005538909696213067,
      "loss": 0.2,
      "step": 3475
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.027419591322541237,
      "learning_rate": 0.005534748231377444,
      "loss": 0.168,
      "step": 3476
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.01882113330066204,
      "learning_rate": 0.005530586766541823,
      "loss": 0.0735,
      "step": 3477
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.028466900810599327,
      "learning_rate": 0.0055264253017062006,
      "loss": 0.172,
      "step": 3478
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.0235134307295084,
      "learning_rate": 0.005522263836870579,
      "loss": 0.1443,
      "step": 3479
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.04183943569660187,
      "learning_rate": 0.005518102372034956,
      "loss": 0.4138,
      "step": 3480
    },
    {
      "epoch": 4.35,
      "grad_norm": 0.005324280820786953,
      "learning_rate": 0.005513940907199334,
      "loss": 0.0084,
      "step": 3481
    },
    {
      "epoch": 4.35,
      "grad_norm": 0.031857930123806,
      "learning_rate": 0.005509779442363713,
      "loss": 0.2195,
      "step": 3482
    },
    {
      "epoch": 4.35,
      "grad_norm": 0.024516740813851357,
      "learning_rate": 0.00550561797752809,
      "loss": 0.1943,
      "step": 3483
    },
    {
      "epoch": 4.35,
      "grad_norm": 0.020636767148971558,
      "learning_rate": 0.0055014565126924675,
      "loss": 0.3232,
      "step": 3484
    },
    {
      "epoch": 4.35,
      "grad_norm": 0.04060252383351326,
      "learning_rate": 0.005497295047856846,
      "loss": 0.2085,
      "step": 3485
    },
    {
      "epoch": 4.35,
      "grad_norm": 0.02793070487678051,
      "learning_rate": 0.005493133583021224,
      "loss": 0.1554,
      "step": 3486
    },
    {
      "epoch": 4.35,
      "grad_norm": 0.025252697989344597,
      "learning_rate": 0.005488972118185601,
      "loss": 0.2294,
      "step": 3487
    },
    {
      "epoch": 4.35,
      "grad_norm": 0.025176478549838066,
      "learning_rate": 0.00548481065334998,
      "loss": 0.2032,
      "step": 3488
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.03154861181974411,
      "learning_rate": 0.0054806491885143575,
      "loss": 0.2117,
      "step": 3489
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.021281873807311058,
      "learning_rate": 0.0054764877236787345,
      "loss": 0.1052,
      "step": 3490
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.00256437249481678,
      "learning_rate": 0.005472326258843113,
      "loss": 0.003,
      "step": 3491
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.018344253301620483,
      "learning_rate": 0.005468164794007491,
      "loss": 0.1466,
      "step": 3492
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.03486689180135727,
      "learning_rate": 0.005464003329171868,
      "loss": 0.2069,
      "step": 3493
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.022019581869244576,
      "learning_rate": 0.005459841864336247,
      "loss": 0.2861,
      "step": 3494
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.00033355318009853363,
      "learning_rate": 0.0054556803995006245,
      "loss": 0.0003,
      "step": 3495
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.024010466411709785,
      "learning_rate": 0.0054515189346650015,
      "loss": 0.1606,
      "step": 3496
    },
    {
      "epoch": 4.37,
      "grad_norm": 0.019811803475022316,
      "learning_rate": 0.00544735746982938,
      "loss": 0.1096,
      "step": 3497
    },
    {
      "epoch": 4.37,
      "grad_norm": 0.026636432856321335,
      "learning_rate": 0.005443196004993758,
      "loss": 0.1036,
      "step": 3498
    },
    {
      "epoch": 4.37,
      "grad_norm": 0.009262734092772007,
      "learning_rate": 0.005439034540158135,
      "loss": 0.0241,
      "step": 3499
    },
    {
      "epoch": 4.37,
      "grad_norm": 0.022193655371665955,
      "learning_rate": 0.005434873075322514,
      "loss": 0.1161,
      "step": 3500
    },
    {
      "epoch": 4.37,
      "grad_norm": 0.019152270630002022,
      "learning_rate": 0.0054307116104868915,
      "loss": 0.1603,
      "step": 3501
    },
    {
      "epoch": 4.37,
      "grad_norm": 0.020231500267982483,
      "learning_rate": 0.005426550145651269,
      "loss": 0.1199,
      "step": 3502
    },
    {
      "epoch": 4.37,
      "grad_norm": 0.01647963561117649,
      "learning_rate": 0.005422388680815647,
      "loss": 0.1102,
      "step": 3503
    },
    {
      "epoch": 4.37,
      "grad_norm": 0.030995305627584457,
      "learning_rate": 0.005418227215980025,
      "loss": 0.0815,
      "step": 3504
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.02909182757139206,
      "learning_rate": 0.005414065751144403,
      "loss": 0.3062,
      "step": 3505
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.024453340098261833,
      "learning_rate": 0.0054099042863087815,
      "loss": 0.1771,
      "step": 3506
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.026184849441051483,
      "learning_rate": 0.005405742821473158,
      "loss": 0.1549,
      "step": 3507
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.02363199181854725,
      "learning_rate": 0.005401581356637536,
      "loss": 0.1487,
      "step": 3508
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.02276409976184368,
      "learning_rate": 0.005397419891801915,
      "loss": 0.1385,
      "step": 3509
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.007369557395577431,
      "learning_rate": 0.005393258426966292,
      "loss": 0.0161,
      "step": 3510
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.017069049179553986,
      "learning_rate": 0.00538909696213067,
      "loss": 0.0906,
      "step": 3511
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.023124489933252335,
      "learning_rate": 0.0053849354972950484,
      "loss": 0.162,
      "step": 3512
    },
    {
      "epoch": 4.39,
      "grad_norm": 0.021411223337054253,
      "learning_rate": 0.005380774032459425,
      "loss": 0.1268,
      "step": 3513
    },
    {
      "epoch": 4.39,
      "grad_norm": 0.022482169792056084,
      "learning_rate": 0.005376612567623804,
      "loss": 0.1709,
      "step": 3514
    },
    {
      "epoch": 4.39,
      "grad_norm": 0.023696595802903175,
      "learning_rate": 0.005372451102788182,
      "loss": 0.1323,
      "step": 3515
    },
    {
      "epoch": 4.39,
      "grad_norm": 0.03152932971715927,
      "learning_rate": 0.005368289637952559,
      "loss": 0.158,
      "step": 3516
    },
    {
      "epoch": 4.39,
      "grad_norm": 0.017584143206477165,
      "learning_rate": 0.005364128173116938,
      "loss": 0.0401,
      "step": 3517
    },
    {
      "epoch": 4.39,
      "grad_norm": 0.0344432033598423,
      "learning_rate": 0.005359966708281315,
      "loss": 0.312,
      "step": 3518
    },
    {
      "epoch": 4.39,
      "grad_norm": 0.025702929124236107,
      "learning_rate": 0.005355805243445692,
      "loss": 0.0787,
      "step": 3519
    },
    {
      "epoch": 4.39,
      "grad_norm": 0.02820902317762375,
      "learning_rate": 0.005351643778610071,
      "loss": 0.1825,
      "step": 3520
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.018696948885917664,
      "learning_rate": 0.005347482313774449,
      "loss": 0.0607,
      "step": 3521
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.023014014586806297,
      "learning_rate": 0.005343320848938827,
      "loss": 0.1073,
      "step": 3522
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.024543212726712227,
      "learning_rate": 0.0053391593841032045,
      "loss": 0.2776,
      "step": 3523
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.028840119019150734,
      "learning_rate": 0.005334997919267582,
      "loss": 0.1792,
      "step": 3524
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.0006089126691222191,
      "learning_rate": 0.00533083645443196,
      "loss": 0.0004,
      "step": 3525
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.022763334214687347,
      "learning_rate": 0.005326674989596338,
      "loss": 0.1567,
      "step": 3526
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.00021819472021888942,
      "learning_rate": 0.005322513524760716,
      "loss": 0.0002,
      "step": 3527
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.018153849989175797,
      "learning_rate": 0.005318352059925094,
      "loss": 0.0939,
      "step": 3528
    },
    {
      "epoch": 4.41,
      "grad_norm": 0.028563806787133217,
      "learning_rate": 0.005314190595089472,
      "loss": 0.1477,
      "step": 3529
    },
    {
      "epoch": 4.41,
      "grad_norm": 0.021174386143684387,
      "learning_rate": 0.005310029130253849,
      "loss": 0.0757,
      "step": 3530
    },
    {
      "epoch": 4.41,
      "grad_norm": 0.020619111135601997,
      "learning_rate": 0.005305867665418227,
      "loss": 0.0821,
      "step": 3531
    },
    {
      "epoch": 4.41,
      "grad_norm": 0.0212760791182518,
      "learning_rate": 0.005301706200582606,
      "loss": 0.0941,
      "step": 3532
    },
    {
      "epoch": 4.41,
      "grad_norm": 0.04359610378742218,
      "learning_rate": 0.005297544735746983,
      "loss": 0.2152,
      "step": 3533
    },
    {
      "epoch": 4.41,
      "grad_norm": 0.016529986634850502,
      "learning_rate": 0.005293383270911361,
      "loss": 0.0353,
      "step": 3534
    },
    {
      "epoch": 4.41,
      "grad_norm": 0.023268084973096848,
      "learning_rate": 0.005289221806075739,
      "loss": 0.1115,
      "step": 3535
    },
    {
      "epoch": 4.41,
      "grad_norm": 0.022917183116078377,
      "learning_rate": 0.005285060341240116,
      "loss": 0.2025,
      "step": 3536
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.002455709967762232,
      "learning_rate": 0.005280898876404494,
      "loss": 0.0036,
      "step": 3537
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.025794394314289093,
      "learning_rate": 0.005276737411568873,
      "loss": 0.1692,
      "step": 3538
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.03258179500699043,
      "learning_rate": 0.00527257594673325,
      "loss": 0.1838,
      "step": 3539
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.030258184298872948,
      "learning_rate": 0.005268414481897628,
      "loss": 0.1754,
      "step": 3540
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.019148312509059906,
      "learning_rate": 0.005264253017062006,
      "loss": 0.0894,
      "step": 3541
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.03350820392370224,
      "learning_rate": 0.005260091552226383,
      "loss": 0.202,
      "step": 3542
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.020543675869703293,
      "learning_rate": 0.005255930087390761,
      "loss": 0.05,
      "step": 3543
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.027754778042435646,
      "learning_rate": 0.00525176862255514,
      "loss": 0.1299,
      "step": 3544
    },
    {
      "epoch": 4.43,
      "grad_norm": 0.01213155873119831,
      "learning_rate": 0.005247607157719518,
      "loss": 0.0317,
      "step": 3545
    },
    {
      "epoch": 4.43,
      "grad_norm": 0.02839839644730091,
      "learning_rate": 0.005243445692883895,
      "loss": 0.2197,
      "step": 3546
    },
    {
      "epoch": 4.43,
      "grad_norm": 0.01944499835371971,
      "learning_rate": 0.005239284228048273,
      "loss": 0.071,
      "step": 3547
    },
    {
      "epoch": 4.43,
      "grad_norm": 0.023704564198851585,
      "learning_rate": 0.005235122763212651,
      "loss": 0.1075,
      "step": 3548
    },
    {
      "epoch": 4.43,
      "grad_norm": 0.03418133407831192,
      "learning_rate": 0.005230961298377028,
      "loss": 0.1819,
      "step": 3549
    },
    {
      "epoch": 4.43,
      "grad_norm": 0.01884033903479576,
      "learning_rate": 0.005226799833541407,
      "loss": 0.0928,
      "step": 3550
    },
    {
      "epoch": 4.43,
      "grad_norm": 0.02707977406680584,
      "learning_rate": 0.005222638368705785,
      "loss": 0.1442,
      "step": 3551
    },
    {
      "epoch": 4.43,
      "grad_norm": 0.019476788118481636,
      "learning_rate": 0.005218476903870163,
      "loss": 0.0859,
      "step": 3552
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.030031653121113777,
      "learning_rate": 0.00521431543903454,
      "loss": 0.4065,
      "step": 3553
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.017679106444120407,
      "learning_rate": 0.005210153974198918,
      "loss": 0.1193,
      "step": 3554
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.03824363648891449,
      "learning_rate": 0.005205992509363297,
      "loss": 0.1583,
      "step": 3555
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.02455917000770569,
      "learning_rate": 0.005201831044527674,
      "loss": 0.1292,
      "step": 3556
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.023543527349829674,
      "learning_rate": 0.0051976695796920516,
      "loss": 0.1005,
      "step": 3557
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.03493258357048035,
      "learning_rate": 0.00519350811485643,
      "loss": 0.1301,
      "step": 3558
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.025322142988443375,
      "learning_rate": 0.005189346650020807,
      "loss": 0.1261,
      "step": 3559
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.029382750391960144,
      "learning_rate": 0.005185185185185185,
      "loss": 0.1826,
      "step": 3560
    },
    {
      "epoch": 4.45,
      "grad_norm": 0.04105312004685402,
      "learning_rate": 0.005181023720349564,
      "loss": 0.3193,
      "step": 3561
    },
    {
      "epoch": 4.45,
      "grad_norm": 0.02230415679514408,
      "learning_rate": 0.005176862255513941,
      "loss": 0.1489,
      "step": 3562
    },
    {
      "epoch": 4.45,
      "grad_norm": 0.012525084428489208,
      "learning_rate": 0.0051727007906783185,
      "loss": 0.03,
      "step": 3563
    },
    {
      "epoch": 4.45,
      "grad_norm": 0.019648179411888123,
      "learning_rate": 0.005168539325842697,
      "loss": 0.0567,
      "step": 3564
    },
    {
      "epoch": 4.45,
      "grad_norm": 0.02390655130147934,
      "learning_rate": 0.005164377861007075,
      "loss": 0.1096,
      "step": 3565
    },
    {
      "epoch": 4.45,
      "grad_norm": 0.01678580418229103,
      "learning_rate": 0.005160216396171452,
      "loss": 0.0566,
      "step": 3566
    },
    {
      "epoch": 4.45,
      "grad_norm": 0.015313616022467613,
      "learning_rate": 0.005156054931335831,
      "loss": 0.0436,
      "step": 3567
    },
    {
      "epoch": 4.45,
      "grad_norm": 0.025366997346282005,
      "learning_rate": 0.0051518934665002085,
      "loss": 0.1494,
      "step": 3568
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.023350290954113007,
      "learning_rate": 0.0051477320016645855,
      "loss": 0.2059,
      "step": 3569
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.016752973198890686,
      "learning_rate": 0.005143570536828964,
      "loss": 0.0591,
      "step": 3570
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.0314360111951828,
      "learning_rate": 0.005139409071993342,
      "loss": 0.1816,
      "step": 3571
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.03135834261775017,
      "learning_rate": 0.005135247607157719,
      "loss": 0.2744,
      "step": 3572
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.024911269545555115,
      "learning_rate": 0.005131086142322098,
      "loss": 0.11,
      "step": 3573
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.028993971645832062,
      "learning_rate": 0.0051269246774864755,
      "loss": 0.1879,
      "step": 3574
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.028970615938305855,
      "learning_rate": 0.0051227632126508525,
      "loss": 0.1044,
      "step": 3575
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.028058597818017006,
      "learning_rate": 0.005118601747815231,
      "loss": 0.2556,
      "step": 3576
    },
    {
      "epoch": 4.47,
      "grad_norm": 0.019019290804862976,
      "learning_rate": 0.005114440282979609,
      "loss": 0.0734,
      "step": 3577
    },
    {
      "epoch": 4.47,
      "grad_norm": 0.039958253502845764,
      "learning_rate": 0.005110278818143986,
      "loss": 0.2688,
      "step": 3578
    },
    {
      "epoch": 4.47,
      "grad_norm": 0.02369806542992592,
      "learning_rate": 0.005106117353308365,
      "loss": 0.1073,
      "step": 3579
    },
    {
      "epoch": 4.47,
      "grad_norm": 0.0365569144487381,
      "learning_rate": 0.0051019558884727425,
      "loss": 0.2257,
      "step": 3580
    },
    {
      "epoch": 4.47,
      "grad_norm": 0.026226412504911423,
      "learning_rate": 0.00509779442363712,
      "loss": 0.0634,
      "step": 3581
    },
    {
      "epoch": 4.47,
      "grad_norm": 0.03198345750570297,
      "learning_rate": 0.005093632958801498,
      "loss": 0.1827,
      "step": 3582
    },
    {
      "epoch": 4.47,
      "grad_norm": 0.0184856615960598,
      "learning_rate": 0.005089471493965876,
      "loss": 0.1216,
      "step": 3583
    },
    {
      "epoch": 4.47,
      "grad_norm": 0.017880816012620926,
      "learning_rate": 0.005085310029130254,
      "loss": 0.1175,
      "step": 3584
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.02403322421014309,
      "learning_rate": 0.005081148564294632,
      "loss": 0.1641,
      "step": 3585
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.021283116191625595,
      "learning_rate": 0.005076987099459009,
      "loss": 0.1821,
      "step": 3586
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.025780940428376198,
      "learning_rate": 0.005072825634623388,
      "loss": 0.0492,
      "step": 3587
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.014021790586411953,
      "learning_rate": 0.005068664169787766,
      "loss": 0.1006,
      "step": 3588
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.02826346643269062,
      "learning_rate": 0.005064502704952143,
      "loss": 0.1727,
      "step": 3589
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.02132757194340229,
      "learning_rate": 0.005060341240116522,
      "loss": 0.1155,
      "step": 3590
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.04026760160923004,
      "learning_rate": 0.005056179775280899,
      "loss": 0.1498,
      "step": 3591
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.014647956006228924,
      "learning_rate": 0.005052018310445276,
      "loss": 0.0267,
      "step": 3592
    },
    {
      "epoch": 4.49,
      "grad_norm": 0.016542822122573853,
      "learning_rate": 0.005047856845609655,
      "loss": 0.0911,
      "step": 3593
    },
    {
      "epoch": 4.49,
      "grad_norm": 0.02143695019185543,
      "learning_rate": 0.005043695380774033,
      "loss": 0.2092,
      "step": 3594
    },
    {
      "epoch": 4.49,
      "grad_norm": 0.028245478868484497,
      "learning_rate": 0.00503953391593841,
      "loss": 0.2391,
      "step": 3595
    },
    {
      "epoch": 4.49,
      "grad_norm": 0.014377542771399021,
      "learning_rate": 0.005035372451102789,
      "loss": 0.0459,
      "step": 3596
    },
    {
      "epoch": 4.49,
      "grad_norm": 0.010652601718902588,
      "learning_rate": 0.005031210986267166,
      "loss": 0.0244,
      "step": 3597
    },
    {
      "epoch": 4.49,
      "grad_norm": 0.01230910886079073,
      "learning_rate": 0.005027049521431543,
      "loss": 0.0079,
      "step": 3598
    },
    {
      "epoch": 4.49,
      "grad_norm": 0.02824745886027813,
      "learning_rate": 0.005022888056595922,
      "loss": 0.1962,
      "step": 3599
    },
    {
      "epoch": 4.49,
      "grad_norm": 0.02367478422820568,
      "learning_rate": 0.0050187265917603,
      "loss": 0.1827,
      "step": 3600
    },
    {
      "epoch": 4.49,
      "eval_loss": 0.25048828125,
      "eval_runtime": 182.8448,
      "eval_samples_per_second": 1.099,
      "eval_steps_per_second": 0.552,
      "step": 3600
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.029129955917596817,
      "learning_rate": 0.005014565126924677,
      "loss": 0.0907,
      "step": 3601
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.02025226503610611,
      "learning_rate": 0.0050104036620890555,
      "loss": 0.1006,
      "step": 3602
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.017007621005177498,
      "learning_rate": 0.005006242197253433,
      "loss": 0.1033,
      "step": 3603
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.021113485097885132,
      "learning_rate": 0.005002080732417811,
      "loss": 0.0844,
      "step": 3604
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.0283720251172781,
      "learning_rate": 0.004997919267582189,
      "loss": 0.1625,
      "step": 3605
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.029061656445264816,
      "learning_rate": 0.004993757802746567,
      "loss": 0.155,
      "step": 3606
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.019039610400795937,
      "learning_rate": 0.004989596337910945,
      "loss": 0.0587,
      "step": 3607
    },
    {
      "epoch": 4.5,
      "grad_norm": 0.019895419478416443,
      "learning_rate": 0.0049854348730753225,
      "loss": 0.0673,
      "step": 3608
    },
    {
      "epoch": 4.51,
      "grad_norm": 0.029275763779878616,
      "learning_rate": 0.0049812734082397,
      "loss": 0.174,
      "step": 3609
    },
    {
      "epoch": 4.51,
      "grad_norm": 0.016538603231310844,
      "learning_rate": 0.004977111943404078,
      "loss": 0.1213,
      "step": 3610
    },
    {
      "epoch": 4.51,
      "grad_norm": 0.019129563122987747,
      "learning_rate": 0.004972950478568456,
      "loss": 0.0854,
      "step": 3611
    },
    {
      "epoch": 4.51,
      "grad_norm": 0.03208283334970474,
      "learning_rate": 0.004968789013732834,
      "loss": 0.1959,
      "step": 3612
    },
    {
      "epoch": 4.51,
      "grad_norm": 0.03174670785665512,
      "learning_rate": 0.004964627548897212,
      "loss": 0.1906,
      "step": 3613
    },
    {
      "epoch": 4.51,
      "grad_norm": 0.014462238177657127,
      "learning_rate": 0.00496046608406159,
      "loss": 0.0412,
      "step": 3614
    },
    {
      "epoch": 4.51,
      "grad_norm": 0.01002372708171606,
      "learning_rate": 0.004956304619225967,
      "loss": 0.0274,
      "step": 3615
    },
    {
      "epoch": 4.51,
      "grad_norm": 0.011578231118619442,
      "learning_rate": 0.004952143154390346,
      "loss": 0.0394,
      "step": 3616
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.028818851336836815,
      "learning_rate": 0.004947981689554724,
      "loss": 0.1486,
      "step": 3617
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.011849245987832546,
      "learning_rate": 0.004943820224719101,
      "loss": 0.0284,
      "step": 3618
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.013325352221727371,
      "learning_rate": 0.0049396587598834795,
      "loss": 0.0388,
      "step": 3619
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.02058897539973259,
      "learning_rate": 0.004935497295047857,
      "loss": 0.105,
      "step": 3620
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.025792675092816353,
      "learning_rate": 0.004931335830212234,
      "loss": 0.0763,
      "step": 3621
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.017821822315454483,
      "learning_rate": 0.004927174365376613,
      "loss": 0.0969,
      "step": 3622
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.0233574528247118,
      "learning_rate": 0.004923012900540991,
      "loss": 0.1208,
      "step": 3623
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.021786298602819443,
      "learning_rate": 0.004918851435705369,
      "loss": 0.1226,
      "step": 3624
    },
    {
      "epoch": 4.53,
      "grad_norm": 0.030711794272065163,
      "learning_rate": 0.0049146899708697464,
      "loss": 0.161,
      "step": 3625
    },
    {
      "epoch": 4.53,
      "grad_norm": 0.01452828198671341,
      "learning_rate": 0.004910528506034124,
      "loss": 0.0613,
      "step": 3626
    },
    {
      "epoch": 4.53,
      "grad_norm": 0.024379760026931763,
      "learning_rate": 0.004906367041198502,
      "loss": 0.0767,
      "step": 3627
    },
    {
      "epoch": 4.53,
      "grad_norm": 0.01686566323041916,
      "learning_rate": 0.00490220557636288,
      "loss": 0.0892,
      "step": 3628
    },
    {
      "epoch": 4.53,
      "grad_norm": 0.01961177960038185,
      "learning_rate": 0.004898044111527258,
      "loss": 0.0403,
      "step": 3629
    },
    {
      "epoch": 4.53,
      "grad_norm": 0.027433784678578377,
      "learning_rate": 0.004893882646691636,
      "loss": 0.1301,
      "step": 3630
    },
    {
      "epoch": 4.53,
      "grad_norm": 0.014682114124298096,
      "learning_rate": 0.004889721181856013,
      "loss": 0.0196,
      "step": 3631
    },
    {
      "epoch": 4.53,
      "grad_norm": 0.020747877657413483,
      "learning_rate": 0.004885559717020391,
      "loss": 0.1327,
      "step": 3632
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.03035859763622284,
      "learning_rate": 0.004881398252184769,
      "loss": 0.302,
      "step": 3633
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.021577339619398117,
      "learning_rate": 0.004877236787349147,
      "loss": 0.3083,
      "step": 3634
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.021573161706328392,
      "learning_rate": 0.004873075322513525,
      "loss": 0.1497,
      "step": 3635
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.039449237287044525,
      "learning_rate": 0.0048689138576779025,
      "loss": 0.3667,
      "step": 3636
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.030936691910028458,
      "learning_rate": 0.00486475239284228,
      "loss": 0.2556,
      "step": 3637
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.017130330204963684,
      "learning_rate": 0.004860590928006658,
      "loss": 0.1088,
      "step": 3638
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.02256993018090725,
      "learning_rate": 0.004856429463171037,
      "loss": 0.1043,
      "step": 3639
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.016301125288009644,
      "learning_rate": 0.004852267998335414,
      "loss": 0.056,
      "step": 3640
    },
    {
      "epoch": 4.55,
      "grad_norm": 0.020364802330732346,
      "learning_rate": 0.004848106533499792,
      "loss": 0.1317,
      "step": 3641
    },
    {
      "epoch": 4.55,
      "grad_norm": 0.026647673919796944,
      "learning_rate": 0.00484394506866417,
      "loss": 0.1588,
      "step": 3642
    },
    {
      "epoch": 4.55,
      "grad_norm": 0.01918039470911026,
      "learning_rate": 0.004839783603828547,
      "loss": 0.0562,
      "step": 3643
    },
    {
      "epoch": 4.55,
      "grad_norm": 0.03454625606536865,
      "learning_rate": 0.004835622138992925,
      "loss": 0.1737,
      "step": 3644
    },
    {
      "epoch": 4.55,
      "grad_norm": 0.01826007105410099,
      "learning_rate": 0.004831460674157304,
      "loss": 0.0677,
      "step": 3645
    },
    {
      "epoch": 4.55,
      "grad_norm": 0.021406739950180054,
      "learning_rate": 0.004827299209321681,
      "loss": 0.1444,
      "step": 3646
    },
    {
      "epoch": 4.55,
      "grad_norm": 0.029388144612312317,
      "learning_rate": 0.0048231377444860595,
      "loss": 0.1182,
      "step": 3647
    },
    {
      "epoch": 4.55,
      "grad_norm": 0.03573416918516159,
      "learning_rate": 0.004818976279650437,
      "loss": 0.2593,
      "step": 3648
    },
    {
      "epoch": 4.56,
      "grad_norm": 0.02017734944820404,
      "learning_rate": 0.004814814814814814,
      "loss": 0.0975,
      "step": 3649
    },
    {
      "epoch": 4.56,
      "grad_norm": 0.0001137309882324189,
      "learning_rate": 0.004810653349979193,
      "loss": 0.0001,
      "step": 3650
    },
    {
      "epoch": 4.56,
      "grad_norm": 0.03499489277601242,
      "learning_rate": 0.004806491885143571,
      "loss": 0.1781,
      "step": 3651
    },
    {
      "epoch": 4.56,
      "grad_norm": 0.03355468437075615,
      "learning_rate": 0.004802330420307949,
      "loss": 0.261,
      "step": 3652
    },
    {
      "epoch": 4.56,
      "grad_norm": 0.021639008074998856,
      "learning_rate": 0.0047981689554723265,
      "loss": 0.1345,
      "step": 3653
    },
    {
      "epoch": 4.56,
      "grad_norm": 0.011132907122373581,
      "learning_rate": 0.004794007490636704,
      "loss": 0.0185,
      "step": 3654
    },
    {
      "epoch": 4.56,
      "grad_norm": 0.03355347737669945,
      "learning_rate": 0.004789846025801082,
      "loss": 0.1832,
      "step": 3655
    },
    {
      "epoch": 4.56,
      "grad_norm": 0.012600651942193508,
      "learning_rate": 0.00478568456096546,
      "loss": 0.0229,
      "step": 3656
    },
    {
      "epoch": 4.57,
      "grad_norm": 0.011292017996311188,
      "learning_rate": 0.004781523096129838,
      "loss": 0.0356,
      "step": 3657
    },
    {
      "epoch": 4.57,
      "grad_norm": 0.020422108471393585,
      "learning_rate": 0.004777361631294216,
      "loss": 0.0361,
      "step": 3658
    },
    {
      "epoch": 4.57,
      "grad_norm": 0.021264856681227684,
      "learning_rate": 0.0047732001664585934,
      "loss": 0.0887,
      "step": 3659
    },
    {
      "epoch": 4.57,
      "grad_norm": 0.023063210770487785,
      "learning_rate": 0.004769038701622971,
      "loss": 0.1135,
      "step": 3660
    },
    {
      "epoch": 4.57,
      "grad_norm": 0.03165540471673012,
      "learning_rate": 0.004764877236787349,
      "loss": 0.1175,
      "step": 3661
    },
    {
      "epoch": 4.57,
      "grad_norm": 0.008452076464891434,
      "learning_rate": 0.004760715771951727,
      "loss": 0.0118,
      "step": 3662
    },
    {
      "epoch": 4.57,
      "grad_norm": 0.03761233389377594,
      "learning_rate": 0.004756554307116105,
      "loss": 0.1929,
      "step": 3663
    },
    {
      "epoch": 4.57,
      "grad_norm": 0.020118772983551025,
      "learning_rate": 0.004752392842280483,
      "loss": 0.09,
      "step": 3664
    },
    {
      "epoch": 4.58,
      "grad_norm": 0.0006301498506218195,
      "learning_rate": 0.00474823137744486,
      "loss": 0.0005,
      "step": 3665
    },
    {
      "epoch": 4.58,
      "grad_norm": 0.03220965340733528,
      "learning_rate": 0.004744069912609238,
      "loss": 0.1779,
      "step": 3666
    },
    {
      "epoch": 4.58,
      "grad_norm": 0.0353519581258297,
      "learning_rate": 0.004739908447773617,
      "loss": 0.4148,
      "step": 3667
    },
    {
      "epoch": 4.58,
      "grad_norm": 0.017051268368959427,
      "learning_rate": 0.004735746982937994,
      "loss": 0.0859,
      "step": 3668
    },
    {
      "epoch": 4.58,
      "grad_norm": 6.935007695574313e-05,
      "learning_rate": 0.004731585518102372,
      "loss": 0.0001,
      "step": 3669
    },
    {
      "epoch": 4.58,
      "grad_norm": 0.035935599356889725,
      "learning_rate": 0.00472742405326675,
      "loss": 0.4976,
      "step": 3670
    },
    {
      "epoch": 4.58,
      "grad_norm": 0.025818629190325737,
      "learning_rate": 0.004723262588431128,
      "loss": 0.2493,
      "step": 3671
    },
    {
      "epoch": 4.58,
      "grad_norm": 0.01031804271042347,
      "learning_rate": 0.004719101123595505,
      "loss": 0.0181,
      "step": 3672
    },
    {
      "epoch": 4.59,
      "grad_norm": 0.017328312620520592,
      "learning_rate": 0.004714939658759884,
      "loss": 0.0693,
      "step": 3673
    },
    {
      "epoch": 4.59,
      "grad_norm": 0.04279383271932602,
      "learning_rate": 0.004710778193924262,
      "loss": 0.0926,
      "step": 3674
    },
    {
      "epoch": 4.59,
      "grad_norm": 0.029655752703547478,
      "learning_rate": 0.0047066167290886396,
      "loss": 0.1941,
      "step": 3675
    },
    {
      "epoch": 4.59,
      "grad_norm": 0.0037502646446228027,
      "learning_rate": 0.004702455264253017,
      "loss": 0.0046,
      "step": 3676
    },
    {
      "epoch": 4.59,
      "grad_norm": 0.02430540695786476,
      "learning_rate": 0.004698293799417395,
      "loss": 0.2471,
      "step": 3677
    },
    {
      "epoch": 4.59,
      "grad_norm": 0.031773004680871964,
      "learning_rate": 0.004694132334581773,
      "loss": 0.1595,
      "step": 3678
    },
    {
      "epoch": 4.59,
      "grad_norm": 0.02574041672050953,
      "learning_rate": 0.004689970869746151,
      "loss": 0.0683,
      "step": 3679
    },
    {
      "epoch": 4.59,
      "grad_norm": 5.402154056355357e-05,
      "learning_rate": 0.004685809404910529,
      "loss": 0.0001,
      "step": 3680
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.013639169745147228,
      "learning_rate": 0.0046816479400749065,
      "loss": 0.0421,
      "step": 3681
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.01551780104637146,
      "learning_rate": 0.004677486475239284,
      "loss": 0.0406,
      "step": 3682
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.026053208857774734,
      "learning_rate": 0.004673325010403662,
      "loss": 0.166,
      "step": 3683
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.01662847027182579,
      "learning_rate": 0.00466916354556804,
      "loss": 0.0451,
      "step": 3684
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.01889331080019474,
      "learning_rate": 0.004665002080732418,
      "loss": 0.0659,
      "step": 3685
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.02732817828655243,
      "learning_rate": 0.004660840615896796,
      "loss": 0.1273,
      "step": 3686
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.024991203099489212,
      "learning_rate": 0.0046566791510611735,
      "loss": 0.1926,
      "step": 3687
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.01438846718519926,
      "learning_rate": 0.004652517686225551,
      "loss": 0.0469,
      "step": 3688
    },
    {
      "epoch": 4.61,
      "grad_norm": 0.025508658960461617,
      "learning_rate": 0.004648356221389929,
      "loss": 0.0881,
      "step": 3689
    },
    {
      "epoch": 4.61,
      "grad_norm": 0.02420109324157238,
      "learning_rate": 0.004644194756554308,
      "loss": 0.1558,
      "step": 3690
    },
    {
      "epoch": 4.61,
      "grad_norm": 0.01517496071755886,
      "learning_rate": 0.004640033291718685,
      "loss": 0.0544,
      "step": 3691
    },
    {
      "epoch": 4.61,
      "grad_norm": 0.020442450419068336,
      "learning_rate": 0.004635871826883063,
      "loss": 0.1044,
      "step": 3692
    },
    {
      "epoch": 4.61,
      "grad_norm": 0.020956585183739662,
      "learning_rate": 0.004631710362047441,
      "loss": 0.0699,
      "step": 3693
    },
    {
      "epoch": 4.61,
      "grad_norm": 0.036702413111925125,
      "learning_rate": 0.004627548897211818,
      "loss": 0.1277,
      "step": 3694
    },
    {
      "epoch": 4.61,
      "grad_norm": 0.04161643981933594,
      "learning_rate": 0.004623387432376196,
      "loss": 0.3213,
      "step": 3695
    },
    {
      "epoch": 4.61,
      "grad_norm": 0.03438429534435272,
      "learning_rate": 0.004619225967540575,
      "loss": 0.187,
      "step": 3696
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.01901412382721901,
      "learning_rate": 0.004615064502704952,
      "loss": 0.1183,
      "step": 3697
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.028745805844664574,
      "learning_rate": 0.0046109030378693305,
      "loss": 0.1915,
      "step": 3698
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.03177424892783165,
      "learning_rate": 0.004606741573033708,
      "loss": 0.2053,
      "step": 3699
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.015612061135470867,
      "learning_rate": 0.004602580108198085,
      "loss": 0.0767,
      "step": 3700
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.013517406769096851,
      "learning_rate": 0.004598418643362464,
      "loss": 0.0247,
      "step": 3701
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.014232104644179344,
      "learning_rate": 0.004594257178526842,
      "loss": 0.032,
      "step": 3702
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.03407784178853035,
      "learning_rate": 0.004590095713691219,
      "loss": 0.1785,
      "step": 3703
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.031451426446437836,
      "learning_rate": 0.004585934248855597,
      "loss": 0.1675,
      "step": 3704
    },
    {
      "epoch": 4.63,
      "grad_norm": 0.00018052494851872325,
      "learning_rate": 0.004581772784019975,
      "loss": 0.0002,
      "step": 3705
    },
    {
      "epoch": 4.63,
      "grad_norm": 0.016298580914735794,
      "learning_rate": 0.004577611319184353,
      "loss": 0.0428,
      "step": 3706
    },
    {
      "epoch": 4.63,
      "grad_norm": 0.01851016841828823,
      "learning_rate": 0.004573449854348731,
      "loss": 0.0938,
      "step": 3707
    },
    {
      "epoch": 4.63,
      "grad_norm": 0.031151654198765755,
      "learning_rate": 0.004569288389513109,
      "loss": 0.1329,
      "step": 3708
    },
    {
      "epoch": 4.63,
      "grad_norm": 0.028027037158608437,
      "learning_rate": 0.004565126924677487,
      "loss": 0.1633,
      "step": 3709
    },
    {
      "epoch": 4.63,
      "grad_norm": 0.027260899543762207,
      "learning_rate": 0.004560965459841864,
      "loss": 0.1169,
      "step": 3710
    },
    {
      "epoch": 4.63,
      "grad_norm": 0.025173908099532127,
      "learning_rate": 0.004556803995006242,
      "loss": 0.0954,
      "step": 3711
    },
    {
      "epoch": 4.63,
      "grad_norm": 0.019175227731466293,
      "learning_rate": 0.00455264253017062,
      "loss": 0.0956,
      "step": 3712
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.001076320419088006,
      "learning_rate": 0.004548481065334998,
      "loss": 0.001,
      "step": 3713
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.015859335660934448,
      "learning_rate": 0.004544319600499376,
      "loss": 0.0704,
      "step": 3714
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.019564645364880562,
      "learning_rate": 0.0045401581356637535,
      "loss": 0.0439,
      "step": 3715
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.03147464618086815,
      "learning_rate": 0.004535996670828131,
      "loss": 0.1754,
      "step": 3716
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.02037680707871914,
      "learning_rate": 0.004531835205992509,
      "loss": 0.1128,
      "step": 3717
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.022228559479117393,
      "learning_rate": 0.004527673741156888,
      "loss": 0.2043,
      "step": 3718
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.02941538207232952,
      "learning_rate": 0.004523512276321265,
      "loss": 0.1758,
      "step": 3719
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.04001694172620773,
      "learning_rate": 0.004519350811485643,
      "loss": 0.2178,
      "step": 3720
    },
    {
      "epoch": 4.65,
      "grad_norm": 0.023271504789590836,
      "learning_rate": 0.004515189346650021,
      "loss": 0.0674,
      "step": 3721
    },
    {
      "epoch": 4.65,
      "grad_norm": 0.02318800613284111,
      "learning_rate": 0.004511027881814398,
      "loss": 0.2761,
      "step": 3722
    },
    {
      "epoch": 4.65,
      "grad_norm": 0.0002199652517447248,
      "learning_rate": 0.004506866416978776,
      "loss": 0.0002,
      "step": 3723
    },
    {
      "epoch": 4.65,
      "grad_norm": 0.0031018725130707026,
      "learning_rate": 0.004502704952143155,
      "loss": 0.0021,
      "step": 3724
    },
    {
      "epoch": 4.65,
      "grad_norm": 0.037195757031440735,
      "learning_rate": 0.004498543487307533,
      "loss": 0.1259,
      "step": 3725
    },
    {
      "epoch": 4.65,
      "grad_norm": 0.014490136876702309,
      "learning_rate": 0.0044943820224719105,
      "loss": 0.0812,
      "step": 3726
    },
    {
      "epoch": 4.65,
      "grad_norm": 0.024151204153895378,
      "learning_rate": 0.004490220557636288,
      "loss": 0.1058,
      "step": 3727
    },
    {
      "epoch": 4.65,
      "grad_norm": 0.02724933624267578,
      "learning_rate": 0.004486059092800666,
      "loss": 0.1221,
      "step": 3728
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.029313204810023308,
      "learning_rate": 0.004481897627965044,
      "loss": 0.1781,
      "step": 3729
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.031722549349069595,
      "learning_rate": 0.004477736163129422,
      "loss": 0.1687,
      "step": 3730
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.03065621107816696,
      "learning_rate": 0.0044735746982938,
      "loss": 0.1993,
      "step": 3731
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.016407614573836327,
      "learning_rate": 0.0044694132334581775,
      "loss": 0.0583,
      "step": 3732
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.02332237735390663,
      "learning_rate": 0.004465251768622555,
      "loss": 0.1365,
      "step": 3733
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.00515657477080822,
      "learning_rate": 0.004461090303786933,
      "loss": 0.0073,
      "step": 3734
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.013532347977161407,
      "learning_rate": 0.004456928838951311,
      "loss": 0.0498,
      "step": 3735
    },
    {
      "epoch": 4.66,
      "grad_norm": 0.008313730359077454,
      "learning_rate": 0.004452767374115689,
      "loss": 0.024,
      "step": 3736
    },
    {
      "epoch": 4.67,
      "grad_norm": 0.015343270264565945,
      "learning_rate": 0.004448605909280067,
      "loss": 0.0256,
      "step": 3737
    },
    {
      "epoch": 4.67,
      "grad_norm": 0.010586454533040524,
      "learning_rate": 0.0044444444444444444,
      "loss": 0.0276,
      "step": 3738
    },
    {
      "epoch": 4.67,
      "grad_norm": 0.01994352601468563,
      "learning_rate": 0.004440282979608822,
      "loss": 0.1027,
      "step": 3739
    },
    {
      "epoch": 4.67,
      "grad_norm": 0.021923227235674858,
      "learning_rate": 0.0044361215147732,
      "loss": 0.1604,
      "step": 3740
    },
    {
      "epoch": 4.67,
      "grad_norm": 0.021603481844067574,
      "learning_rate": 0.004431960049937578,
      "loss": 0.1788,
      "step": 3741
    },
    {
      "epoch": 4.67,
      "grad_norm": 0.02131359837949276,
      "learning_rate": 0.004427798585101956,
      "loss": 0.0927,
      "step": 3742
    },
    {
      "epoch": 4.67,
      "grad_norm": 0.024806197732686996,
      "learning_rate": 0.004423637120266334,
      "loss": 0.1713,
      "step": 3743
    },
    {
      "epoch": 4.67,
      "grad_norm": 0.044741418212652206,
      "learning_rate": 0.004419475655430712,
      "loss": 0.2311,
      "step": 3744
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.026623202487826347,
      "learning_rate": 0.004415314190595089,
      "loss": 0.1857,
      "step": 3745
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.022661341354250908,
      "learning_rate": 0.004411152725759467,
      "loss": 0.0963,
      "step": 3746
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.027787886559963226,
      "learning_rate": 0.004406991260923846,
      "loss": 0.1058,
      "step": 3747
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.007160007953643799,
      "learning_rate": 0.004402829796088223,
      "loss": 0.0146,
      "step": 3748
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.01621672511100769,
      "learning_rate": 0.004398668331252601,
      "loss": 0.0601,
      "step": 3749
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.00032911106245592237,
      "learning_rate": 0.004394506866416979,
      "loss": 0.0004,
      "step": 3750
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.03167753294110298,
      "learning_rate": 0.004390345401581356,
      "loss": 0.2031,
      "step": 3751
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.023577773943543434,
      "learning_rate": 0.004386183936745735,
      "loss": 0.1439,
      "step": 3752
    },
    {
      "epoch": 4.69,
      "grad_norm": 0.022353732958436012,
      "learning_rate": 0.004382022471910113,
      "loss": 0.1464,
      "step": 3753
    },
    {
      "epoch": 4.69,
      "grad_norm": 0.02158292941749096,
      "learning_rate": 0.00437786100707449,
      "loss": 0.0841,
      "step": 3754
    },
    {
      "epoch": 4.69,
      "grad_norm": 0.026600874960422516,
      "learning_rate": 0.004373699542238868,
      "loss": 0.1812,
      "step": 3755
    },
    {
      "epoch": 4.69,
      "grad_norm": 0.023556886240839958,
      "learning_rate": 0.004369538077403246,
      "loss": 0.134,
      "step": 3756
    },
    {
      "epoch": 4.69,
      "grad_norm": 0.02296127751469612,
      "learning_rate": 0.004365376612567624,
      "loss": 0.0497,
      "step": 3757
    },
    {
      "epoch": 4.69,
      "grad_norm": 0.020596498623490334,
      "learning_rate": 0.004361215147732002,
      "loss": 0.2267,
      "step": 3758
    },
    {
      "epoch": 4.69,
      "grad_norm": 0.02567685768008232,
      "learning_rate": 0.00435705368289638,
      "loss": 0.1702,
      "step": 3759
    },
    {
      "epoch": 4.69,
      "grad_norm": 0.026590317487716675,
      "learning_rate": 0.0043528922180607575,
      "loss": 0.0672,
      "step": 3760
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.007286667823791504,
      "learning_rate": 0.004348730753225135,
      "loss": 0.0121,
      "step": 3761
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.03430037200450897,
      "learning_rate": 0.004344569288389513,
      "loss": 0.293,
      "step": 3762
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.026628848165273666,
      "learning_rate": 0.004340407823553891,
      "loss": 0.09,
      "step": 3763
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.014065890572965145,
      "learning_rate": 0.004336246358718269,
      "loss": 0.025,
      "step": 3764
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.036070022732019424,
      "learning_rate": 0.004332084893882647,
      "loss": 0.2227,
      "step": 3765
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.01747623272240162,
      "learning_rate": 0.0043279234290470245,
      "loss": 0.0391,
      "step": 3766
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.02310761623084545,
      "learning_rate": 0.004323761964211402,
      "loss": 0.1328,
      "step": 3767
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.030022310093045235,
      "learning_rate": 0.00431960049937578,
      "loss": 0.2404,
      "step": 3768
    },
    {
      "epoch": 4.71,
      "grad_norm": 0.023599162697792053,
      "learning_rate": 0.004315439034540159,
      "loss": 0.1207,
      "step": 3769
    },
    {
      "epoch": 4.71,
      "grad_norm": 0.02848854660987854,
      "learning_rate": 0.004311277569704536,
      "loss": 0.0429,
      "step": 3770
    },
    {
      "epoch": 4.71,
      "grad_norm": 0.00028597289929166436,
      "learning_rate": 0.004307116104868914,
      "loss": 0.0002,
      "step": 3771
    },
    {
      "epoch": 4.71,
      "grad_norm": 0.01564766652882099,
      "learning_rate": 0.004302954640033292,
      "loss": 0.0678,
      "step": 3772
    },
    {
      "epoch": 4.71,
      "grad_norm": 0.022790059447288513,
      "learning_rate": 0.004298793175197669,
      "loss": 0.1565,
      "step": 3773
    },
    {
      "epoch": 4.71,
      "grad_norm": 0.022283414378762245,
      "learning_rate": 0.004294631710362047,
      "loss": 0.1103,
      "step": 3774
    },
    {
      "epoch": 4.71,
      "grad_norm": 0.01799558661878109,
      "learning_rate": 0.004290470245526426,
      "loss": 0.0341,
      "step": 3775
    },
    {
      "epoch": 4.71,
      "grad_norm": 0.02295808121562004,
      "learning_rate": 0.004286308780690803,
      "loss": 0.1146,
      "step": 3776
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.004384818021208048,
      "learning_rate": 0.0042821473158551815,
      "loss": 0.0078,
      "step": 3777
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.02127508632838726,
      "learning_rate": 0.004277985851019559,
      "loss": 0.1401,
      "step": 3778
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.03308796510100365,
      "learning_rate": 0.004273824386183937,
      "loss": 0.0728,
      "step": 3779
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.014834375120699406,
      "learning_rate": 0.004269662921348315,
      "loss": 0.0432,
      "step": 3780
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.018423009663820267,
      "learning_rate": 0.004265501456512693,
      "loss": 0.0758,
      "step": 3781
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.022583631798624992,
      "learning_rate": 0.004261339991677071,
      "loss": 0.0578,
      "step": 3782
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.022195709869265556,
      "learning_rate": 0.004257178526841448,
      "loss": 0.129,
      "step": 3783
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.022951889783143997,
      "learning_rate": 0.004253017062005826,
      "loss": 0.1656,
      "step": 3784
    },
    {
      "epoch": 4.73,
      "grad_norm": 0.02452574484050274,
      "learning_rate": 0.004248855597170204,
      "loss": 0.1014,
      "step": 3785
    },
    {
      "epoch": 4.73,
      "grad_norm": 0.03493499010801315,
      "learning_rate": 0.004244694132334582,
      "loss": 0.3235,
      "step": 3786
    },
    {
      "epoch": 4.73,
      "grad_norm": 0.022447481751441956,
      "learning_rate": 0.00424053266749896,
      "loss": 0.0579,
      "step": 3787
    },
    {
      "epoch": 4.73,
      "grad_norm": 0.006579990033060312,
      "learning_rate": 0.0042363712026633376,
      "loss": 0.0066,
      "step": 3788
    },
    {
      "epoch": 4.73,
      "grad_norm": 0.033310845494270325,
      "learning_rate": 0.004232209737827715,
      "loss": 0.1392,
      "step": 3789
    },
    {
      "epoch": 4.73,
      "grad_norm": 0.02406691201031208,
      "learning_rate": 0.004228048272992093,
      "loss": 0.0571,
      "step": 3790
    },
    {
      "epoch": 4.73,
      "grad_norm": 0.027935955673456192,
      "learning_rate": 0.004223886808156471,
      "loss": 0.1438,
      "step": 3791
    },
    {
      "epoch": 4.73,
      "grad_norm": 0.03152600675821304,
      "learning_rate": 0.004219725343320849,
      "loss": 0.2175,
      "step": 3792
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.03285421431064606,
      "learning_rate": 0.004215563878485227,
      "loss": 0.2482,
      "step": 3793
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.030818384140729904,
      "learning_rate": 0.0042114024136496045,
      "loss": 0.1747,
      "step": 3794
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.028618281707167625,
      "learning_rate": 0.004207240948813982,
      "loss": 0.2266,
      "step": 3795
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.010984188877046108,
      "learning_rate": 0.00420307948397836,
      "loss": 0.0245,
      "step": 3796
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.025985857471823692,
      "learning_rate": 0.004198918019142738,
      "loss": 0.1298,
      "step": 3797
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.026163440197706223,
      "learning_rate": 0.004194756554307117,
      "loss": 0.1976,
      "step": 3798
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.028552288189530373,
      "learning_rate": 0.004190595089471494,
      "loss": 0.2255,
      "step": 3799
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.020759889855980873,
      "learning_rate": 0.004186433624635872,
      "loss": 0.0488,
      "step": 3800
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.03662974014878273,
      "learning_rate": 0.00418227215980025,
      "loss": 0.1525,
      "step": 3801
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.009451269172132015,
      "learning_rate": 0.004178110694964627,
      "loss": 0.0161,
      "step": 3802
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.021042076870799065,
      "learning_rate": 0.004173949230129006,
      "loss": 0.1635,
      "step": 3803
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.0227289330214262,
      "learning_rate": 0.004169787765293384,
      "loss": 0.0939,
      "step": 3804
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.02973376214504242,
      "learning_rate": 0.004165626300457761,
      "loss": 0.2605,
      "step": 3805
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.01644456572830677,
      "learning_rate": 0.004161464835622139,
      "loss": 0.0981,
      "step": 3806
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.017018035054206848,
      "learning_rate": 0.004157303370786517,
      "loss": 0.1093,
      "step": 3807
    },
    {
      "epoch": 4.75,
      "grad_norm": 0.02431766502559185,
      "learning_rate": 0.004153141905950895,
      "loss": 0.1339,
      "step": 3808
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.029722191393375397,
      "learning_rate": 0.004148980441115273,
      "loss": 0.0728,
      "step": 3809
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.015540544874966145,
      "learning_rate": 0.004144818976279651,
      "loss": 0.0352,
      "step": 3810
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.02115580067038536,
      "learning_rate": 0.0041406575114440285,
      "loss": 0.1292,
      "step": 3811
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.021426508203148842,
      "learning_rate": 0.004136496046608406,
      "loss": 0.1194,
      "step": 3812
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.026626301929354668,
      "learning_rate": 0.004132334581772784,
      "loss": 0.161,
      "step": 3813
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.011887550354003906,
      "learning_rate": 0.004128173116937162,
      "loss": 0.0156,
      "step": 3814
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.029506193473935127,
      "learning_rate": 0.00412401165210154,
      "loss": 0.1313,
      "step": 3815
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.019974948838353157,
      "learning_rate": 0.004119850187265918,
      "loss": 0.0802,
      "step": 3816
    },
    {
      "epoch": 4.77,
      "grad_norm": 0.039263736456632614,
      "learning_rate": 0.004115688722430295,
      "loss": 0.2617,
      "step": 3817
    },
    {
      "epoch": 4.77,
      "grad_norm": 0.00017813539307098836,
      "learning_rate": 0.004111527257594673,
      "loss": 0.0002,
      "step": 3818
    },
    {
      "epoch": 4.77,
      "grad_norm": 0.029021788388490677,
      "learning_rate": 0.004107365792759051,
      "loss": 0.2064,
      "step": 3819
    },
    {
      "epoch": 4.77,
      "grad_norm": 0.028962431475520134,
      "learning_rate": 0.004103204327923429,
      "loss": 0.2041,
      "step": 3820
    },
    {
      "epoch": 4.77,
      "grad_norm": 0.03665093705058098,
      "learning_rate": 0.004099042863087807,
      "loss": 0.1802,
      "step": 3821
    },
    {
      "epoch": 4.77,
      "grad_norm": 0.032415807247161865,
      "learning_rate": 0.004094881398252185,
      "loss": 0.2308,
      "step": 3822
    },
    {
      "epoch": 4.77,
      "grad_norm": 0.01668584905564785,
      "learning_rate": 0.004090719933416563,
      "loss": 0.064,
      "step": 3823
    },
    {
      "epoch": 4.77,
      "grad_norm": 0.03401903063058853,
      "learning_rate": 0.00408655846858094,
      "loss": 0.0822,
      "step": 3824
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.025740619748830795,
      "learning_rate": 0.004082397003745318,
      "loss": 0.269,
      "step": 3825
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.02910546399652958,
      "learning_rate": 0.004078235538909697,
      "loss": 0.1943,
      "step": 3826
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.01961277611553669,
      "learning_rate": 0.004074074074074074,
      "loss": 0.0668,
      "step": 3827
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.03933838754892349,
      "learning_rate": 0.0040699126092384515,
      "loss": 0.1824,
      "step": 3828
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.0157929677516222,
      "learning_rate": 0.00406575114440283,
      "loss": 0.0271,
      "step": 3829
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.020523566752672195,
      "learning_rate": 0.004061589679567207,
      "loss": 0.1135,
      "step": 3830
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.028131602331995964,
      "learning_rate": 0.004057428214731586,
      "loss": 0.2142,
      "step": 3831
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.03036409802734852,
      "learning_rate": 0.004053266749895964,
      "loss": 0.2151,
      "step": 3832
    },
    {
      "epoch": 4.79,
      "grad_norm": 0.035154491662979126,
      "learning_rate": 0.0040491052850603415,
      "loss": 0.1356,
      "step": 3833
    },
    {
      "epoch": 4.79,
      "grad_norm": 0.03311875835061073,
      "learning_rate": 0.004044943820224719,
      "loss": 0.2491,
      "step": 3834
    },
    {
      "epoch": 4.79,
      "grad_norm": 0.020199492573738098,
      "learning_rate": 0.004040782355389097,
      "loss": 0.072,
      "step": 3835
    },
    {
      "epoch": 4.79,
      "grad_norm": 0.015663281083106995,
      "learning_rate": 0.004036620890553475,
      "loss": 0.037,
      "step": 3836
    },
    {
      "epoch": 4.79,
      "grad_norm": 0.030800262466073036,
      "learning_rate": 0.004032459425717853,
      "loss": 0.1115,
      "step": 3837
    },
    {
      "epoch": 4.79,
      "grad_norm": 0.03276811167597771,
      "learning_rate": 0.004028297960882231,
      "loss": 0.2377,
      "step": 3838
    },
    {
      "epoch": 4.79,
      "grad_norm": 0.026386680081486702,
      "learning_rate": 0.0040241364960466085,
      "loss": 0.1109,
      "step": 3839
    },
    {
      "epoch": 4.79,
      "grad_norm": 0.021614952012896538,
      "learning_rate": 0.004019975031210986,
      "loss": 0.1205,
      "step": 3840
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.02319277822971344,
      "learning_rate": 0.004015813566375364,
      "loss": 0.1103,
      "step": 3841
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.04137790575623512,
      "learning_rate": 0.004011652101539742,
      "loss": 0.3777,
      "step": 3842
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.009659744799137115,
      "learning_rate": 0.00400749063670412,
      "loss": 0.017,
      "step": 3843
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.018543502315878868,
      "learning_rate": 0.004003329171868498,
      "loss": 0.0487,
      "step": 3844
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.02139715664088726,
      "learning_rate": 0.0039991677070328755,
      "loss": 0.0692,
      "step": 3845
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.030826430767774582,
      "learning_rate": 0.003995006242197253,
      "loss": 0.2502,
      "step": 3846
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.020539553835988045,
      "learning_rate": 0.003990844777361631,
      "loss": 0.0617,
      "step": 3847
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.017524290829896927,
      "learning_rate": 0.003986683312526009,
      "loss": 0.1016,
      "step": 3848
    },
    {
      "epoch": 4.81,
      "grad_norm": 0.025506293401122093,
      "learning_rate": 0.003982521847690387,
      "loss": 0.116,
      "step": 3849
    },
    {
      "epoch": 4.81,
      "grad_norm": 0.02244599536061287,
      "learning_rate": 0.003978360382854765,
      "loss": 0.0879,
      "step": 3850
    },
    {
      "epoch": 4.81,
      "grad_norm": 0.015553341247141361,
      "learning_rate": 0.003974198918019143,
      "loss": 0.0291,
      "step": 3851
    },
    {
      "epoch": 4.81,
      "grad_norm": 0.020014474168419838,
      "learning_rate": 0.003970037453183521,
      "loss": 0.0778,
      "step": 3852
    },
    {
      "epoch": 4.81,
      "grad_norm": 0.04979262873530388,
      "learning_rate": 0.003965875988347898,
      "loss": 0.2175,
      "step": 3853
    },
    {
      "epoch": 4.81,
      "grad_norm": 0.02103806473314762,
      "learning_rate": 0.003961714523512277,
      "loss": 0.101,
      "step": 3854
    },
    {
      "epoch": 4.81,
      "grad_norm": 0.004138020798563957,
      "learning_rate": 0.003957553058676655,
      "loss": 0.0039,
      "step": 3855
    },
    {
      "epoch": 4.81,
      "grad_norm": 0.025836830958724022,
      "learning_rate": 0.003953391593841032,
      "loss": 0.1809,
      "step": 3856
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.03301927074790001,
      "learning_rate": 0.00394923012900541,
      "loss": 0.1581,
      "step": 3857
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.02047717012465,
      "learning_rate": 0.003945068664169788,
      "loss": 0.1377,
      "step": 3858
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.023153556510806084,
      "learning_rate": 0.003940907199334166,
      "loss": 0.097,
      "step": 3859
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.025860631838440895,
      "learning_rate": 0.003936745734498544,
      "loss": 0.0871,
      "step": 3860
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.030921930447220802,
      "learning_rate": 0.003932584269662922,
      "loss": 0.2086,
      "step": 3861
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.025799350813031197,
      "learning_rate": 0.003928422804827299,
      "loss": 0.0795,
      "step": 3862
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.020559154450893402,
      "learning_rate": 0.003924261339991677,
      "loss": 0.0967,
      "step": 3863
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.026439499109983444,
      "learning_rate": 0.003920099875156055,
      "loss": 0.0997,
      "step": 3864
    },
    {
      "epoch": 4.83,
      "grad_norm": 0.028088673949241638,
      "learning_rate": 0.003915938410320433,
      "loss": 0.0208,
      "step": 3865
    },
    {
      "epoch": 4.83,
      "grad_norm": 0.029131729155778885,
      "learning_rate": 0.003911776945484811,
      "loss": 0.2786,
      "step": 3866
    },
    {
      "epoch": 4.83,
      "grad_norm": 0.0451950840651989,
      "learning_rate": 0.0039076154806491886,
      "loss": 0.3237,
      "step": 3867
    },
    {
      "epoch": 4.83,
      "grad_norm": 0.03126266598701477,
      "learning_rate": 0.0039034540158135664,
      "loss": 0.0972,
      "step": 3868
    },
    {
      "epoch": 4.83,
      "grad_norm": 0.029225101694464684,
      "learning_rate": 0.003899292550977944,
      "loss": 0.0531,
      "step": 3869
    },
    {
      "epoch": 4.83,
      "grad_norm": 0.022569460794329643,
      "learning_rate": 0.0038951310861423225,
      "loss": 0.1005,
      "step": 3870
    },
    {
      "epoch": 4.83,
      "grad_norm": 0.0326865129172802,
      "learning_rate": 0.0038909696213067003,
      "loss": 0.2644,
      "step": 3871
    },
    {
      "epoch": 4.83,
      "grad_norm": 0.026412997394800186,
      "learning_rate": 0.0038868081564710777,
      "loss": 0.1527,
      "step": 3872
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.021016474813222885,
      "learning_rate": 0.003882646691635456,
      "loss": 0.0716,
      "step": 3873
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.02494640275835991,
      "learning_rate": 0.0038784852267998338,
      "loss": 0.149,
      "step": 3874
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.021458113566040993,
      "learning_rate": 0.003874323761964211,
      "loss": 0.0743,
      "step": 3875
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.026171136647462845,
      "learning_rate": 0.0038701622971285894,
      "loss": 0.1632,
      "step": 3876
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.01598879136145115,
      "learning_rate": 0.0038660008322929673,
      "loss": 0.0315,
      "step": 3877
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.01615132763981819,
      "learning_rate": 0.003861839367457345,
      "loss": 0.0396,
      "step": 3878
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.026258274912834167,
      "learning_rate": 0.003857677902621723,
      "loss": 0.1429,
      "step": 3879
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.006579315755516291,
      "learning_rate": 0.0038535164377861007,
      "loss": 0.0047,
      "step": 3880
    },
    {
      "epoch": 4.85,
      "grad_norm": 0.03169470652937889,
      "learning_rate": 0.0038493549729504786,
      "loss": 0.2874,
      "step": 3881
    },
    {
      "epoch": 4.85,
      "grad_norm": 0.027188220992684364,
      "learning_rate": 0.0038451935081148564,
      "loss": 0.1797,
      "step": 3882
    },
    {
      "epoch": 4.85,
      "grad_norm": 0.0176385547965765,
      "learning_rate": 0.0038410320432792347,
      "loss": 0.1334,
      "step": 3883
    },
    {
      "epoch": 4.85,
      "grad_norm": 0.02594999596476555,
      "learning_rate": 0.003836870578443612,
      "loss": 0.0941,
      "step": 3884
    },
    {
      "epoch": 4.85,
      "grad_norm": 0.00042651090188883245,
      "learning_rate": 0.00383270911360799,
      "loss": 0.0003,
      "step": 3885
    },
    {
      "epoch": 4.85,
      "grad_norm": 0.027992811053991318,
      "learning_rate": 0.003828547648772368,
      "loss": 0.1696,
      "step": 3886
    },
    {
      "epoch": 4.85,
      "grad_norm": 0.007546452805399895,
      "learning_rate": 0.003824386183936746,
      "loss": 0.0053,
      "step": 3887
    },
    {
      "epoch": 4.85,
      "grad_norm": 0.030724087730050087,
      "learning_rate": 0.0038202247191011234,
      "loss": 0.2327,
      "step": 3888
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.029840288683772087,
      "learning_rate": 0.0038160632542655016,
      "loss": 0.1874,
      "step": 3889
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.0022953380830585957,
      "learning_rate": 0.0038119017894298795,
      "loss": 0.003,
      "step": 3890
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.014369924552738667,
      "learning_rate": 0.0038077403245942573,
      "loss": 0.0328,
      "step": 3891
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.020305801182985306,
      "learning_rate": 0.003803578859758635,
      "loss": 0.1642,
      "step": 3892
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.018595710396766663,
      "learning_rate": 0.0037994173949230134,
      "loss": 0.099,
      "step": 3893
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.023214174434542656,
      "learning_rate": 0.0037952559300873908,
      "loss": 0.2151,
      "step": 3894
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.03033781237900257,
      "learning_rate": 0.0037910944652517686,
      "loss": 0.3379,
      "step": 3895
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.01771094836294651,
      "learning_rate": 0.003786933000416147,
      "loss": 0.0837,
      "step": 3896
    },
    {
      "epoch": 4.87,
      "grad_norm": 0.02956162951886654,
      "learning_rate": 0.0037827715355805243,
      "loss": 0.1436,
      "step": 3897
    },
    {
      "epoch": 4.87,
      "grad_norm": 0.013264167122542858,
      "learning_rate": 0.003778610070744902,
      "loss": 0.02,
      "step": 3898
    },
    {
      "epoch": 4.87,
      "grad_norm": 0.03126860409975052,
      "learning_rate": 0.0037744486059092803,
      "loss": 0.322,
      "step": 3899
    },
    {
      "epoch": 4.87,
      "grad_norm": 0.027962638065218925,
      "learning_rate": 0.0037702871410736577,
      "loss": 0.222,
      "step": 3900
    },
    {
      "epoch": 4.87,
      "eval_loss": 0.24755859375,
      "eval_runtime": 183.0305,
      "eval_samples_per_second": 1.098,
      "eval_steps_per_second": 0.552,
      "step": 3900
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 4806,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 300,
  "total_flos": 4.487763801176801e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
