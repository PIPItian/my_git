{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.8726591760299627,
  "eval_steps": 300,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0,
      "grad_norm": 0.013946587219834328,
      "learning_rate": 0.01999583853516438,
      "loss": 1.1357,
      "step": 1
    },
    {
      "epoch": 0.0,
      "grad_norm": 0.03152783587574959,
      "learning_rate": 0.019991677070328756,
      "loss": 1.3057,
      "step": 2
    },
    {
      "epoch": 0.0,
      "grad_norm": 0.02430560812354088,
      "learning_rate": 0.019987515605493136,
      "loss": 1.5762,
      "step": 3
    },
    {
      "epoch": 0.0,
      "grad_norm": 0.03687096759676933,
      "learning_rate": 0.01998335414065751,
      "loss": 0.9321,
      "step": 4
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.026205476373434067,
      "learning_rate": 0.019979192675821888,
      "loss": 0.9751,
      "step": 5
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.04537992179393768,
      "learning_rate": 0.019975031210986267,
      "loss": 0.7012,
      "step": 6
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.02319790981709957,
      "learning_rate": 0.019970869746150647,
      "loss": 1.1514,
      "step": 7
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.027198510244488716,
      "learning_rate": 0.019966708281315023,
      "loss": 1.1768,
      "step": 8
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.026688028126955032,
      "learning_rate": 0.019962546816479403,
      "loss": 1.0107,
      "step": 9
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.027742978185415268,
      "learning_rate": 0.01995838535164378,
      "loss": 0.52,
      "step": 10
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.017290690913796425,
      "learning_rate": 0.019954223886808155,
      "loss": 0.5259,
      "step": 11
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.024143831804394722,
      "learning_rate": 0.019950062421972534,
      "loss": 0.895,
      "step": 12
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.03602828457951546,
      "learning_rate": 0.019945900957136914,
      "loss": 1.042,
      "step": 13
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.03868493810296059,
      "learning_rate": 0.01994173949230129,
      "loss": 0.9253,
      "step": 14
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.019538432359695435,
      "learning_rate": 0.01993757802746567,
      "loss": 0.8286,
      "step": 15
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.028750460594892502,
      "learning_rate": 0.019933416562630046,
      "loss": 0.5039,
      "step": 16
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.022473318502306938,
      "learning_rate": 0.019929255097794422,
      "loss": 0.8252,
      "step": 17
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.01876644417643547,
      "learning_rate": 0.0199250936329588,
      "loss": 0.5903,
      "step": 18
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.029065033420920372,
      "learning_rate": 0.01992093216812318,
      "loss": 0.6187,
      "step": 19
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.014426208101212978,
      "learning_rate": 0.019916770703287557,
      "loss": 0.2732,
      "step": 20
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.01835629716515541,
      "learning_rate": 0.019912609238451937,
      "loss": 0.5063,
      "step": 21
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.021680880337953568,
      "learning_rate": 0.019908447773616313,
      "loss": 0.5586,
      "step": 22
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.018381770700216293,
      "learning_rate": 0.01990428630878069,
      "loss": 0.4036,
      "step": 23
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.0139843188226223,
      "learning_rate": 0.01990012484394507,
      "loss": 0.2484,
      "step": 24
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.022173702716827393,
      "learning_rate": 0.019895963379109448,
      "loss": 0.7319,
      "step": 25
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.019779937341809273,
      "learning_rate": 0.019891801914273824,
      "loss": 0.25,
      "step": 26
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.015408563427627087,
      "learning_rate": 0.019887640449438203,
      "loss": 0.6279,
      "step": 27
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.017980150878429413,
      "learning_rate": 0.01988347898460258,
      "loss": 0.7153,
      "step": 28
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.01553407683968544,
      "learning_rate": 0.019879317519766956,
      "loss": 0.6553,
      "step": 29
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.017622757703065872,
      "learning_rate": 0.019875156054931335,
      "loss": 0.54,
      "step": 30
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.019046414643526077,
      "learning_rate": 0.019870994590095715,
      "loss": 0.7466,
      "step": 31
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.018101511523127556,
      "learning_rate": 0.01986683312526009,
      "loss": 0.6934,
      "step": 32
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.01025469321757555,
      "learning_rate": 0.01986267166042447,
      "loss": 0.5674,
      "step": 33
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.02104567363858223,
      "learning_rate": 0.019858510195588847,
      "loss": 0.8809,
      "step": 34
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.011890831403434277,
      "learning_rate": 0.019854348730753226,
      "loss": 0.2898,
      "step": 35
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.022102177143096924,
      "learning_rate": 0.019850187265917602,
      "loss": 0.48,
      "step": 36
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.014674047939479351,
      "learning_rate": 0.019846025801081982,
      "loss": 0.3022,
      "step": 37
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.019139140844345093,
      "learning_rate": 0.01984186433624636,
      "loss": 0.3066,
      "step": 38
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.016503287479281425,
      "learning_rate": 0.019837702871410737,
      "loss": 0.5918,
      "step": 39
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.019555386155843735,
      "learning_rate": 0.019833541406575114,
      "loss": 0.4966,
      "step": 40
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.015926335006952286,
      "learning_rate": 0.019829379941739493,
      "loss": 0.4143,
      "step": 41
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.016720635816454887,
      "learning_rate": 0.01982521847690387,
      "loss": 0.6763,
      "step": 42
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.010436618700623512,
      "learning_rate": 0.01982105701206825,
      "loss": 0.7246,
      "step": 43
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.016686901450157166,
      "learning_rate": 0.01981689554723263,
      "loss": 0.321,
      "step": 44
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.013563442043960094,
      "learning_rate": 0.019812734082397004,
      "loss": 0.5669,
      "step": 45
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.01583418808877468,
      "learning_rate": 0.019808572617561384,
      "loss": 0.4619,
      "step": 46
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.012616423889994621,
      "learning_rate": 0.01980441115272576,
      "loss": 0.6016,
      "step": 47
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.019653743132948875,
      "learning_rate": 0.019800249687890136,
      "loss": 0.262,
      "step": 48
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.01714310795068741,
      "learning_rate": 0.019796088223054516,
      "loss": 0.4636,
      "step": 49
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.012529288418591022,
      "learning_rate": 0.019791926758218895,
      "loss": 0.0888,
      "step": 50
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.025462329387664795,
      "learning_rate": 0.01978776529338327,
      "loss": 1.1182,
      "step": 51
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.013185207732021809,
      "learning_rate": 0.01978360382854765,
      "loss": 0.6724,
      "step": 52
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.025705890730023384,
      "learning_rate": 0.019779442363712027,
      "loss": 1.2998,
      "step": 53
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.018231689929962158,
      "learning_rate": 0.019775280898876403,
      "loss": 0.4458,
      "step": 54
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.013859216123819351,
      "learning_rate": 0.019771119434040783,
      "loss": 0.623,
      "step": 55
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.020070232450962067,
      "learning_rate": 0.019766957969205162,
      "loss": 0.73,
      "step": 56
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.012890408746898174,
      "learning_rate": 0.01976279650436954,
      "loss": 0.4128,
      "step": 57
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.014063001610338688,
      "learning_rate": 0.019758635039533918,
      "loss": 0.4192,
      "step": 58
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.016345465555787086,
      "learning_rate": 0.019754473574698294,
      "loss": 0.5889,
      "step": 59
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.0205059964209795,
      "learning_rate": 0.01975031210986267,
      "loss": 0.4709,
      "step": 60
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.014671096578240395,
      "learning_rate": 0.01974615064502705,
      "loss": 0.3325,
      "step": 61
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.011892193928360939,
      "learning_rate": 0.01974198918019143,
      "loss": 0.4302,
      "step": 62
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.015515914186835289,
      "learning_rate": 0.019737827715355805,
      "loss": 0.9307,
      "step": 63
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.01472906768321991,
      "learning_rate": 0.019733666250520185,
      "loss": 0.3135,
      "step": 64
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.013207866810262203,
      "learning_rate": 0.01972950478568456,
      "loss": 0.4983,
      "step": 65
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.006755692884325981,
      "learning_rate": 0.019725343320848937,
      "loss": 0.4126,
      "step": 66
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.013154924847185612,
      "learning_rate": 0.019721181856013317,
      "loss": 0.5415,
      "step": 67
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.012332231737673283,
      "learning_rate": 0.019717020391177696,
      "loss": 0.5249,
      "step": 68
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.01766934059560299,
      "learning_rate": 0.019712858926342072,
      "loss": 0.4358,
      "step": 69
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.018197795376181602,
      "learning_rate": 0.019708697461506452,
      "loss": 0.6753,
      "step": 70
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.016164090484380722,
      "learning_rate": 0.019704535996670828,
      "loss": 0.4712,
      "step": 71
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.029932130128145218,
      "learning_rate": 0.019700374531835204,
      "loss": 0.5342,
      "step": 72
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.01345884520560503,
      "learning_rate": 0.019696213066999584,
      "loss": 0.5029,
      "step": 73
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.01129131205379963,
      "learning_rate": 0.019692051602163963,
      "loss": 0.5376,
      "step": 74
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.016407852992415428,
      "learning_rate": 0.01968789013732834,
      "loss": 0.791,
      "step": 75
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.019868925213813782,
      "learning_rate": 0.01968372867249272,
      "loss": 0.5015,
      "step": 76
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.0146525539457798,
      "learning_rate": 0.019679567207657095,
      "loss": 0.541,
      "step": 77
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.010244067758321762,
      "learning_rate": 0.019675405742821474,
      "loss": 0.2642,
      "step": 78
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.015067693777382374,
      "learning_rate": 0.01967124427798585,
      "loss": 0.2817,
      "step": 79
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.010389878414571285,
      "learning_rate": 0.01966708281315023,
      "loss": 0.6113,
      "step": 80
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.011934892274439335,
      "learning_rate": 0.019662921348314606,
      "loss": 0.0456,
      "step": 81
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.01124374195933342,
      "learning_rate": 0.019658759883478986,
      "loss": 0.2357,
      "step": 82
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.017671743407845497,
      "learning_rate": 0.019654598418643362,
      "loss": 0.5024,
      "step": 83
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.02609691210091114,
      "learning_rate": 0.01965043695380774,
      "loss": 0.5361,
      "step": 84
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.01812068186700344,
      "learning_rate": 0.019646275488972118,
      "loss": 0.4084,
      "step": 85
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.014044742099940777,
      "learning_rate": 0.019642114024136497,
      "loss": 0.2783,
      "step": 86
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.013514760881662369,
      "learning_rate": 0.019637952559300873,
      "loss": 0.5366,
      "step": 87
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.01608353666961193,
      "learning_rate": 0.019633791094465253,
      "loss": 0.4722,
      "step": 88
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.014681047759950161,
      "learning_rate": 0.019629629629629632,
      "loss": 0.3625,
      "step": 89
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.011236661113798618,
      "learning_rate": 0.01962546816479401,
      "loss": 0.2866,
      "step": 90
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.014246581122279167,
      "learning_rate": 0.019621306699958384,
      "loss": 0.4597,
      "step": 91
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.015461236238479614,
      "learning_rate": 0.019617145235122764,
      "loss": 0.251,
      "step": 92
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.013209463097155094,
      "learning_rate": 0.01961298377028714,
      "loss": 0.4397,
      "step": 93
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.017247267067432404,
      "learning_rate": 0.01960882230545152,
      "loss": 0.1571,
      "step": 94
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.0060082110576331615,
      "learning_rate": 0.0196046608406159,
      "loss": 0.0955,
      "step": 95
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.04800603538751602,
      "learning_rate": 0.019600499375780275,
      "loss": 0.5498,
      "step": 96
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.010818454436957836,
      "learning_rate": 0.01959633791094465,
      "loss": 0.3552,
      "step": 97
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.013875529170036316,
      "learning_rate": 0.01959217644610903,
      "loss": 0.2754,
      "step": 98
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.009726175107061863,
      "learning_rate": 0.019588014981273407,
      "loss": 0.5693,
      "step": 99
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.010561893694102764,
      "learning_rate": 0.019583853516437787,
      "loss": 0.269,
      "step": 100
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.018042325973510742,
      "learning_rate": 0.019579692051602166,
      "loss": 0.2888,
      "step": 101
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.011434352956712246,
      "learning_rate": 0.019575530586766542,
      "loss": 0.3032,
      "step": 102
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.01294125895947218,
      "learning_rate": 0.01957136912193092,
      "loss": 0.3904,
      "step": 103
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.00973493605852127,
      "learning_rate": 0.019567207657095298,
      "loss": 0.1273,
      "step": 104
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.011769156903028488,
      "learning_rate": 0.019563046192259674,
      "loss": 0.3728,
      "step": 105
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.013946526683866978,
      "learning_rate": 0.019558884727424054,
      "loss": 0.4209,
      "step": 106
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.024769112467765808,
      "learning_rate": 0.019554723262588433,
      "loss": 0.5957,
      "step": 107
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.013953709043562412,
      "learning_rate": 0.01955056179775281,
      "loss": 0.896,
      "step": 108
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.016479069367051125,
      "learning_rate": 0.019546400332917185,
      "loss": 0.2915,
      "step": 109
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.012262118980288506,
      "learning_rate": 0.019542238868081565,
      "loss": 0.3962,
      "step": 110
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.009211440570652485,
      "learning_rate": 0.019538077403245944,
      "loss": 0.2974,
      "step": 111
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.01734967529773712,
      "learning_rate": 0.01953391593841032,
      "loss": 0.583,
      "step": 112
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.02681916393339634,
      "learning_rate": 0.0195297544735747,
      "loss": 0.5518,
      "step": 113
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.019412348046898842,
      "learning_rate": 0.019525593008739076,
      "loss": 0.1301,
      "step": 114
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.0728633925318718,
      "learning_rate": 0.019521431543903452,
      "loss": 0.3469,
      "step": 115
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.010789559222757816,
      "learning_rate": 0.019517270079067832,
      "loss": 0.2566,
      "step": 116
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.01410316675901413,
      "learning_rate": 0.01951310861423221,
      "loss": 0.6997,
      "step": 117
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.02386266365647316,
      "learning_rate": 0.019508947149396588,
      "loss": 0.7905,
      "step": 118
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.01500307023525238,
      "learning_rate": 0.019504785684560967,
      "loss": 0.311,
      "step": 119
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.01638227328658104,
      "learning_rate": 0.019500624219725343,
      "loss": 0.377,
      "step": 120
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.018486998975276947,
      "learning_rate": 0.019496462754889723,
      "loss": 0.4883,
      "step": 121
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.004287502728402615,
      "learning_rate": 0.0194923012900541,
      "loss": 0.0809,
      "step": 122
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.012090643867850304,
      "learning_rate": 0.01948813982521848,
      "loss": 0.3147,
      "step": 123
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.012809454463422298,
      "learning_rate": 0.019483978360382855,
      "loss": 0.2346,
      "step": 124
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.03159857168793678,
      "learning_rate": 0.019479816895547234,
      "loss": 0.0633,
      "step": 125
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.03378335013985634,
      "learning_rate": 0.01947565543071161,
      "loss": 0.4414,
      "step": 126
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.00972415879368782,
      "learning_rate": 0.01947149396587599,
      "loss": 0.1332,
      "step": 127
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.014316284097731113,
      "learning_rate": 0.019467332501040366,
      "loss": 0.2642,
      "step": 128
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.01945355348289013,
      "learning_rate": 0.019463171036204745,
      "loss": 0.2568,
      "step": 129
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.014685138128697872,
      "learning_rate": 0.01945900957136912,
      "loss": 0.2495,
      "step": 130
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.010893230326473713,
      "learning_rate": 0.0194548481065335,
      "loss": 0.1844,
      "step": 131
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.015432030893862247,
      "learning_rate": 0.01945068664169788,
      "loss": 0.2192,
      "step": 132
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.023381555452942848,
      "learning_rate": 0.019446525176862257,
      "loss": 0.1019,
      "step": 133
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.025069458410143852,
      "learning_rate": 0.019442363712026633,
      "loss": 0.6982,
      "step": 134
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.013284072279930115,
      "learning_rate": 0.019438202247191012,
      "loss": 0.2725,
      "step": 135
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.01459498330950737,
      "learning_rate": 0.01943404078235539,
      "loss": 0.342,
      "step": 136
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.010649852454662323,
      "learning_rate": 0.019429879317519768,
      "loss": 0.1597,
      "step": 137
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.00572684733197093,
      "learning_rate": 0.019425717852684148,
      "loss": 0.0163,
      "step": 138
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.010500311851501465,
      "learning_rate": 0.019421556387848524,
      "loss": 0.3718,
      "step": 139
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.02403194084763527,
      "learning_rate": 0.0194173949230129,
      "loss": 0.3464,
      "step": 140
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.015808913856744766,
      "learning_rate": 0.01941323345817728,
      "loss": 0.0919,
      "step": 141
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.017955824732780457,
      "learning_rate": 0.019409071993341655,
      "loss": 0.3323,
      "step": 142
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.015312157571315765,
      "learning_rate": 0.019404910528506035,
      "loss": 0.4712,
      "step": 143
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.013958172872662544,
      "learning_rate": 0.019400749063670415,
      "loss": 0.1681,
      "step": 144
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.01720457337796688,
      "learning_rate": 0.01939658759883479,
      "loss": 0.541,
      "step": 145
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.017649512737989426,
      "learning_rate": 0.019392426133999167,
      "loss": 0.6196,
      "step": 146
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.016069641336798668,
      "learning_rate": 0.019388264669163546,
      "loss": 0.3416,
      "step": 147
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.015727760270237923,
      "learning_rate": 0.019384103204327922,
      "loss": 0.2766,
      "step": 148
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.011026602238416672,
      "learning_rate": 0.019379941739492302,
      "loss": 0.2603,
      "step": 149
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.011536615900695324,
      "learning_rate": 0.01937578027465668,
      "loss": 0.0615,
      "step": 150
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.018519138917326927,
      "learning_rate": 0.019371618809821058,
      "loss": 0.2013,
      "step": 151
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.019297972321510315,
      "learning_rate": 0.019367457344985434,
      "loss": 0.7739,
      "step": 152
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.035224251449108124,
      "learning_rate": 0.019363295880149813,
      "loss": 0.1853,
      "step": 153
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.011118430644273758,
      "learning_rate": 0.01935913441531419,
      "loss": 0.585,
      "step": 154
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.01876828819513321,
      "learning_rate": 0.01935497295047857,
      "loss": 0.439,
      "step": 155
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.013631806708872318,
      "learning_rate": 0.01935081148564295,
      "loss": 0.2147,
      "step": 156
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.013377000577747822,
      "learning_rate": 0.019346650020807325,
      "loss": 0.0768,
      "step": 157
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.009209630079567432,
      "learning_rate": 0.0193424885559717,
      "loss": 0.12,
      "step": 158
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.010687937960028648,
      "learning_rate": 0.01933832709113608,
      "loss": 0.3562,
      "step": 159
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.01562159787863493,
      "learning_rate": 0.019334165626300456,
      "loss": 0.2192,
      "step": 160
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.005969279445707798,
      "learning_rate": 0.019330004161464836,
      "loss": 0.0499,
      "step": 161
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.01860101707279682,
      "learning_rate": 0.019325842696629215,
      "loss": 0.4082,
      "step": 162
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.013926600106060505,
      "learning_rate": 0.01932168123179359,
      "loss": 0.27,
      "step": 163
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.007220026105642319,
      "learning_rate": 0.01931751976695797,
      "loss": 0.139,
      "step": 164
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.016826435923576355,
      "learning_rate": 0.019313358302122347,
      "loss": 0.4153,
      "step": 165
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.008984547108411789,
      "learning_rate": 0.019309196837286723,
      "loss": 0.2205,
      "step": 166
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.014899051748216152,
      "learning_rate": 0.019305035372451103,
      "loss": 0.4397,
      "step": 167
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.008856400847434998,
      "learning_rate": 0.019300873907615482,
      "loss": 0.0795,
      "step": 168
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.015470389276742935,
      "learning_rate": 0.01929671244277986,
      "loss": 0.6055,
      "step": 169
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.01177644170820713,
      "learning_rate": 0.019292550977944238,
      "loss": 0.3594,
      "step": 170
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.018051499500870705,
      "learning_rate": 0.019288389513108614,
      "loss": 0.7207,
      "step": 171
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.01194706466048956,
      "learning_rate": 0.01928422804827299,
      "loss": 0.4624,
      "step": 172
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.021377161145210266,
      "learning_rate": 0.01928006658343737,
      "loss": 0.4302,
      "step": 173
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.011635289527475834,
      "learning_rate": 0.01927590511860175,
      "loss": 0.2068,
      "step": 174
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.012977547012269497,
      "learning_rate": 0.019271743653766125,
      "loss": 0.3076,
      "step": 175
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.011483551934361458,
      "learning_rate": 0.019267582188930505,
      "loss": 0.2173,
      "step": 176
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.018402379006147385,
      "learning_rate": 0.01926342072409488,
      "loss": 0.3831,
      "step": 177
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.008669276721775532,
      "learning_rate": 0.019259259259259257,
      "loss": 0.1942,
      "step": 178
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.009989948943257332,
      "learning_rate": 0.019255097794423637,
      "loss": 0.4062,
      "step": 179
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.007285960018634796,
      "learning_rate": 0.019250936329588016,
      "loss": 0.0542,
      "step": 180
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.022931115701794624,
      "learning_rate": 0.019246774864752392,
      "loss": 0.3311,
      "step": 181
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.015280869789421558,
      "learning_rate": 0.019242613399916772,
      "loss": 0.1851,
      "step": 182
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.016446208581328392,
      "learning_rate": 0.019238451935081148,
      "loss": 0.3059,
      "step": 183
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.009221578016877174,
      "learning_rate": 0.019234290470245528,
      "loss": 0.1036,
      "step": 184
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.01323636807501316,
      "learning_rate": 0.019230129005409904,
      "loss": 0.3828,
      "step": 185
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.011723767034709454,
      "learning_rate": 0.019225967540574283,
      "loss": 0.6211,
      "step": 186
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.00718157272785902,
      "learning_rate": 0.019221806075738663,
      "loss": 0.1534,
      "step": 187
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.009665646590292454,
      "learning_rate": 0.01921764461090304,
      "loss": 0.1459,
      "step": 188
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.01421988382935524,
      "learning_rate": 0.019213483146067415,
      "loss": 0.3362,
      "step": 189
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.014052601531147957,
      "learning_rate": 0.019209321681231795,
      "loss": 0.4727,
      "step": 190
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.007880319841206074,
      "learning_rate": 0.01920516021639617,
      "loss": 0.0958,
      "step": 191
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.013098879717290401,
      "learning_rate": 0.01920099875156055,
      "loss": 0.2483,
      "step": 192
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.010881153866648674,
      "learning_rate": 0.01919683728672493,
      "loss": 0.0822,
      "step": 193
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.011607570573687553,
      "learning_rate": 0.019192675821889306,
      "loss": 0.4939,
      "step": 194
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.015284847468137741,
      "learning_rate": 0.019188514357053682,
      "loss": 0.4248,
      "step": 195
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.006532551255077124,
      "learning_rate": 0.01918435289221806,
      "loss": 0.0183,
      "step": 196
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.00698343338444829,
      "learning_rate": 0.019180191427382438,
      "loss": 0.0674,
      "step": 197
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.01601043902337551,
      "learning_rate": 0.019176029962546817,
      "loss": 1.0,
      "step": 198
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.018778465688228607,
      "learning_rate": 0.019171868497711197,
      "loss": 0.5615,
      "step": 199
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.017414990812540054,
      "learning_rate": 0.019167707032875573,
      "loss": 0.4424,
      "step": 200
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.009499352425336838,
      "learning_rate": 0.01916354556803995,
      "loss": 0.0865,
      "step": 201
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.011209850199520588,
      "learning_rate": 0.01915938410320433,
      "loss": 0.3103,
      "step": 202
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.011232535354793072,
      "learning_rate": 0.019155222638368705,
      "loss": 0.3169,
      "step": 203
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.016806548461318016,
      "learning_rate": 0.019151061173533084,
      "loss": 0.6245,
      "step": 204
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.01189274899661541,
      "learning_rate": 0.019146899708697464,
      "loss": 0.1663,
      "step": 205
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.015232094563543797,
      "learning_rate": 0.01914273824386184,
      "loss": 0.6851,
      "step": 206
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.02008361555635929,
      "learning_rate": 0.01913857677902622,
      "loss": 0.4065,
      "step": 207
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.007860634475946426,
      "learning_rate": 0.019134415314190596,
      "loss": 0.114,
      "step": 208
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.009345010854303837,
      "learning_rate": 0.01913025384935497,
      "loss": 0.3083,
      "step": 209
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.01482164952903986,
      "learning_rate": 0.01912609238451935,
      "loss": 0.5288,
      "step": 210
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.013445671647787094,
      "learning_rate": 0.01912193091968373,
      "loss": 0.0577,
      "step": 211
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.01909550465643406,
      "learning_rate": 0.019117769454848107,
      "loss": 0.3801,
      "step": 212
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.013162663206458092,
      "learning_rate": 0.019113607990012486,
      "loss": 0.2639,
      "step": 213
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.01083003357052803,
      "learning_rate": 0.019109446525176862,
      "loss": 0.1499,
      "step": 214
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.00959020759910345,
      "learning_rate": 0.01910528506034124,
      "loss": 0.1101,
      "step": 215
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.010005058720707893,
      "learning_rate": 0.019101123595505618,
      "loss": 0.2026,
      "step": 216
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.02039876952767372,
      "learning_rate": 0.019096962130669998,
      "loss": 0.282,
      "step": 217
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.014765151776373386,
      "learning_rate": 0.019092800665834374,
      "loss": 0.3125,
      "step": 218
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.009571024216711521,
      "learning_rate": 0.019088639200998753,
      "loss": 0.183,
      "step": 219
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.010038415901362896,
      "learning_rate": 0.01908447773616313,
      "loss": 0.1028,
      "step": 220
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.012818839401006699,
      "learning_rate": 0.019080316271327506,
      "loss": 0.5225,
      "step": 221
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.014451993629336357,
      "learning_rate": 0.019076154806491885,
      "loss": 0.168,
      "step": 222
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.006471678148955107,
      "learning_rate": 0.019071993341656265,
      "loss": 0.0197,
      "step": 223
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.004482632502913475,
      "learning_rate": 0.01906783187682064,
      "loss": 0.0293,
      "step": 224
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.016593338921666145,
      "learning_rate": 0.01906367041198502,
      "loss": 0.5806,
      "step": 225
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.00906510278582573,
      "learning_rate": 0.019059508947149396,
      "loss": 0.1466,
      "step": 226
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.010454477742314339,
      "learning_rate": 0.019055347482313773,
      "loss": 0.1746,
      "step": 227
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.0206717811524868,
      "learning_rate": 0.019051186017478152,
      "loss": 0.6543,
      "step": 228
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.009957603178918362,
      "learning_rate": 0.01904702455264253,
      "loss": 0.0541,
      "step": 229
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.02599034458398819,
      "learning_rate": 0.019042863087806908,
      "loss": 0.4255,
      "step": 230
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.017418760806322098,
      "learning_rate": 0.019038701622971287,
      "loss": 0.4253,
      "step": 231
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.013599650003015995,
      "learning_rate": 0.019034540158135663,
      "loss": 0.3621,
      "step": 232
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.012645847164094448,
      "learning_rate": 0.01903037869330004,
      "loss": 0.1808,
      "step": 233
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.011502467095851898,
      "learning_rate": 0.01902621722846442,
      "loss": 0.4675,
      "step": 234
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.009859512560069561,
      "learning_rate": 0.0190220557636288,
      "loss": 0.0282,
      "step": 235
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.009849142283201218,
      "learning_rate": 0.019017894298793175,
      "loss": 0.0837,
      "step": 236
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.015944460406899452,
      "learning_rate": 0.019013732833957554,
      "loss": 0.2374,
      "step": 237
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.015828672796487808,
      "learning_rate": 0.01900957136912193,
      "loss": 0.5649,
      "step": 238
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.011765114963054657,
      "learning_rate": 0.01900540990428631,
      "loss": 0.4055,
      "step": 239
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.012066415511071682,
      "learning_rate": 0.019001248439450686,
      "loss": 0.3311,
      "step": 240
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.01208413578569889,
      "learning_rate": 0.018997086974615066,
      "loss": 0.3303,
      "step": 241
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.020172521471977234,
      "learning_rate": 0.01899292550977944,
      "loss": 0.6533,
      "step": 242
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.013071781024336815,
      "learning_rate": 0.01898876404494382,
      "loss": 0.2922,
      "step": 243
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.014257648959755898,
      "learning_rate": 0.018984602580108197,
      "loss": 0.5166,
      "step": 244
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.008450665511190891,
      "learning_rate": 0.018980441115272577,
      "loss": 0.1483,
      "step": 245
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01024819165468216,
      "learning_rate": 0.018976279650436953,
      "loss": 0.3325,
      "step": 246
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.012138486839830875,
      "learning_rate": 0.018972118185601333,
      "loss": 0.3181,
      "step": 247
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01706678234040737,
      "learning_rate": 0.01896795672076571,
      "loss": 0.5381,
      "step": 248
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.0165503341704607,
      "learning_rate": 0.018963795255930088,
      "loss": 0.3474,
      "step": 249
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.017817780375480652,
      "learning_rate": 0.018959633791094468,
      "loss": 0.1743,
      "step": 250
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01351422630250454,
      "learning_rate": 0.018955472326258844,
      "loss": 0.1572,
      "step": 251
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01826195791363716,
      "learning_rate": 0.01895131086142322,
      "loss": 0.3682,
      "step": 252
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.01721639558672905,
      "learning_rate": 0.0189471493965876,
      "loss": 0.4712,
      "step": 253
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.0058861286379396915,
      "learning_rate": 0.018942987931751976,
      "loss": 0.0426,
      "step": 254
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.0133828679099679,
      "learning_rate": 0.018938826466916355,
      "loss": 0.3984,
      "step": 255
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.015871714800596237,
      "learning_rate": 0.018934665002080735,
      "loss": 0.4358,
      "step": 256
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.012308573350310326,
      "learning_rate": 0.01893050353724511,
      "loss": 0.303,
      "step": 257
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.010923982597887516,
      "learning_rate": 0.018926342072409487,
      "loss": 0.1774,
      "step": 258
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.013635121285915375,
      "learning_rate": 0.018922180607573866,
      "loss": 0.4199,
      "step": 259
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.013736642897129059,
      "learning_rate": 0.018918019142738246,
      "loss": 0.3162,
      "step": 260
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.018210401758551598,
      "learning_rate": 0.018913857677902622,
      "loss": 0.3315,
      "step": 261
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.01903851330280304,
      "learning_rate": 0.018909696213067,
      "loss": 0.6294,
      "step": 262
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.01758592203259468,
      "learning_rate": 0.018905534748231378,
      "loss": 0.853,
      "step": 263
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.014722253195941448,
      "learning_rate": 0.018901373283395754,
      "loss": 0.2148,
      "step": 264
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.010228004306554794,
      "learning_rate": 0.018897211818560133,
      "loss": 0.07,
      "step": 265
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.017041178420186043,
      "learning_rate": 0.018893050353724513,
      "loss": 0.3865,
      "step": 266
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.024548158049583435,
      "learning_rate": 0.01888888888888889,
      "loss": 0.408,
      "step": 267
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.021002428606152534,
      "learning_rate": 0.01888472742405327,
      "loss": 0.4521,
      "step": 268
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.023030998185276985,
      "learning_rate": 0.018880565959217645,
      "loss": 0.6274,
      "step": 269
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.01349740382283926,
      "learning_rate": 0.01887640449438202,
      "loss": 0.3054,
      "step": 270
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.01190619170665741,
      "learning_rate": 0.0188722430295464,
      "loss": 0.1816,
      "step": 271
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.01658334955573082,
      "learning_rate": 0.01886808156471078,
      "loss": 0.491,
      "step": 272
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.008173597976565361,
      "learning_rate": 0.018863920099875156,
      "loss": 0.0178,
      "step": 273
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.017877396196126938,
      "learning_rate": 0.018859758635039536,
      "loss": 0.4114,
      "step": 274
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.012737761251628399,
      "learning_rate": 0.01885559717020391,
      "loss": 0.2288,
      "step": 275
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.02273883856832981,
      "learning_rate": 0.018851435705368288,
      "loss": 0.4519,
      "step": 276
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.006959815509617329,
      "learning_rate": 0.018847274240532667,
      "loss": 0.0447,
      "step": 277
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.012611573562026024,
      "learning_rate": 0.018843112775697047,
      "loss": 0.1226,
      "step": 278
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.014003496617078781,
      "learning_rate": 0.018838951310861423,
      "loss": 0.4014,
      "step": 279
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.01877623423933983,
      "learning_rate": 0.018834789846025803,
      "loss": 0.2646,
      "step": 280
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.018713688477873802,
      "learning_rate": 0.01883062838119018,
      "loss": 0.5034,
      "step": 281
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.02422862872481346,
      "learning_rate": 0.018826466916354558,
      "loss": 0.8271,
      "step": 282
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.016888445243239403,
      "learning_rate": 0.018822305451518934,
      "loss": 0.541,
      "step": 283
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.025673290714621544,
      "learning_rate": 0.018818143986683314,
      "loss": 0.5269,
      "step": 284
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.013471826910972595,
      "learning_rate": 0.01881398252184769,
      "loss": 0.1483,
      "step": 285
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.016805335879325867,
      "learning_rate": 0.01880982105701207,
      "loss": 0.4895,
      "step": 286
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.02230897918343544,
      "learning_rate": 0.018805659592176446,
      "loss": 0.2627,
      "step": 287
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.017700405791401863,
      "learning_rate": 0.018801498127340825,
      "loss": 0.2869,
      "step": 288
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.026683760806918144,
      "learning_rate": 0.0187973366625052,
      "loss": 0.4714,
      "step": 289
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.017930233851075172,
      "learning_rate": 0.01879317519766958,
      "loss": 0.124,
      "step": 290
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.014429387636482716,
      "learning_rate": 0.018789013732833957,
      "loss": 0.4141,
      "step": 291
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.012393507175147533,
      "learning_rate": 0.018784852267998337,
      "loss": 0.1812,
      "step": 292
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.013746536336839199,
      "learning_rate": 0.018780690803162716,
      "loss": 0.2296,
      "step": 293
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.017592409625649452,
      "learning_rate": 0.018776529338327092,
      "loss": 0.4626,
      "step": 294
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.02273542992770672,
      "learning_rate": 0.01877236787349147,
      "loss": 0.2491,
      "step": 295
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.04013910889625549,
      "learning_rate": 0.018768206408655848,
      "loss": 0.5811,
      "step": 296
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.012396056205034256,
      "learning_rate": 0.018764044943820224,
      "loss": 0.5254,
      "step": 297
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.01785169169306755,
      "learning_rate": 0.018759883478984603,
      "loss": 0.397,
      "step": 298
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.01874547079205513,
      "learning_rate": 0.018755722014148983,
      "loss": 0.2937,
      "step": 299
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.01092853955924511,
      "learning_rate": 0.01875156054931336,
      "loss": 0.4192,
      "step": 300
    },
    {
      "epoch": 0.37,
      "eval_loss": 0.3251953125,
      "eval_runtime": 183.3307,
      "eval_samples_per_second": 1.096,
      "eval_steps_per_second": 0.551,
      "step": 300
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.012453447096049786,
      "learning_rate": 0.018747399084477735,
      "loss": 0.2705,
      "step": 301
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.020620588213205338,
      "learning_rate": 0.018743237619642115,
      "loss": 0.6685,
      "step": 302
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.018006984144449234,
      "learning_rate": 0.01873907615480649,
      "loss": 0.4458,
      "step": 303
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.012783350422978401,
      "learning_rate": 0.01873491468997087,
      "loss": 0.345,
      "step": 304
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.019229650497436523,
      "learning_rate": 0.01873075322513525,
      "loss": 0.3953,
      "step": 305
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.017787378281354904,
      "learning_rate": 0.018726591760299626,
      "loss": 0.624,
      "step": 306
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.01644415408372879,
      "learning_rate": 0.018722430295464002,
      "loss": 0.2329,
      "step": 307
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.012315182946622372,
      "learning_rate": 0.018718268830628382,
      "loss": 0.1984,
      "step": 308
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.012109145522117615,
      "learning_rate": 0.018714107365792758,
      "loss": 0.2041,
      "step": 309
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.013556061312556267,
      "learning_rate": 0.018709945900957137,
      "loss": 0.4844,
      "step": 310
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.016224991530179977,
      "learning_rate": 0.018705784436121517,
      "loss": 0.5752,
      "step": 311
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.021627897396683693,
      "learning_rate": 0.018701622971285893,
      "loss": 0.2301,
      "step": 312
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.022303931415081024,
      "learning_rate": 0.01869746150645027,
      "loss": 0.4536,
      "step": 313
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.014758111909031868,
      "learning_rate": 0.01869330004161465,
      "loss": 0.3098,
      "step": 314
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.013408167287707329,
      "learning_rate": 0.018689138576779025,
      "loss": 0.0468,
      "step": 315
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.017526132985949516,
      "learning_rate": 0.018684977111943404,
      "loss": 0.1798,
      "step": 316
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.025102650746703148,
      "learning_rate": 0.018680815647107784,
      "loss": 0.5762,
      "step": 317
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.014943995513021946,
      "learning_rate": 0.01867665418227216,
      "loss": 0.4561,
      "step": 318
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.016996843740344048,
      "learning_rate": 0.018672492717436536,
      "loss": 0.2896,
      "step": 319
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.011097901500761509,
      "learning_rate": 0.018668331252600916,
      "loss": 0.1134,
      "step": 320
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.013591892085969448,
      "learning_rate": 0.018664169787765292,
      "loss": 0.1129,
      "step": 321
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.015822574496269226,
      "learning_rate": 0.01866000832292967,
      "loss": 0.2207,
      "step": 322
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.015380003489553928,
      "learning_rate": 0.01865584685809405,
      "loss": 0.16,
      "step": 323
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.008600973524153233,
      "learning_rate": 0.018651685393258427,
      "loss": 0.0347,
      "step": 324
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.014758262783288956,
      "learning_rate": 0.018647523928422807,
      "loss": 0.2202,
      "step": 325
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.016456298530101776,
      "learning_rate": 0.018643362463587183,
      "loss": 0.2969,
      "step": 326
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.014062338508665562,
      "learning_rate": 0.01863920099875156,
      "loss": 0.2715,
      "step": 327
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.014881882816553116,
      "learning_rate": 0.01863503953391594,
      "loss": 0.022,
      "step": 328
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.0236114040017128,
      "learning_rate": 0.018630878069080318,
      "loss": 0.2834,
      "step": 329
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.023272477090358734,
      "learning_rate": 0.018626716604244694,
      "loss": 0.5132,
      "step": 330
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.012915327213704586,
      "learning_rate": 0.018622555139409074,
      "loss": 0.1646,
      "step": 331
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.027405433356761932,
      "learning_rate": 0.01861839367457345,
      "loss": 0.4375,
      "step": 332
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.019116155803203583,
      "learning_rate": 0.01861423220973783,
      "loss": 0.2905,
      "step": 333
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.015979696065187454,
      "learning_rate": 0.018610070744902205,
      "loss": 0.2188,
      "step": 334
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.016301333904266357,
      "learning_rate": 0.018605909280066585,
      "loss": 0.2358,
      "step": 335
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.011888011358678341,
      "learning_rate": 0.018601747815230964,
      "loss": 0.246,
      "step": 336
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.023288020864129066,
      "learning_rate": 0.01859758635039534,
      "loss": 0.541,
      "step": 337
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.005208710674196482,
      "learning_rate": 0.018593424885559717,
      "loss": 0.0097,
      "step": 338
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.011656765826046467,
      "learning_rate": 0.018589263420724096,
      "loss": 0.0943,
      "step": 339
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.026298977434635162,
      "learning_rate": 0.018585101955888472,
      "loss": 1.1572,
      "step": 340
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.015261673368513584,
      "learning_rate": 0.018580940491052852,
      "loss": 0.0914,
      "step": 341
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.018320735543966293,
      "learning_rate": 0.01857677902621723,
      "loss": 0.4023,
      "step": 342
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.020216815173625946,
      "learning_rate": 0.018572617561381607,
      "loss": 0.26,
      "step": 343
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.016558609902858734,
      "learning_rate": 0.018568456096545984,
      "loss": 0.301,
      "step": 344
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.013275112956762314,
      "learning_rate": 0.018564294631710363,
      "loss": 0.1666,
      "step": 345
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.023631185293197632,
      "learning_rate": 0.01856013316687474,
      "loss": 0.365,
      "step": 346
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.023109402507543564,
      "learning_rate": 0.01855597170203912,
      "loss": 0.4705,
      "step": 347
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.024755585938692093,
      "learning_rate": 0.0185518102372035,
      "loss": 0.6177,
      "step": 348
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.038343414664268494,
      "learning_rate": 0.018547648772367874,
      "loss": 0.4514,
      "step": 349
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.026825403794646263,
      "learning_rate": 0.01854348730753225,
      "loss": 0.5698,
      "step": 350
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.01578792929649353,
      "learning_rate": 0.01853932584269663,
      "loss": 0.2073,
      "step": 351
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.010524454526603222,
      "learning_rate": 0.018535164377861006,
      "loss": 0.2559,
      "step": 352
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.010751763358712196,
      "learning_rate": 0.018531002913025386,
      "loss": 0.1526,
      "step": 353
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.015715204179286957,
      "learning_rate": 0.018526841448189765,
      "loss": 0.4263,
      "step": 354
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.01730133220553398,
      "learning_rate": 0.01852267998335414,
      "loss": 0.3289,
      "step": 355
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.016789477318525314,
      "learning_rate": 0.018518518518518517,
      "loss": 0.209,
      "step": 356
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.01814727671444416,
      "learning_rate": 0.018514357053682897,
      "loss": 0.2617,
      "step": 357
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.02244481071829796,
      "learning_rate": 0.018510195588847273,
      "loss": 0.5991,
      "step": 358
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.01240987703204155,
      "learning_rate": 0.018506034124011653,
      "loss": 0.187,
      "step": 359
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.01305888220667839,
      "learning_rate": 0.018501872659176032,
      "loss": 0.1776,
      "step": 360
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.015885083004832268,
      "learning_rate": 0.01849771119434041,
      "loss": 0.2406,
      "step": 361
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.018674463033676147,
      "learning_rate": 0.018493549729504784,
      "loss": 0.4717,
      "step": 362
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.011706807650625706,
      "learning_rate": 0.018489388264669164,
      "loss": 0.1733,
      "step": 363
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.012210343964397907,
      "learning_rate": 0.01848522679983354,
      "loss": 0.0668,
      "step": 364
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.023025935515761375,
      "learning_rate": 0.01848106533499792,
      "loss": 0.4929,
      "step": 365
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.019617287442088127,
      "learning_rate": 0.0184769038701623,
      "loss": 0.3542,
      "step": 366
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.019019676372408867,
      "learning_rate": 0.018472742405326675,
      "loss": 0.1937,
      "step": 367
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.019696107134222984,
      "learning_rate": 0.018468580940491055,
      "loss": 0.5415,
      "step": 368
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.01248240377753973,
      "learning_rate": 0.01846441947565543,
      "loss": 0.3943,
      "step": 369
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.01081449817866087,
      "learning_rate": 0.018460258010819807,
      "loss": 0.1473,
      "step": 370
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.02130720019340515,
      "learning_rate": 0.018456096545984187,
      "loss": 0.2279,
      "step": 371
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.023912722244858742,
      "learning_rate": 0.018451935081148566,
      "loss": 0.623,
      "step": 372
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.013592472299933434,
      "learning_rate": 0.018447773616312942,
      "loss": 0.1094,
      "step": 373
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.01946699246764183,
      "learning_rate": 0.018443612151477322,
      "loss": 0.3538,
      "step": 374
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.01677008904516697,
      "learning_rate": 0.018439450686641698,
      "loss": 0.5879,
      "step": 375
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.014828789979219437,
      "learning_rate": 0.018435289221806074,
      "loss": 0.4316,
      "step": 376
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.020289145410060883,
      "learning_rate": 0.018431127756970454,
      "loss": 0.4058,
      "step": 377
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.02077432908117771,
      "learning_rate": 0.018426966292134833,
      "loss": 0.1469,
      "step": 378
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.010249602608382702,
      "learning_rate": 0.01842280482729921,
      "loss": 0.015,
      "step": 379
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.016949528828263283,
      "learning_rate": 0.01841864336246359,
      "loss": 0.2949,
      "step": 380
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.01481599546968937,
      "learning_rate": 0.018414481897627965,
      "loss": 0.1646,
      "step": 381
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.030077632516622543,
      "learning_rate": 0.01841032043279234,
      "loss": 0.4893,
      "step": 382
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.013509195297956467,
      "learning_rate": 0.01840615896795672,
      "loss": 0.1575,
      "step": 383
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.015224408358335495,
      "learning_rate": 0.0184019975031211,
      "loss": 0.4365,
      "step": 384
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.019919302314519882,
      "learning_rate": 0.018397836038285476,
      "loss": 0.2722,
      "step": 385
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.011082062497735023,
      "learning_rate": 0.018393674573449856,
      "loss": 0.259,
      "step": 386
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.028045201674103737,
      "learning_rate": 0.018389513108614232,
      "loss": 0.2676,
      "step": 387
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.027130700647830963,
      "learning_rate": 0.018385351643778608,
      "loss": 0.3445,
      "step": 388
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.013158266432583332,
      "learning_rate": 0.018381190178942988,
      "loss": 0.1779,
      "step": 389
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.021059006452560425,
      "learning_rate": 0.018377028714107367,
      "loss": 0.0562,
      "step": 390
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.009331466630101204,
      "learning_rate": 0.018372867249271743,
      "loss": 0.1804,
      "step": 391
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.012068395502865314,
      "learning_rate": 0.018368705784436123,
      "loss": 0.2776,
      "step": 392
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.026677824556827545,
      "learning_rate": 0.0183645443196005,
      "loss": 0.3213,
      "step": 393
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.029340950772166252,
      "learning_rate": 0.018360382854764875,
      "loss": 0.3765,
      "step": 394
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.015520433895289898,
      "learning_rate": 0.018356221389929255,
      "loss": 0.2681,
      "step": 395
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.02110999822616577,
      "learning_rate": 0.018352059925093634,
      "loss": 0.2029,
      "step": 396
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.01838153973221779,
      "learning_rate": 0.01834789846025801,
      "loss": 0.0622,
      "step": 397
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.01988046057522297,
      "learning_rate": 0.01834373699542239,
      "loss": 0.3376,
      "step": 398
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.020841067656874657,
      "learning_rate": 0.018339575530586766,
      "loss": 0.4333,
      "step": 399
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.009150462225079536,
      "learning_rate": 0.018335414065751145,
      "loss": 0.0217,
      "step": 400
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.028743427246809006,
      "learning_rate": 0.01833125260091552,
      "loss": 0.0804,
      "step": 401
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.013679815456271172,
      "learning_rate": 0.0183270911360799,
      "loss": 0.1171,
      "step": 402
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.019026050344109535,
      "learning_rate": 0.01832292967124428,
      "loss": 0.2389,
      "step": 403
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.025706058368086815,
      "learning_rate": 0.018318768206408657,
      "loss": 0.6992,
      "step": 404
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.0252694021910429,
      "learning_rate": 0.018314606741573033,
      "loss": 0.4209,
      "step": 405
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.024600857868790627,
      "learning_rate": 0.018310445276737412,
      "loss": 0.6948,
      "step": 406
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.022102979943156242,
      "learning_rate": 0.01830628381190179,
      "loss": 0.3425,
      "step": 407
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.0011589693604037166,
      "learning_rate": 0.018302122347066168,
      "loss": 0.0023,
      "step": 408
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.01646098494529724,
      "learning_rate": 0.018297960882230548,
      "loss": 0.2659,
      "step": 409
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.01534626167267561,
      "learning_rate": 0.018293799417394924,
      "loss": 0.3,
      "step": 410
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.022580618038773537,
      "learning_rate": 0.018289637952559303,
      "loss": 0.4175,
      "step": 411
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.017710313200950623,
      "learning_rate": 0.01828547648772368,
      "loss": 0.4795,
      "step": 412
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.015363357961177826,
      "learning_rate": 0.018281315022888055,
      "loss": 0.1886,
      "step": 413
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.021407626569271088,
      "learning_rate": 0.018277153558052435,
      "loss": 0.1099,
      "step": 414
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.01610243320465088,
      "learning_rate": 0.018272992093216815,
      "loss": 0.4968,
      "step": 415
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.03274524584412575,
      "learning_rate": 0.01826883062838119,
      "loss": 0.7466,
      "step": 416
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.028339523822069168,
      "learning_rate": 0.01826466916354557,
      "loss": 0.7749,
      "step": 417
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.013688563369214535,
      "learning_rate": 0.018260507698709946,
      "loss": 0.0635,
      "step": 418
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.021380651742219925,
      "learning_rate": 0.018256346233874322,
      "loss": 0.4131,
      "step": 419
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.026248306035995483,
      "learning_rate": 0.018252184769038702,
      "loss": 0.2974,
      "step": 420
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.02018003910779953,
      "learning_rate": 0.01824802330420308,
      "loss": 0.2115,
      "step": 421
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.024361971765756607,
      "learning_rate": 0.018243861839367458,
      "loss": 0.6206,
      "step": 422
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.015359626151621342,
      "learning_rate": 0.018239700374531837,
      "loss": 0.1203,
      "step": 423
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.016143817454576492,
      "learning_rate": 0.018235538909696213,
      "loss": 0.2698,
      "step": 424
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.015773026272654533,
      "learning_rate": 0.01823137744486059,
      "loss": 0.1071,
      "step": 425
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.011392384767532349,
      "learning_rate": 0.01822721598002497,
      "loss": 0.1879,
      "step": 426
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.012423066422343254,
      "learning_rate": 0.01822305451518935,
      "loss": 0.0672,
      "step": 427
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.029530595988035202,
      "learning_rate": 0.018218893050353725,
      "loss": 0.3613,
      "step": 428
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.024436235427856445,
      "learning_rate": 0.018214731585518104,
      "loss": 0.3896,
      "step": 429
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.016348615288734436,
      "learning_rate": 0.01821057012068248,
      "loss": 0.3174,
      "step": 430
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.015601023100316525,
      "learning_rate": 0.018206408655846856,
      "loss": 0.2079,
      "step": 431
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.014723938889801502,
      "learning_rate": 0.018202247191011236,
      "loss": 0.2329,
      "step": 432
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.024064524099230766,
      "learning_rate": 0.018198085726175615,
      "loss": 0.4507,
      "step": 433
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.025292137637734413,
      "learning_rate": 0.01819392426133999,
      "loss": 0.6035,
      "step": 434
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.022207777947187424,
      "learning_rate": 0.01818976279650437,
      "loss": 0.7056,
      "step": 435
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.022870400920510292,
      "learning_rate": 0.018185601331668747,
      "loss": 0.2622,
      "step": 436
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.020909421145915985,
      "learning_rate": 0.018181439866833123,
      "loss": 0.2377,
      "step": 437
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.021142246201634407,
      "learning_rate": 0.018177278401997503,
      "loss": 0.2996,
      "step": 438
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.01641058176755905,
      "learning_rate": 0.018173116937161882,
      "loss": 0.3174,
      "step": 439
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.021069413051009178,
      "learning_rate": 0.01816895547232626,
      "loss": 0.2654,
      "step": 440
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.025653362274169922,
      "learning_rate": 0.018164794007490638,
      "loss": 0.4221,
      "step": 441
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.01986721344292164,
      "learning_rate": 0.018160632542655014,
      "loss": 0.3657,
      "step": 442
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.021333815529942513,
      "learning_rate": 0.018156471077819394,
      "loss": 0.467,
      "step": 443
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.018084486946463585,
      "learning_rate": 0.01815230961298377,
      "loss": 0.2644,
      "step": 444
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.028349634259939194,
      "learning_rate": 0.01814814814814815,
      "loss": 0.5391,
      "step": 445
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.022226015105843544,
      "learning_rate": 0.018143986683312525,
      "loss": 0.5645,
      "step": 446
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.015429804101586342,
      "learning_rate": 0.018139825218476905,
      "loss": 0.0782,
      "step": 447
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.024509834125638008,
      "learning_rate": 0.01813566375364128,
      "loss": 0.4612,
      "step": 448
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.011498290114104748,
      "learning_rate": 0.01813150228880566,
      "loss": 0.1084,
      "step": 449
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.016364967450499535,
      "learning_rate": 0.018127340823970037,
      "loss": 0.3008,
      "step": 450
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.0184890478849411,
      "learning_rate": 0.018123179359134416,
      "loss": 0.2186,
      "step": 451
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.01742672361433506,
      "learning_rate": 0.018119017894298792,
      "loss": 0.243,
      "step": 452
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.01943567395210266,
      "learning_rate": 0.018114856429463172,
      "loss": 0.2913,
      "step": 453
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.024606555700302124,
      "learning_rate": 0.01811069496462755,
      "loss": 0.0767,
      "step": 454
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.008739195764064789,
      "learning_rate": 0.018106533499791928,
      "loss": 0.0287,
      "step": 455
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.01797635853290558,
      "learning_rate": 0.018102372034956304,
      "loss": 0.1814,
      "step": 456
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.015862181782722473,
      "learning_rate": 0.018098210570120683,
      "loss": 0.4111,
      "step": 457
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.022920627146959305,
      "learning_rate": 0.01809404910528506,
      "loss": 0.375,
      "step": 458
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.01702135242521763,
      "learning_rate": 0.01808988764044944,
      "loss": 0.3601,
      "step": 459
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.016336023807525635,
      "learning_rate": 0.01808572617561382,
      "loss": 0.1782,
      "step": 460
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.015450933948159218,
      "learning_rate": 0.018081564710778195,
      "loss": 0.2888,
      "step": 461
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.02359917387366295,
      "learning_rate": 0.01807740324594257,
      "loss": 0.313,
      "step": 462
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.011471382342278957,
      "learning_rate": 0.01807324178110695,
      "loss": 0.1418,
      "step": 463
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.019025051966309547,
      "learning_rate": 0.018069080316271326,
      "loss": 0.3232,
      "step": 464
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.0016236061928793788,
      "learning_rate": 0.018064918851435706,
      "loss": 0.0021,
      "step": 465
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.018086018040776253,
      "learning_rate": 0.018060757386600085,
      "loss": 0.3213,
      "step": 466
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.014555457048118114,
      "learning_rate": 0.01805659592176446,
      "loss": 0.144,
      "step": 467
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.01344548910856247,
      "learning_rate": 0.018052434456928838,
      "loss": 0.1176,
      "step": 468
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.014173297211527824,
      "learning_rate": 0.018048272992093217,
      "loss": 0.0638,
      "step": 469
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.021349281072616577,
      "learning_rate": 0.018044111527257593,
      "loss": 0.1447,
      "step": 470
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.02288207970559597,
      "learning_rate": 0.018039950062421973,
      "loss": 0.3296,
      "step": 471
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.0221555233001709,
      "learning_rate": 0.018035788597586352,
      "loss": 0.2883,
      "step": 472
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.020956367254257202,
      "learning_rate": 0.01803162713275073,
      "loss": 0.2776,
      "step": 473
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.03914099186658859,
      "learning_rate": 0.018027465667915105,
      "loss": 0.2311,
      "step": 474
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.016052138060331345,
      "learning_rate": 0.018023304203079484,
      "loss": 0.2822,
      "step": 475
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.02345450036227703,
      "learning_rate": 0.018019142738243864,
      "loss": 0.4175,
      "step": 476
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.015222625806927681,
      "learning_rate": 0.01801498127340824,
      "loss": 0.2379,
      "step": 477
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.01661856472492218,
      "learning_rate": 0.01801081980857262,
      "loss": 0.142,
      "step": 478
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.027985790744423866,
      "learning_rate": 0.018006658343736996,
      "loss": 0.2676,
      "step": 479
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.005155233666300774,
      "learning_rate": 0.01800249687890137,
      "loss": 0.0064,
      "step": 480
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.01854134164750576,
      "learning_rate": 0.01799833541406575,
      "loss": 0.3274,
      "step": 481
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.021399671211838722,
      "learning_rate": 0.01799417394923013,
      "loss": 0.3025,
      "step": 482
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.013370412401854992,
      "learning_rate": 0.017990012484394507,
      "loss": 0.22,
      "step": 483
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.02801896072924137,
      "learning_rate": 0.017985851019558886,
      "loss": 0.5527,
      "step": 484
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.016469566151499748,
      "learning_rate": 0.017981689554723262,
      "loss": 0.3855,
      "step": 485
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.018887661397457123,
      "learning_rate": 0.017977528089887642,
      "loss": 0.1444,
      "step": 486
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.016790995374321938,
      "learning_rate": 0.017973366625052018,
      "loss": 0.1682,
      "step": 487
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.01708918623626232,
      "learning_rate": 0.017969205160216398,
      "loss": 0.2532,
      "step": 488
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.013119114562869072,
      "learning_rate": 0.017965043695380774,
      "loss": 0.2593,
      "step": 489
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.01730186864733696,
      "learning_rate": 0.017960882230545153,
      "loss": 0.4368,
      "step": 490
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.0007973984465934336,
      "learning_rate": 0.01795672076570953,
      "loss": 0.0015,
      "step": 491
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.016564833000302315,
      "learning_rate": 0.01795255930087391,
      "loss": 0.1823,
      "step": 492
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.017495324835181236,
      "learning_rate": 0.017948397836038285,
      "loss": 0.2197,
      "step": 493
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.013706967234611511,
      "learning_rate": 0.017944236371202665,
      "loss": 0.0506,
      "step": 494
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.02236577868461609,
      "learning_rate": 0.01794007490636704,
      "loss": 0.0427,
      "step": 495
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.016671421006321907,
      "learning_rate": 0.01793591344153142,
      "loss": 0.4666,
      "step": 496
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.021448083221912384,
      "learning_rate": 0.0179317519766958,
      "loss": 0.2856,
      "step": 497
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.022695811465382576,
      "learning_rate": 0.017927590511860176,
      "loss": 0.4028,
      "step": 498
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.013902803882956505,
      "learning_rate": 0.017923429047024552,
      "loss": 0.386,
      "step": 499
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.013315980322659016,
      "learning_rate": 0.01791926758218893,
      "loss": 0.0199,
      "step": 500
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.01322450116276741,
      "learning_rate": 0.017915106117353308,
      "loss": 0.0723,
      "step": 501
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.026917418465018272,
      "learning_rate": 0.017910944652517687,
      "loss": 0.4121,
      "step": 502
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.017656033858656883,
      "learning_rate": 0.017906783187682067,
      "loss": 0.166,
      "step": 503
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.016564955934882164,
      "learning_rate": 0.017902621722846443,
      "loss": 0.4065,
      "step": 504
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.008763215504586697,
      "learning_rate": 0.01789846025801082,
      "loss": 0.0299,
      "step": 505
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.0215713270008564,
      "learning_rate": 0.0178942987931752,
      "loss": 0.4661,
      "step": 506
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.011343676596879959,
      "learning_rate": 0.017890137328339575,
      "loss": 0.1272,
      "step": 507
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.021843140944838524,
      "learning_rate": 0.017885975863503954,
      "loss": 0.3967,
      "step": 508
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.005304041784256697,
      "learning_rate": 0.017881814398668334,
      "loss": 0.0106,
      "step": 509
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.014229088090360165,
      "learning_rate": 0.01787765293383271,
      "loss": 0.1688,
      "step": 510
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.02197921648621559,
      "learning_rate": 0.017873491468997086,
      "loss": 0.3103,
      "step": 511
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.0247966218739748,
      "learning_rate": 0.017869330004161466,
      "loss": 0.2664,
      "step": 512
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.017200419679284096,
      "learning_rate": 0.01786516853932584,
      "loss": 0.1678,
      "step": 513
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.01293894462287426,
      "learning_rate": 0.01786100707449022,
      "loss": 0.136,
      "step": 514
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.015838829800486565,
      "learning_rate": 0.0178568456096546,
      "loss": 0.2961,
      "step": 515
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.014234489761292934,
      "learning_rate": 0.017852684144818977,
      "loss": 0.4495,
      "step": 516
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.022854570299386978,
      "learning_rate": 0.017848522679983353,
      "loss": 0.1954,
      "step": 517
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.03496082127094269,
      "learning_rate": 0.017844361215147733,
      "loss": 0.8726,
      "step": 518
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.007512242998927832,
      "learning_rate": 0.01784019975031211,
      "loss": 0.0353,
      "step": 519
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.012978347018361092,
      "learning_rate": 0.017836038285476488,
      "loss": 0.47,
      "step": 520
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.0132742365822196,
      "learning_rate": 0.017831876820640868,
      "loss": 0.0959,
      "step": 521
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.023542677983641624,
      "learning_rate": 0.017827715355805244,
      "loss": 0.217,
      "step": 522
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.01400215644389391,
      "learning_rate": 0.01782355389096962,
      "loss": 0.0919,
      "step": 523
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.010237505659461021,
      "learning_rate": 0.017819392426134,
      "loss": 0.0629,
      "step": 524
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.015069641172885895,
      "learning_rate": 0.017815230961298376,
      "loss": 0.1357,
      "step": 525
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.011609483510255814,
      "learning_rate": 0.017811069496462755,
      "loss": 0.0459,
      "step": 526
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.01928263157606125,
      "learning_rate": 0.017806908031627135,
      "loss": 0.3618,
      "step": 527
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.015118389390408993,
      "learning_rate": 0.01780274656679151,
      "loss": 0.1812,
      "step": 528
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.030556663870811462,
      "learning_rate": 0.01779858510195589,
      "loss": 0.4568,
      "step": 529
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.02257608063519001,
      "learning_rate": 0.017794423637120266,
      "loss": 0.2603,
      "step": 530
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.02450978383421898,
      "learning_rate": 0.017790262172284643,
      "loss": 0.3921,
      "step": 531
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.018064459785819054,
      "learning_rate": 0.017786100707449022,
      "loss": 0.1653,
      "step": 532
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.014579844661056995,
      "learning_rate": 0.0177819392426134,
      "loss": 0.5142,
      "step": 533
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.018851060420274734,
      "learning_rate": 0.017777777777777778,
      "loss": 0.2979,
      "step": 534
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.01819692738354206,
      "learning_rate": 0.017773616312942157,
      "loss": 0.4919,
      "step": 535
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.012211748398840427,
      "learning_rate": 0.017769454848106533,
      "loss": 0.3062,
      "step": 536
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.019866351038217545,
      "learning_rate": 0.01776529338327091,
      "loss": 0.5698,
      "step": 537
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.01991969533264637,
      "learning_rate": 0.01776113191843529,
      "loss": 0.4614,
      "step": 538
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.01758577860891819,
      "learning_rate": 0.01775697045359967,
      "loss": 0.1968,
      "step": 539
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.011264219880104065,
      "learning_rate": 0.017752808988764045,
      "loss": 0.0826,
      "step": 540
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.018839716911315918,
      "learning_rate": 0.017748647523928424,
      "loss": 0.4075,
      "step": 541
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.012899573892354965,
      "learning_rate": 0.0177444860590928,
      "loss": 0.2368,
      "step": 542
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.02240430936217308,
      "learning_rate": 0.017740324594257176,
      "loss": 0.2886,
      "step": 543
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.01477088499814272,
      "learning_rate": 0.017736163129421556,
      "loss": 0.0632,
      "step": 544
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.0271300058811903,
      "learning_rate": 0.017732001664585936,
      "loss": 0.98,
      "step": 545
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.03747643902897835,
      "learning_rate": 0.01772784019975031,
      "loss": 0.2114,
      "step": 546
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.013209293596446514,
      "learning_rate": 0.01772367873491469,
      "loss": 0.1293,
      "step": 547
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.023283572867512703,
      "learning_rate": 0.017719517270079067,
      "loss": 0.1272,
      "step": 548
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.02200518362224102,
      "learning_rate": 0.017715355805243447,
      "loss": 0.6167,
      "step": 549
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.018459653481841087,
      "learning_rate": 0.017711194340407823,
      "loss": 0.1938,
      "step": 550
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.01839401386678219,
      "learning_rate": 0.017707032875572203,
      "loss": 0.4368,
      "step": 551
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.031117385253310204,
      "learning_rate": 0.017702871410736582,
      "loss": 0.5078,
      "step": 552
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.016220442950725555,
      "learning_rate": 0.017698709945900958,
      "loss": 0.2343,
      "step": 553
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.028234465047717094,
      "learning_rate": 0.017694548481065334,
      "loss": 0.4602,
      "step": 554
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.021623658016324043,
      "learning_rate": 0.017690387016229714,
      "loss": 0.241,
      "step": 555
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.018278444185853004,
      "learning_rate": 0.01768622555139409,
      "loss": 0.2126,
      "step": 556
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.018106509000062943,
      "learning_rate": 0.01768206408655847,
      "loss": 0.4719,
      "step": 557
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.012071166187524796,
      "learning_rate": 0.01767790262172285,
      "loss": 0.1615,
      "step": 558
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.026161981746554375,
      "learning_rate": 0.017673741156887225,
      "loss": 0.6118,
      "step": 559
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.01915688067674637,
      "learning_rate": 0.0176695796920516,
      "loss": 0.0596,
      "step": 560
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.01590966060757637,
      "learning_rate": 0.01766541822721598,
      "loss": 0.1901,
      "step": 561
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.009362030774354935,
      "learning_rate": 0.017661256762380357,
      "loss": 0.0426,
      "step": 562
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.015719180926680565,
      "learning_rate": 0.017657095297544737,
      "loss": 0.2035,
      "step": 563
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.014808082953095436,
      "learning_rate": 0.017652933832709116,
      "loss": 0.1589,
      "step": 564
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.022726433351635933,
      "learning_rate": 0.017648772367873492,
      "loss": 0.2559,
      "step": 565
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.02343447506427765,
      "learning_rate": 0.017644610903037868,
      "loss": 0.4946,
      "step": 566
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.016504652798175812,
      "learning_rate": 0.017640449438202248,
      "loss": 0.1726,
      "step": 567
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.016712628304958344,
      "learning_rate": 0.017636287973366624,
      "loss": 0.2517,
      "step": 568
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.023331880569458008,
      "learning_rate": 0.017632126508531003,
      "loss": 0.3794,
      "step": 569
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.005479931831359863,
      "learning_rate": 0.017627965043695383,
      "loss": 0.0262,
      "step": 570
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.015947291627526283,
      "learning_rate": 0.01762380357885976,
      "loss": 0.2195,
      "step": 571
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.0175675917416811,
      "learning_rate": 0.01761964211402414,
      "loss": 0.1621,
      "step": 572
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.02074829861521721,
      "learning_rate": 0.017615480649188515,
      "loss": 0.27,
      "step": 573
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.0031139347702264786,
      "learning_rate": 0.01761131918435289,
      "loss": 0.005,
      "step": 574
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.013330874964594841,
      "learning_rate": 0.01760715771951727,
      "loss": 0.2313,
      "step": 575
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.018731823191046715,
      "learning_rate": 0.01760299625468165,
      "loss": 0.3057,
      "step": 576
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.03177861496806145,
      "learning_rate": 0.017598834789846026,
      "loss": 0.3982,
      "step": 577
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.022089118137955666,
      "learning_rate": 0.017594673325010406,
      "loss": 0.3254,
      "step": 578
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.010733268223702908,
      "learning_rate": 0.017590511860174782,
      "loss": 0.0645,
      "step": 579
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.022312788292765617,
      "learning_rate": 0.017586350395339158,
      "loss": 0.4197,
      "step": 580
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.020953146740794182,
      "learning_rate": 0.017582188930503537,
      "loss": 0.4355,
      "step": 581
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.014730071648955345,
      "learning_rate": 0.017578027465667917,
      "loss": 0.3032,
      "step": 582
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.008113733492791653,
      "learning_rate": 0.017573866000832293,
      "loss": 0.0445,
      "step": 583
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.015508532524108887,
      "learning_rate": 0.017569704535996673,
      "loss": 0.1904,
      "step": 584
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.028984874486923218,
      "learning_rate": 0.01756554307116105,
      "loss": 0.522,
      "step": 585
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.017001159489154816,
      "learning_rate": 0.017561381606325425,
      "loss": 0.5171,
      "step": 586
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.006894498132169247,
      "learning_rate": 0.017557220141489804,
      "loss": 0.0688,
      "step": 587
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.020242296159267426,
      "learning_rate": 0.017553058676654184,
      "loss": 0.3547,
      "step": 588
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.02047143317759037,
      "learning_rate": 0.01754889721181856,
      "loss": 0.1675,
      "step": 589
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.026067951694130898,
      "learning_rate": 0.01754473574698294,
      "loss": 0.1515,
      "step": 590
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.030283140018582344,
      "learning_rate": 0.017540574282147316,
      "loss": 0.5273,
      "step": 591
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.018824715167284012,
      "learning_rate": 0.017536412817311692,
      "loss": 0.4636,
      "step": 592
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.013927336782217026,
      "learning_rate": 0.01753225135247607,
      "loss": 0.332,
      "step": 593
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.014410550706088543,
      "learning_rate": 0.01752808988764045,
      "loss": 0.1453,
      "step": 594
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.0069348462857306,
      "learning_rate": 0.017523928422804827,
      "loss": 0.0132,
      "step": 595
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.014951486140489578,
      "learning_rate": 0.017519766957969207,
      "loss": 0.2013,
      "step": 596
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.023131584748625755,
      "learning_rate": 0.017515605493133583,
      "loss": 0.5049,
      "step": 597
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.016293618828058243,
      "learning_rate": 0.01751144402829796,
      "loss": 0.303,
      "step": 598
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.011231700889766216,
      "learning_rate": 0.01750728256346234,
      "loss": 0.1244,
      "step": 599
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.02238783799111843,
      "learning_rate": 0.017503121098626718,
      "loss": 0.4756,
      "step": 600
    },
    {
      "epoch": 0.75,
      "eval_loss": 0.2890625,
      "eval_runtime": 183.0178,
      "eval_samples_per_second": 1.098,
      "eval_steps_per_second": 0.552,
      "step": 600
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.007596211973577738,
      "learning_rate": 0.017498959633791094,
      "loss": 0.0335,
      "step": 601
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.023596379905939102,
      "learning_rate": 0.017494798168955474,
      "loss": 0.281,
      "step": 602
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.015985485166311264,
      "learning_rate": 0.01749063670411985,
      "loss": 0.1886,
      "step": 603
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.017524354159832,
      "learning_rate": 0.01748647523928423,
      "loss": 0.3987,
      "step": 604
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.03007538802921772,
      "learning_rate": 0.017482313774448605,
      "loss": 0.2864,
      "step": 605
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.019697928801178932,
      "learning_rate": 0.017478152309612985,
      "loss": 0.2177,
      "step": 606
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.029588017612695694,
      "learning_rate": 0.01747399084477736,
      "loss": 0.1004,
      "step": 607
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.010724203661084175,
      "learning_rate": 0.01746982937994174,
      "loss": 0.0515,
      "step": 608
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.011881942860782146,
      "learning_rate": 0.017465667915106117,
      "loss": 0.0547,
      "step": 609
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.03622346371412277,
      "learning_rate": 0.017461506450270496,
      "loss": 0.5923,
      "step": 610
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.019429568201303482,
      "learning_rate": 0.017457344985434872,
      "loss": 0.2776,
      "step": 611
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.028999803587794304,
      "learning_rate": 0.017453183520599252,
      "loss": 0.25,
      "step": 612
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.012266678735613823,
      "learning_rate": 0.017449022055763628,
      "loss": 0.0726,
      "step": 613
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.007757336366921663,
      "learning_rate": 0.017444860590928007,
      "loss": 0.0082,
      "step": 614
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.023186028003692627,
      "learning_rate": 0.017440699126092387,
      "loss": 0.4475,
      "step": 615
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.01969732716679573,
      "learning_rate": 0.017436537661256763,
      "loss": 0.0712,
      "step": 616
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.03147481009364128,
      "learning_rate": 0.01743237619642114,
      "loss": 0.3835,
      "step": 617
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.03012148104608059,
      "learning_rate": 0.01742821473158552,
      "loss": 0.4275,
      "step": 618
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.022663293406367302,
      "learning_rate": 0.0174240532667499,
      "loss": 0.4573,
      "step": 619
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.015420160256326199,
      "learning_rate": 0.017419891801914274,
      "loss": 0.068,
      "step": 620
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.014434471726417542,
      "learning_rate": 0.017415730337078654,
      "loss": 0.1478,
      "step": 621
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.013026262633502483,
      "learning_rate": 0.01741156887224303,
      "loss": 0.0424,
      "step": 622
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.02457118220627308,
      "learning_rate": 0.017407407407407406,
      "loss": 0.4966,
      "step": 623
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.017207419499754906,
      "learning_rate": 0.017403245942571786,
      "loss": 0.332,
      "step": 624
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.020753245800733566,
      "learning_rate": 0.017399084477736165,
      "loss": 0.2368,
      "step": 625
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.007681884802877903,
      "learning_rate": 0.01739492301290054,
      "loss": 0.0363,
      "step": 626
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.020835790783166885,
      "learning_rate": 0.01739076154806492,
      "loss": 0.4446,
      "step": 627
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.02038772776722908,
      "learning_rate": 0.017386600083229297,
      "loss": 0.3643,
      "step": 628
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.015672018751502037,
      "learning_rate": 0.017382438618393673,
      "loss": 0.1254,
      "step": 629
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.033103521913290024,
      "learning_rate": 0.017378277153558053,
      "loss": 0.4282,
      "step": 630
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.00956597737967968,
      "learning_rate": 0.017374115688722432,
      "loss": 0.0905,
      "step": 631
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.011758465319871902,
      "learning_rate": 0.01736995422388681,
      "loss": 0.0837,
      "step": 632
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.007047138176858425,
      "learning_rate": 0.017365792759051188,
      "loss": 0.0301,
      "step": 633
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.011444054543972015,
      "learning_rate": 0.017361631294215564,
      "loss": 0.2379,
      "step": 634
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.020829778164625168,
      "learning_rate": 0.01735746982937994,
      "loss": 0.2778,
      "step": 635
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.0010183991398662329,
      "learning_rate": 0.01735330836454432,
      "loss": 0.0013,
      "step": 636
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.023843087255954742,
      "learning_rate": 0.0173491468997087,
      "loss": 0.2979,
      "step": 637
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.015374841168522835,
      "learning_rate": 0.017344985434873075,
      "loss": 0.1357,
      "step": 638
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.01591675356030464,
      "learning_rate": 0.017340823970037455,
      "loss": 0.2201,
      "step": 639
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.009581562131643295,
      "learning_rate": 0.01733666250520183,
      "loss": 0.0497,
      "step": 640
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.04708545282483101,
      "learning_rate": 0.017332501040366207,
      "loss": 0.3418,
      "step": 641
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.02598581276834011,
      "learning_rate": 0.017328339575530587,
      "loss": 0.5991,
      "step": 642
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.021948708221316338,
      "learning_rate": 0.017324178110694966,
      "loss": 0.2468,
      "step": 643
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.0162848848849535,
      "learning_rate": 0.017320016645859342,
      "loss": 0.2642,
      "step": 644
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.01911388337612152,
      "learning_rate": 0.017315855181023722,
      "loss": 0.4583,
      "step": 645
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.012276459485292435,
      "learning_rate": 0.017311693716188098,
      "loss": 0.2476,
      "step": 646
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.020888086408376694,
      "learning_rate": 0.017307532251352477,
      "loss": 0.3142,
      "step": 647
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.01172264851629734,
      "learning_rate": 0.017303370786516854,
      "loss": 0.0508,
      "step": 648
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.016026314347982407,
      "learning_rate": 0.017299209321681233,
      "loss": 0.28,
      "step": 649
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.022636111825704575,
      "learning_rate": 0.01729504785684561,
      "loss": 0.4592,
      "step": 650
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.01944619044661522,
      "learning_rate": 0.01729088639200999,
      "loss": 0.4524,
      "step": 651
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.017689630389213562,
      "learning_rate": 0.017286724927174365,
      "loss": 0.2343,
      "step": 652
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.009402558207511902,
      "learning_rate": 0.017282563462338744,
      "loss": 0.1232,
      "step": 653
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.02710762619972229,
      "learning_rate": 0.01727840199750312,
      "loss": 0.2598,
      "step": 654
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.028154436498880386,
      "learning_rate": 0.0172742405326675,
      "loss": 0.3254,
      "step": 655
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.013529193587601185,
      "learning_rate": 0.017270079067831876,
      "loss": 0.2168,
      "step": 656
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.01863241009414196,
      "learning_rate": 0.017265917602996256,
      "loss": 0.4363,
      "step": 657
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.01757151633501053,
      "learning_rate": 0.017261756138160635,
      "loss": 0.3752,
      "step": 658
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.01853569597005844,
      "learning_rate": 0.01725759467332501,
      "loss": 0.1473,
      "step": 659
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.00975044071674347,
      "learning_rate": 0.017253433208489388,
      "loss": 0.0475,
      "step": 660
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.010218817740678787,
      "learning_rate": 0.017249271743653767,
      "loss": 0.0728,
      "step": 661
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.025808019563555717,
      "learning_rate": 0.017245110278818143,
      "loss": 0.5747,
      "step": 662
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.01887434720993042,
      "learning_rate": 0.017240948813982523,
      "loss": 0.2776,
      "step": 663
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.019917313009500504,
      "learning_rate": 0.017236787349146902,
      "loss": 0.3391,
      "step": 664
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.020668426528573036,
      "learning_rate": 0.01723262588431128,
      "loss": 0.1902,
      "step": 665
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.013723928481340408,
      "learning_rate": 0.017228464419475654,
      "loss": 0.103,
      "step": 666
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.002858448540791869,
      "learning_rate": 0.017224302954640034,
      "loss": 0.0057,
      "step": 667
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.010656685568392277,
      "learning_rate": 0.01722014148980441,
      "loss": 0.0876,
      "step": 668
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.02592632547020912,
      "learning_rate": 0.01721598002496879,
      "loss": 0.7422,
      "step": 669
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.015538211911916733,
      "learning_rate": 0.01721181856013317,
      "loss": 0.2396,
      "step": 670
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.02267232909798622,
      "learning_rate": 0.017207657095297545,
      "loss": 0.2925,
      "step": 671
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.018011275678873062,
      "learning_rate": 0.01720349563046192,
      "loss": 0.4634,
      "step": 672
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.01288380566984415,
      "learning_rate": 0.0171993341656263,
      "loss": 0.1312,
      "step": 673
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.024027708917856216,
      "learning_rate": 0.017195172700790677,
      "loss": 0.2993,
      "step": 674
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.007085581310093403,
      "learning_rate": 0.017191011235955057,
      "loss": 0.0382,
      "step": 675
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.02409108355641365,
      "learning_rate": 0.017186849771119436,
      "loss": 0.1493,
      "step": 676
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.018357884138822556,
      "learning_rate": 0.017182688306283812,
      "loss": 0.2805,
      "step": 677
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.017132099717855453,
      "learning_rate": 0.01717852684144819,
      "loss": 0.249,
      "step": 678
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.02037871442735195,
      "learning_rate": 0.017174365376612568,
      "loss": 0.5376,
      "step": 679
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.023976625874638557,
      "learning_rate": 0.017170203911776944,
      "loss": 0.1683,
      "step": 680
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.034002043306827545,
      "learning_rate": 0.017166042446941324,
      "loss": 0.2203,
      "step": 681
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.013092076405882835,
      "learning_rate": 0.017161880982105703,
      "loss": 0.0602,
      "step": 682
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.01644989661872387,
      "learning_rate": 0.01715771951727008,
      "loss": 0.2192,
      "step": 683
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.020339403301477432,
      "learning_rate": 0.017153558052434455,
      "loss": 0.4336,
      "step": 684
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.00023564025468658656,
      "learning_rate": 0.017149396587598835,
      "loss": 0.0004,
      "step": 685
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.018476612865924835,
      "learning_rate": 0.01714523512276321,
      "loss": 0.2593,
      "step": 686
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.020325573161244392,
      "learning_rate": 0.01714107365792759,
      "loss": 0.2556,
      "step": 687
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.024439135566353798,
      "learning_rate": 0.01713691219309197,
      "loss": 0.2673,
      "step": 688
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.024870112538337708,
      "learning_rate": 0.017132750728256346,
      "loss": 0.5039,
      "step": 689
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.02139340527355671,
      "learning_rate": 0.017128589263420726,
      "loss": 0.2881,
      "step": 690
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.019896239042282104,
      "learning_rate": 0.017124427798585102,
      "loss": 0.261,
      "step": 691
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.021802013739943504,
      "learning_rate": 0.01712026633374948,
      "loss": 0.3359,
      "step": 692
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.024331288412213326,
      "learning_rate": 0.017116104868913858,
      "loss": 0.5405,
      "step": 693
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.0015833770157769322,
      "learning_rate": 0.017111943404078237,
      "loss": 0.0021,
      "step": 694
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.016407884657382965,
      "learning_rate": 0.017107781939242613,
      "loss": 0.3918,
      "step": 695
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.016509268432855606,
      "learning_rate": 0.017103620474406993,
      "loss": 0.1449,
      "step": 696
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.013785818591713905,
      "learning_rate": 0.01709945900957137,
      "loss": 0.0516,
      "step": 697
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.023755798116326332,
      "learning_rate": 0.01709529754473575,
      "loss": 0.2493,
      "step": 698
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.01698322221636772,
      "learning_rate": 0.017091136079900125,
      "loss": 0.2065,
      "step": 699
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.015968430787324905,
      "learning_rate": 0.017086974615064504,
      "loss": 0.153,
      "step": 700
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.02535528875887394,
      "learning_rate": 0.017082813150228884,
      "loss": 0.4792,
      "step": 701
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.013174477964639664,
      "learning_rate": 0.01707865168539326,
      "loss": 0.1099,
      "step": 702
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.016354139894247055,
      "learning_rate": 0.017074490220557636,
      "loss": 0.16,
      "step": 703
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.025194713845849037,
      "learning_rate": 0.017070328755722015,
      "loss": 0.6577,
      "step": 704
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.017464514821767807,
      "learning_rate": 0.01706616729088639,
      "loss": 0.2629,
      "step": 705
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.026380835101008415,
      "learning_rate": 0.01706200582605077,
      "loss": 0.3901,
      "step": 706
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.00990260485559702,
      "learning_rate": 0.01705784436121515,
      "loss": 0.0551,
      "step": 707
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.01918584108352661,
      "learning_rate": 0.017053682896379527,
      "loss": 0.5674,
      "step": 708
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.020656105130910873,
      "learning_rate": 0.017049521431543903,
      "loss": 0.184,
      "step": 709
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.0175065565854311,
      "learning_rate": 0.017045359966708282,
      "loss": 0.224,
      "step": 710
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.01879909820854664,
      "learning_rate": 0.01704119850187266,
      "loss": 0.1205,
      "step": 711
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.017924316227436066,
      "learning_rate": 0.017037037037037038,
      "loss": 0.3433,
      "step": 712
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.027081698179244995,
      "learning_rate": 0.017032875572201418,
      "loss": 0.7124,
      "step": 713
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.010771729983389378,
      "learning_rate": 0.017028714107365794,
      "loss": 0.0534,
      "step": 714
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.0018802395788952708,
      "learning_rate": 0.01702455264253017,
      "loss": 0.0037,
      "step": 715
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.0325336679816246,
      "learning_rate": 0.01702039117769455,
      "loss": 0.3564,
      "step": 716
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.018861383199691772,
      "learning_rate": 0.017016229712858925,
      "loss": 0.3057,
      "step": 717
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.015037810429930687,
      "learning_rate": 0.017012068248023305,
      "loss": 0.1246,
      "step": 718
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.01766980066895485,
      "learning_rate": 0.017007906783187685,
      "loss": 0.4333,
      "step": 719
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.030631523579359055,
      "learning_rate": 0.01700374531835206,
      "loss": 0.9087,
      "step": 720
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.01510903611779213,
      "learning_rate": 0.016999583853516437,
      "loss": 0.2042,
      "step": 721
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.02484944276511669,
      "learning_rate": 0.016995422388680816,
      "loss": 0.1501,
      "step": 722
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.021651616320014,
      "learning_rate": 0.016991260923845192,
      "loss": 0.5435,
      "step": 723
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.02143322303891182,
      "learning_rate": 0.016987099459009572,
      "loss": 0.303,
      "step": 724
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.021336205303668976,
      "learning_rate": 0.01698293799417395,
      "loss": 0.2406,
      "step": 725
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.027396924793720245,
      "learning_rate": 0.016978776529338328,
      "loss": 0.201,
      "step": 726
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.021244604140520096,
      "learning_rate": 0.016974615064502704,
      "loss": 0.3933,
      "step": 727
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.01537284255027771,
      "learning_rate": 0.016970453599667083,
      "loss": 0.3054,
      "step": 728
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.010397098027169704,
      "learning_rate": 0.01696629213483146,
      "loss": 0.0867,
      "step": 729
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.019817698746919632,
      "learning_rate": 0.01696213066999584,
      "loss": 0.4824,
      "step": 730
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.00739717110991478,
      "learning_rate": 0.01695796920516022,
      "loss": 0.0374,
      "step": 731
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.024609167128801346,
      "learning_rate": 0.016953807740324595,
      "loss": 0.3767,
      "step": 732
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.014955861493945122,
      "learning_rate": 0.016949646275488974,
      "loss": 0.1862,
      "step": 733
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.021667569875717163,
      "learning_rate": 0.01694548481065335,
      "loss": 0.188,
      "step": 734
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.01052304357290268,
      "learning_rate": 0.016941323345817726,
      "loss": 0.0376,
      "step": 735
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.021990731358528137,
      "learning_rate": 0.016937161880982106,
      "loss": 0.1248,
      "step": 736
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.02426757477223873,
      "learning_rate": 0.016933000416146485,
      "loss": 0.3538,
      "step": 737
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.013726136647164822,
      "learning_rate": 0.01692883895131086,
      "loss": 0.1019,
      "step": 738
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.02444680966436863,
      "learning_rate": 0.01692467748647524,
      "loss": 0.3425,
      "step": 739
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.020823568105697632,
      "learning_rate": 0.016920516021639617,
      "loss": 0.3403,
      "step": 740
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.017308739945292473,
      "learning_rate": 0.016916354556803993,
      "loss": 0.2034,
      "step": 741
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.019694337621331215,
      "learning_rate": 0.016912193091968373,
      "loss": 0.2474,
      "step": 742
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.026313232257962227,
      "learning_rate": 0.016908031627132752,
      "loss": 0.334,
      "step": 743
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.011907829903066158,
      "learning_rate": 0.01690387016229713,
      "loss": 0.1154,
      "step": 744
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.02314845472574234,
      "learning_rate": 0.016899708697461508,
      "loss": 0.4189,
      "step": 745
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.017764708027243614,
      "learning_rate": 0.016895547232625884,
      "loss": 0.3669,
      "step": 746
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.02391207590699196,
      "learning_rate": 0.01689138576779026,
      "loss": 0.5166,
      "step": 747
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.01798044703900814,
      "learning_rate": 0.01688722430295464,
      "loss": 0.2668,
      "step": 748
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.016684608533978462,
      "learning_rate": 0.01688306283811902,
      "loss": 0.1847,
      "step": 749
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.02295888401567936,
      "learning_rate": 0.016878901373283395,
      "loss": 0.4199,
      "step": 750
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.01934513822197914,
      "learning_rate": 0.016874739908447775,
      "loss": 0.4692,
      "step": 751
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.020473016425967216,
      "learning_rate": 0.01687057844361215,
      "loss": 0.2822,
      "step": 752
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.013607135973870754,
      "learning_rate": 0.016866416978776527,
      "loss": 0.0914,
      "step": 753
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.015915434807538986,
      "learning_rate": 0.016862255513940907,
      "loss": 0.0856,
      "step": 754
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.015103944577276707,
      "learning_rate": 0.016858094049105286,
      "loss": 0.2494,
      "step": 755
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.0011906675063073635,
      "learning_rate": 0.016853932584269662,
      "loss": 0.0012,
      "step": 756
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.029274487867951393,
      "learning_rate": 0.016849771119434042,
      "loss": 0.4189,
      "step": 757
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.020976923406124115,
      "learning_rate": 0.016845609654598418,
      "loss": 0.3796,
      "step": 758
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.012252322398126125,
      "learning_rate": 0.016841448189762794,
      "loss": 0.1074,
      "step": 759
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.018603961914777756,
      "learning_rate": 0.016837286724927174,
      "loss": 0.1809,
      "step": 760
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.023209350183606148,
      "learning_rate": 0.016833125260091553,
      "loss": 0.4783,
      "step": 761
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.009813662618398666,
      "learning_rate": 0.01682896379525593,
      "loss": 0.064,
      "step": 762
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.012941231951117516,
      "learning_rate": 0.01682480233042031,
      "loss": 0.1528,
      "step": 763
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.03905802220106125,
      "learning_rate": 0.016820640865584685,
      "loss": 0.6968,
      "step": 764
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.0210886150598526,
      "learning_rate": 0.016816479400749065,
      "loss": 0.3132,
      "step": 765
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.02608836255967617,
      "learning_rate": 0.01681231793591344,
      "loss": 0.3147,
      "step": 766
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.017373185604810715,
      "learning_rate": 0.01680815647107782,
      "loss": 0.3047,
      "step": 767
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.02733996883034706,
      "learning_rate": 0.0168039950062422,
      "loss": 0.2959,
      "step": 768
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.013021211139857769,
      "learning_rate": 0.016799833541406576,
      "loss": 0.0486,
      "step": 769
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.03494340926408768,
      "learning_rate": 0.016795672076570952,
      "loss": 0.6533,
      "step": 770
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.00021946057677268982,
      "learning_rate": 0.01679151061173533,
      "loss": 0.0003,
      "step": 771
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.019399845972657204,
      "learning_rate": 0.016787349146899708,
      "loss": 0.2708,
      "step": 772
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.040895044803619385,
      "learning_rate": 0.016783187682064087,
      "loss": 0.4102,
      "step": 773
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.025216389447450638,
      "learning_rate": 0.016779026217228467,
      "loss": 0.3115,
      "step": 774
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.018137844279408455,
      "learning_rate": 0.016774864752392843,
      "loss": 0.2991,
      "step": 775
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.018322160467505455,
      "learning_rate": 0.016770703287557222,
      "loss": 0.1591,
      "step": 776
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.019225873053073883,
      "learning_rate": 0.0167665418227216,
      "loss": 0.283,
      "step": 777
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.01226904895156622,
      "learning_rate": 0.016762380357885975,
      "loss": 0.1577,
      "step": 778
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.003205365501344204,
      "learning_rate": 0.016758218893050354,
      "loss": 0.0049,
      "step": 779
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.022741660475730896,
      "learning_rate": 0.016754057428214734,
      "loss": 0.355,
      "step": 780
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.01638094149529934,
      "learning_rate": 0.01674989596337911,
      "loss": 0.2808,
      "step": 781
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.025196842849254608,
      "learning_rate": 0.01674573449854349,
      "loss": 0.6519,
      "step": 782
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.012154725380241871,
      "learning_rate": 0.016741573033707866,
      "loss": 0.1292,
      "step": 783
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.010786495171487331,
      "learning_rate": 0.01673741156887224,
      "loss": 0.0632,
      "step": 784
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.017939181998372078,
      "learning_rate": 0.01673325010403662,
      "loss": 0.2375,
      "step": 785
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.021562866866588593,
      "learning_rate": 0.016729088639201,
      "loss": 0.4019,
      "step": 786
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.01772688515484333,
      "learning_rate": 0.016724927174365377,
      "loss": 0.1752,
      "step": 787
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.017077792435884476,
      "learning_rate": 0.016720765709529756,
      "loss": 0.126,
      "step": 788
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.0011668333318084478,
      "learning_rate": 0.016716604244694133,
      "loss": 0.001,
      "step": 789
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.014091866090893745,
      "learning_rate": 0.01671244277985851,
      "loss": 0.1432,
      "step": 790
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.015959175303578377,
      "learning_rate": 0.016708281315022888,
      "loss": 0.3792,
      "step": 791
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.03273507580161095,
      "learning_rate": 0.016704119850187268,
      "loss": 0.4983,
      "step": 792
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.02138064242899418,
      "learning_rate": 0.016699958385351644,
      "loss": 0.5034,
      "step": 793
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.02423279918730259,
      "learning_rate": 0.016695796920516023,
      "loss": 0.1621,
      "step": 794
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.014763862825930119,
      "learning_rate": 0.0166916354556804,
      "loss": 0.127,
      "step": 795
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.021140828728675842,
      "learning_rate": 0.016687473990844776,
      "loss": 0.4648,
      "step": 796
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.016006823629140854,
      "learning_rate": 0.016683312526009155,
      "loss": 0.2067,
      "step": 797
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.01839813031256199,
      "learning_rate": 0.016679151061173535,
      "loss": 0.3167,
      "step": 798
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.017348794266581535,
      "learning_rate": 0.01667498959633791,
      "loss": 0.1687,
      "step": 799
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.0005906976875849068,
      "learning_rate": 0.01667082813150229,
      "loss": 0.0008,
      "step": 800
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.06840900331735611,
      "learning_rate": 0.016666666666666666,
      "loss": 0.4583,
      "step": 801
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.013588778674602509,
      "learning_rate": 0.016662505201831043,
      "loss": 0.4026,
      "step": 802
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.008593741804361343,
      "learning_rate": 0.016658343736995422,
      "loss": 0.0325,
      "step": 803
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.002787481527775526,
      "learning_rate": 0.0166541822721598,
      "loss": 0.0017,
      "step": 804
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.022920945659279823,
      "learning_rate": 0.016650020807324178,
      "loss": 0.4495,
      "step": 805
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.036856673657894135,
      "learning_rate": 0.016645859342488557,
      "loss": 1.2549,
      "step": 806
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.012767734006047249,
      "learning_rate": 0.016641697877652933,
      "loss": 0.3547,
      "step": 807
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.01914498209953308,
      "learning_rate": 0.01663753641281731,
      "loss": 0.2715,
      "step": 808
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.015170822851359844,
      "learning_rate": 0.01663337494798169,
      "loss": 0.261,
      "step": 809
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.020282737910747528,
      "learning_rate": 0.01662921348314607,
      "loss": 0.181,
      "step": 810
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.01731625199317932,
      "learning_rate": 0.016625052018310445,
      "loss": 0.2266,
      "step": 811
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.016341226175427437,
      "learning_rate": 0.016620890553474824,
      "loss": 0.1481,
      "step": 812
    },
    {
      "epoch": 1.01,
      "grad_norm": 0.015706125646829605,
      "learning_rate": 0.0166167290886392,
      "loss": 0.3499,
      "step": 813
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.024310791864991188,
      "learning_rate": 0.01661256762380358,
      "loss": 0.2399,
      "step": 814
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.014865083619952202,
      "learning_rate": 0.016608406158967956,
      "loss": 0.2145,
      "step": 815
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.01781914383172989,
      "learning_rate": 0.016604244694132336,
      "loss": 0.3718,
      "step": 816
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.014720149338245392,
      "learning_rate": 0.01660008322929671,
      "loss": 0.3037,
      "step": 817
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.013470636680722237,
      "learning_rate": 0.01659592176446109,
      "loss": 0.0586,
      "step": 818
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.018091998994350433,
      "learning_rate": 0.016591760299625467,
      "loss": 0.3838,
      "step": 819
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.013013910502195358,
      "learning_rate": 0.016587598834789847,
      "loss": 0.1472,
      "step": 820
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.014777330681681633,
      "learning_rate": 0.016583437369954223,
      "loss": 0.1665,
      "step": 821
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.023757070302963257,
      "learning_rate": 0.016579275905118603,
      "loss": 0.6221,
      "step": 822
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.008868034929037094,
      "learning_rate": 0.01657511444028298,
      "loss": 0.0116,
      "step": 823
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.009676007553935051,
      "learning_rate": 0.016570952975447358,
      "loss": 0.0316,
      "step": 824
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.023485155776143074,
      "learning_rate": 0.016566791510611738,
      "loss": 0.3381,
      "step": 825
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.011961746960878372,
      "learning_rate": 0.016562630045776114,
      "loss": 0.039,
      "step": 826
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.07419464737176895,
      "learning_rate": 0.01655846858094049,
      "loss": 0.2639,
      "step": 827
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.026102904230356216,
      "learning_rate": 0.01655430711610487,
      "loss": 0.1685,
      "step": 828
    },
    {
      "epoch": 1.03,
      "grad_norm": 0.01779910735785961,
      "learning_rate": 0.016550145651269246,
      "loss": 0.1917,
      "step": 829
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.01312586572021246,
      "learning_rate": 0.016545984186433625,
      "loss": 0.1495,
      "step": 830
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.023067975416779518,
      "learning_rate": 0.016541822721598005,
      "loss": 0.481,
      "step": 831
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.01735624298453331,
      "learning_rate": 0.01653766125676238,
      "loss": 0.235,
      "step": 832
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.022691868245601654,
      "learning_rate": 0.016533499791926757,
      "loss": 0.3594,
      "step": 833
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.02032083086669445,
      "learning_rate": 0.016529338327091136,
      "loss": 0.0811,
      "step": 834
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.011677929200232029,
      "learning_rate": 0.016525176862255513,
      "loss": 0.0311,
      "step": 835
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.0012912238016724586,
      "learning_rate": 0.016521015397419892,
      "loss": 0.0021,
      "step": 836
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.009877496398985386,
      "learning_rate": 0.01651685393258427,
      "loss": 0.0571,
      "step": 837
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.022816717624664307,
      "learning_rate": 0.016512692467748648,
      "loss": 0.2988,
      "step": 838
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.020957378670573235,
      "learning_rate": 0.016508531002913024,
      "loss": 0.3513,
      "step": 839
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.01921050064265728,
      "learning_rate": 0.016504369538077403,
      "loss": 0.3225,
      "step": 840
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.015538414008915424,
      "learning_rate": 0.016500208073241783,
      "loss": 0.2856,
      "step": 841
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.005771830677986145,
      "learning_rate": 0.01649604660840616,
      "loss": 0.0359,
      "step": 842
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.019735774025321007,
      "learning_rate": 0.01649188514357054,
      "loss": 0.2627,
      "step": 843
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.01537058874964714,
      "learning_rate": 0.016487723678734915,
      "loss": 0.1748,
      "step": 844
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.017002243548631668,
      "learning_rate": 0.01648356221389929,
      "loss": 0.208,
      "step": 845
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.011843948625028133,
      "learning_rate": 0.01647940074906367,
      "loss": 0.0623,
      "step": 846
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.013152760453522205,
      "learning_rate": 0.01647523928422805,
      "loss": 0.0202,
      "step": 847
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.01586482860147953,
      "learning_rate": 0.016471077819392426,
      "loss": 0.2474,
      "step": 848
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.021015800535678864,
      "learning_rate": 0.016466916354556806,
      "loss": 0.3752,
      "step": 849
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.01488250121474266,
      "learning_rate": 0.01646275488972118,
      "loss": 0.2264,
      "step": 850
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.01454408373683691,
      "learning_rate": 0.016458593424885558,
      "loss": 0.1473,
      "step": 851
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.021884668618440628,
      "learning_rate": 0.016454431960049937,
      "loss": 0.1755,
      "step": 852
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.024333517998456955,
      "learning_rate": 0.016450270495214317,
      "loss": 0.2444,
      "step": 853
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.025943482294678688,
      "learning_rate": 0.016446109030378693,
      "loss": 0.2568,
      "step": 854
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.02077079750597477,
      "learning_rate": 0.016441947565543073,
      "loss": 0.2686,
      "step": 855
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.009182083420455456,
      "learning_rate": 0.01643778610070745,
      "loss": 0.0546,
      "step": 856
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.027752196416258812,
      "learning_rate": 0.016433624635871828,
      "loss": 0.5513,
      "step": 857
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.028133325278759003,
      "learning_rate": 0.016429463171036204,
      "loss": 0.3528,
      "step": 858
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.01251843012869358,
      "learning_rate": 0.016425301706200584,
      "loss": 0.0686,
      "step": 859
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.020108910277485847,
      "learning_rate": 0.01642114024136496,
      "loss": 0.1724,
      "step": 860
    },
    {
      "epoch": 1.07,
      "grad_norm": 0.02237214334309101,
      "learning_rate": 0.01641697877652934,
      "loss": 0.3689,
      "step": 861
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.020316746085882187,
      "learning_rate": 0.016412817311693716,
      "loss": 0.4507,
      "step": 862
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.0023943581618368626,
      "learning_rate": 0.016408655846858095,
      "loss": 0.0014,
      "step": 863
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.015778040513396263,
      "learning_rate": 0.01640449438202247,
      "loss": 0.1205,
      "step": 864
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.025118518620729446,
      "learning_rate": 0.01640033291718685,
      "loss": 0.4604,
      "step": 865
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.03616708144545555,
      "learning_rate": 0.016396171452351227,
      "loss": 0.2974,
      "step": 866
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.019496910274028778,
      "learning_rate": 0.016392009987515607,
      "loss": 0.3013,
      "step": 867
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.032156262546777725,
      "learning_rate": 0.016387848522679986,
      "loss": 0.4255,
      "step": 868
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.020332850515842438,
      "learning_rate": 0.016383687057844362,
      "loss": 0.1069,
      "step": 869
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.017447790130972862,
      "learning_rate": 0.01637952559300874,
      "loss": 0.3247,
      "step": 870
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.022485749796032906,
      "learning_rate": 0.016375364128173118,
      "loss": 0.335,
      "step": 871
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.016633182764053345,
      "learning_rate": 0.016371202663337494,
      "loss": 0.4094,
      "step": 872
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.022176867350935936,
      "learning_rate": 0.016367041198501874,
      "loss": 0.4968,
      "step": 873
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.023500703275203705,
      "learning_rate": 0.016362879733666253,
      "loss": 0.166,
      "step": 874
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.018345315009355545,
      "learning_rate": 0.01635871826883063,
      "loss": 0.134,
      "step": 875
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.020953403785824776,
      "learning_rate": 0.016354556803995005,
      "loss": 0.2262,
      "step": 876
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.019372910261154175,
      "learning_rate": 0.016350395339159385,
      "loss": 0.3599,
      "step": 877
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.01696900837123394,
      "learning_rate": 0.01634623387432376,
      "loss": 0.0999,
      "step": 878
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.02973988838493824,
      "learning_rate": 0.01634207240948814,
      "loss": 0.0488,
      "step": 879
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.01587802916765213,
      "learning_rate": 0.01633791094465252,
      "loss": 0.1545,
      "step": 880
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.013657576404511929,
      "learning_rate": 0.016333749479816896,
      "loss": 0.0683,
      "step": 881
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.01823948137462139,
      "learning_rate": 0.016329588014981272,
      "loss": 0.1791,
      "step": 882
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.020346296951174736,
      "learning_rate": 0.016325426550145652,
      "loss": 0.4141,
      "step": 883
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.011030556634068489,
      "learning_rate": 0.016321265085310028,
      "loss": 0.0443,
      "step": 884
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.019400158897042274,
      "learning_rate": 0.016317103620474407,
      "loss": 0.3291,
      "step": 885
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.019549163058400154,
      "learning_rate": 0.016312942155638787,
      "loss": 0.1863,
      "step": 886
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.018476836383342743,
      "learning_rate": 0.016308780690803163,
      "loss": 0.4438,
      "step": 887
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.025145960971713066,
      "learning_rate": 0.01630461922596754,
      "loss": 0.3721,
      "step": 888
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.01449675764888525,
      "learning_rate": 0.01630045776113192,
      "loss": 0.0373,
      "step": 889
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.025906091555953026,
      "learning_rate": 0.016296296296296295,
      "loss": 0.3435,
      "step": 890
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.02867051772773266,
      "learning_rate": 0.016292134831460674,
      "loss": 0.4929,
      "step": 891
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.004738725256174803,
      "learning_rate": 0.016287973366625054,
      "loss": 0.0032,
      "step": 892
    },
    {
      "epoch": 1.11,
      "grad_norm": 0.017011573538184166,
      "learning_rate": 0.01628381190178943,
      "loss": 0.0698,
      "step": 893
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.02346462942659855,
      "learning_rate": 0.016279650436953806,
      "loss": 0.3755,
      "step": 894
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.01730034500360489,
      "learning_rate": 0.016275488972118186,
      "loss": 0.191,
      "step": 895
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.011661549098789692,
      "learning_rate": 0.016271327507282562,
      "loss": 0.0641,
      "step": 896
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.02210068143904209,
      "learning_rate": 0.01626716604244694,
      "loss": 0.2524,
      "step": 897
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.01413202378898859,
      "learning_rate": 0.01626300457761132,
      "loss": 0.1211,
      "step": 898
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.0009288507280871272,
      "learning_rate": 0.016258843112775697,
      "loss": 0.0004,
      "step": 899
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.7877373695373535,
      "learning_rate": 0.016254681647940077,
      "loss": 0.2069,
      "step": 900
    },
    {
      "epoch": 1.12,
      "eval_loss": 0.276611328125,
      "eval_runtime": 183.0948,
      "eval_samples_per_second": 1.098,
      "eval_steps_per_second": 0.552,
      "step": 900
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.026519136503338814,
      "learning_rate": 0.016250520183104453,
      "loss": 0.0829,
      "step": 901
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.03849050775170326,
      "learning_rate": 0.01624635871826883,
      "loss": 0.7671,
      "step": 902
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.018702959641814232,
      "learning_rate": 0.01624219725343321,
      "loss": 0.345,
      "step": 903
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.018841372802853584,
      "learning_rate": 0.016238035788597588,
      "loss": 0.2462,
      "step": 904
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.016912473365664482,
      "learning_rate": 0.016233874323761964,
      "loss": 0.1604,
      "step": 905
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.0141952745616436,
      "learning_rate": 0.016229712858926344,
      "loss": 0.0792,
      "step": 906
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.01381248701363802,
      "learning_rate": 0.01622555139409072,
      "loss": 0.101,
      "step": 907
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.021296629682183266,
      "learning_rate": 0.016221389929255096,
      "loss": 0.1182,
      "step": 908
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.02041725255548954,
      "learning_rate": 0.016217228464419475,
      "loss": 0.2544,
      "step": 909
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.014363840222358704,
      "learning_rate": 0.016213066999583855,
      "loss": 0.0547,
      "step": 910
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.028791187331080437,
      "learning_rate": 0.016208905534748234,
      "loss": 0.3252,
      "step": 911
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.020284051075577736,
      "learning_rate": 0.01620474406991261,
      "loss": 0.0414,
      "step": 912
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.01956641674041748,
      "learning_rate": 0.016200582605076987,
      "loss": 0.144,
      "step": 913
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.020686237141489983,
      "learning_rate": 0.016196421140241366,
      "loss": 0.2286,
      "step": 914
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.01920429617166519,
      "learning_rate": 0.016192259675405742,
      "loss": 0.1573,
      "step": 915
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.021561048924922943,
      "learning_rate": 0.016188098210570122,
      "loss": 0.5024,
      "step": 916
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.011054680682718754,
      "learning_rate": 0.0161839367457345,
      "loss": 0.0472,
      "step": 917
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.021469352766871452,
      "learning_rate": 0.016179775280898877,
      "loss": 0.1558,
      "step": 918
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.028537174686789513,
      "learning_rate": 0.016175613816063254,
      "loss": 0.3579,
      "step": 919
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.027993837371468544,
      "learning_rate": 0.016171452351227633,
      "loss": 0.5728,
      "step": 920
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.020287811756134033,
      "learning_rate": 0.01616729088639201,
      "loss": 0.3977,
      "step": 921
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.028396226465702057,
      "learning_rate": 0.01616312942155639,
      "loss": 0.2693,
      "step": 922
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.019021200016140938,
      "learning_rate": 0.01615896795672077,
      "loss": 0.2737,
      "step": 923
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.022501075640320778,
      "learning_rate": 0.016154806491885144,
      "loss": 0.2081,
      "step": 924
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.029850833117961884,
      "learning_rate": 0.01615064502704952,
      "loss": 0.4607,
      "step": 925
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.005168376490473747,
      "learning_rate": 0.0161464835622139,
      "loss": 0.0029,
      "step": 926
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.017600636929273605,
      "learning_rate": 0.016142322097378276,
      "loss": 0.2366,
      "step": 927
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.0022962039802223444,
      "learning_rate": 0.016138160632542656,
      "loss": 0.0027,
      "step": 928
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.025218483060598373,
      "learning_rate": 0.016133999167707035,
      "loss": 0.2805,
      "step": 929
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.0006191764841787517,
      "learning_rate": 0.01612983770287141,
      "loss": 0.0008,
      "step": 930
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.02196386642754078,
      "learning_rate": 0.016125676238035788,
      "loss": 0.1945,
      "step": 931
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.017645791172981262,
      "learning_rate": 0.016121514773200167,
      "loss": 0.3062,
      "step": 932
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.023195844143629074,
      "learning_rate": 0.016117353308364543,
      "loss": 0.2666,
      "step": 933
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.022655079141259193,
      "learning_rate": 0.016113191843528923,
      "loss": 0.1132,
      "step": 934
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.02040504291653633,
      "learning_rate": 0.016109030378693302,
      "loss": 0.3386,
      "step": 935
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.015803825110197067,
      "learning_rate": 0.01610486891385768,
      "loss": 0.3064,
      "step": 936
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.011600911617279053,
      "learning_rate": 0.016100707449022054,
      "loss": 0.0737,
      "step": 937
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.016042442992329597,
      "learning_rate": 0.016096545984186434,
      "loss": 0.1802,
      "step": 938
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.015973592177033424,
      "learning_rate": 0.01609238451935081,
      "loss": 0.2563,
      "step": 939
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.02279144898056984,
      "learning_rate": 0.01608822305451519,
      "loss": 0.1506,
      "step": 940
    },
    {
      "epoch": 1.17,
      "grad_norm": 0.025131290778517723,
      "learning_rate": 0.01608406158967957,
      "loss": 0.3315,
      "step": 941
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.03296956792473793,
      "learning_rate": 0.016079900124843945,
      "loss": 0.3972,
      "step": 942
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.02016415260732174,
      "learning_rate": 0.016075738660008325,
      "loss": 0.3557,
      "step": 943
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.025722770020365715,
      "learning_rate": 0.0160715771951727,
      "loss": 0.1357,
      "step": 944
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.01275906153023243,
      "learning_rate": 0.016067415730337077,
      "loss": 0.2053,
      "step": 945
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.007159693632274866,
      "learning_rate": 0.016063254265501457,
      "loss": 0.0319,
      "step": 946
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.012666679918766022,
      "learning_rate": 0.016059092800665836,
      "loss": 0.0644,
      "step": 947
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.023768091574311256,
      "learning_rate": 0.016054931335830212,
      "loss": 0.3425,
      "step": 948
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.018956877291202545,
      "learning_rate": 0.016050769870994592,
      "loss": 0.205,
      "step": 949
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.022229792550206184,
      "learning_rate": 0.016046608406158968,
      "loss": 0.2661,
      "step": 950
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.02749771997332573,
      "learning_rate": 0.016042446941323344,
      "loss": 0.3875,
      "step": 951
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.016075553372502327,
      "learning_rate": 0.016038285476487724,
      "loss": 0.2316,
      "step": 952
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.013823479413986206,
      "learning_rate": 0.016034124011652103,
      "loss": 0.2084,
      "step": 953
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.018673235550522804,
      "learning_rate": 0.01602996254681648,
      "loss": 0.2793,
      "step": 954
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.00583814550191164,
      "learning_rate": 0.01602580108198086,
      "loss": 0.0182,
      "step": 955
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.03060997650027275,
      "learning_rate": 0.016021639617145235,
      "loss": 0.4751,
      "step": 956
    },
    {
      "epoch": 1.19,
      "grad_norm": 0.02101307176053524,
      "learning_rate": 0.01601747815230961,
      "loss": 0.4214,
      "step": 957
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.011083472520112991,
      "learning_rate": 0.01601331668747399,
      "loss": 0.1252,
      "step": 958
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.027596939355134964,
      "learning_rate": 0.01600915522263837,
      "loss": 0.2883,
      "step": 959
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.022441310808062553,
      "learning_rate": 0.016004993757802746,
      "loss": 0.304,
      "step": 960
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.0004867761745117605,
      "learning_rate": 0.016000832292967126,
      "loss": 0.0007,
      "step": 961
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.021003400906920433,
      "learning_rate": 0.015996670828131502,
      "loss": 0.4141,
      "step": 962
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.01835128851234913,
      "learning_rate": 0.015992509363295878,
      "loss": 0.3289,
      "step": 963
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.015276538208127022,
      "learning_rate": 0.015988347898460258,
      "loss": 0.1052,
      "step": 964
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.009803126566112041,
      "learning_rate": 0.015984186433624637,
      "loss": 0.0461,
      "step": 965
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.02007918432354927,
      "learning_rate": 0.015980024968789013,
      "loss": 0.074,
      "step": 966
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.020967651158571243,
      "learning_rate": 0.015975863503953393,
      "loss": 0.4387,
      "step": 967
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.016827093437314034,
      "learning_rate": 0.01597170203911777,
      "loss": 0.0786,
      "step": 968
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.015784306451678276,
      "learning_rate": 0.015967540574282145,
      "loss": 0.244,
      "step": 969
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.015032459981739521,
      "learning_rate": 0.015963379109446525,
      "loss": 0.172,
      "step": 970
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.011052594520151615,
      "learning_rate": 0.015959217644610904,
      "loss": 0.0458,
      "step": 971
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.03352367877960205,
      "learning_rate": 0.01595505617977528,
      "loss": 0.5415,
      "step": 972
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.015644092112779617,
      "learning_rate": 0.01595089471493966,
      "loss": 0.0603,
      "step": 973
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.00045275763841345906,
      "learning_rate": 0.015946733250104036,
      "loss": 0.0006,
      "step": 974
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.01635799929499626,
      "learning_rate": 0.015942571785268415,
      "loss": 0.3152,
      "step": 975
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.021508242934942245,
      "learning_rate": 0.01593841032043279,
      "loss": 0.2922,
      "step": 976
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.021619176492094994,
      "learning_rate": 0.01593424885559717,
      "loss": 0.3474,
      "step": 977
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.019374271854758263,
      "learning_rate": 0.015930087390761547,
      "loss": 0.4309,
      "step": 978
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.021594762802124023,
      "learning_rate": 0.015925925925925927,
      "loss": 0.3157,
      "step": 979
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.02327478863298893,
      "learning_rate": 0.015921764461090303,
      "loss": 0.24,
      "step": 980
    },
    {
      "epoch": 1.22,
      "grad_norm": 0.030785227194428444,
      "learning_rate": 0.015917602996254682,
      "loss": 0.6362,
      "step": 981
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.019039804115891457,
      "learning_rate": 0.01591344153141906,
      "loss": 0.283,
      "step": 982
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.014343260787427425,
      "learning_rate": 0.015909280066583438,
      "loss": 0.1544,
      "step": 983
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.029103873297572136,
      "learning_rate": 0.015905118601747818,
      "loss": 0.4111,
      "step": 984
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.017416486516594887,
      "learning_rate": 0.015900957136912194,
      "loss": 0.1909,
      "step": 985
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.017740193754434586,
      "learning_rate": 0.015896795672076573,
      "loss": 0.2529,
      "step": 986
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.008261576294898987,
      "learning_rate": 0.01589263420724095,
      "loss": 0.0497,
      "step": 987
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.020972803235054016,
      "learning_rate": 0.015888472742405325,
      "loss": 0.271,
      "step": 988
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.013603289611637592,
      "learning_rate": 0.015884311277569705,
      "loss": 0.1375,
      "step": 989
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.013080689124763012,
      "learning_rate": 0.015880149812734085,
      "loss": 0.0198,
      "step": 990
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.014577233232557774,
      "learning_rate": 0.01587598834789846,
      "loss": 0.2607,
      "step": 991
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.016401128843426704,
      "learning_rate": 0.01587182688306284,
      "loss": 0.132,
      "step": 992
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.024706076830625534,
      "learning_rate": 0.015867665418227216,
      "loss": 0.3445,
      "step": 993
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.038260236382484436,
      "learning_rate": 0.015863503953391592,
      "loss": 0.3057,
      "step": 994
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.016283279284834862,
      "learning_rate": 0.015859342488555972,
      "loss": 0.353,
      "step": 995
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.025337805971503258,
      "learning_rate": 0.01585518102372035,
      "loss": 0.1176,
      "step": 996
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.019213566556572914,
      "learning_rate": 0.015851019558884728,
      "loss": 0.2363,
      "step": 997
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.025822242721915245,
      "learning_rate": 0.015846858094049107,
      "loss": 0.3206,
      "step": 998
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.012141147628426552,
      "learning_rate": 0.015842696629213483,
      "loss": 0.0931,
      "step": 999
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.020724572241306305,
      "learning_rate": 0.01583853516437786,
      "loss": 0.209,
      "step": 1000
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.02043132483959198,
      "learning_rate": 0.01583437369954224,
      "loss": 0.3499,
      "step": 1001
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.012707555666565895,
      "learning_rate": 0.01583021223470662,
      "loss": 0.2217,
      "step": 1002
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.023316146805882454,
      "learning_rate": 0.015826050769870995,
      "loss": 0.3777,
      "step": 1003
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.015011832118034363,
      "learning_rate": 0.015821889305035374,
      "loss": 0.0135,
      "step": 1004
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.0184890478849411,
      "learning_rate": 0.01581772784019975,
      "loss": 0.1482,
      "step": 1005
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.01736568473279476,
      "learning_rate": 0.015813566375364126,
      "loss": 0.3743,
      "step": 1006
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.027601389214396477,
      "learning_rate": 0.015809404910528506,
      "loss": 0.3777,
      "step": 1007
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.032839711755514145,
      "learning_rate": 0.015805243445692885,
      "loss": 0.4526,
      "step": 1008
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.02121092565357685,
      "learning_rate": 0.01580108198085726,
      "loss": 0.3813,
      "step": 1009
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.019736194983124733,
      "learning_rate": 0.01579692051602164,
      "loss": 0.2817,
      "step": 1010
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.013008872978389263,
      "learning_rate": 0.015792759051186017,
      "loss": 0.1169,
      "step": 1011
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.025218408554792404,
      "learning_rate": 0.015788597586350393,
      "loss": 0.5791,
      "step": 1012
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.00462852418422699,
      "learning_rate": 0.015784436121514773,
      "loss": 0.0042,
      "step": 1013
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.019213877618312836,
      "learning_rate": 0.015780274656679152,
      "loss": 0.2571,
      "step": 1014
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.019731277599930763,
      "learning_rate": 0.01577611319184353,
      "loss": 0.4185,
      "step": 1015
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.019622253254055977,
      "learning_rate": 0.015771951727007908,
      "loss": 0.1412,
      "step": 1016
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.02248537167906761,
      "learning_rate": 0.015767790262172284,
      "loss": 0.3145,
      "step": 1017
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.023821372538805008,
      "learning_rate": 0.015763628797336664,
      "loss": 0.1793,
      "step": 1018
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.017169175669550896,
      "learning_rate": 0.01575946733250104,
      "loss": 0.0958,
      "step": 1019
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.02006416767835617,
      "learning_rate": 0.01575530586766542,
      "loss": 0.4849,
      "step": 1020
    },
    {
      "epoch": 1.27,
      "grad_norm": 0.015900978818535805,
      "learning_rate": 0.015751144402829795,
      "loss": 0.1081,
      "step": 1021
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.019144194200634956,
      "learning_rate": 0.015746982937994175,
      "loss": 0.1746,
      "step": 1022
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.020386679098010063,
      "learning_rate": 0.01574282147315855,
      "loss": 0.3347,
      "step": 1023
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.0170699842274189,
      "learning_rate": 0.01573866000832293,
      "loss": 0.1436,
      "step": 1024
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.024188198149204254,
      "learning_rate": 0.015734498543487307,
      "loss": 0.415,
      "step": 1025
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.02380228601396084,
      "learning_rate": 0.015730337078651686,
      "loss": 0.7476,
      "step": 1026
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.02469363808631897,
      "learning_rate": 0.015726175613816062,
      "loss": 0.2203,
      "step": 1027
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.023834528401494026,
      "learning_rate": 0.015722014148980442,
      "loss": 0.1089,
      "step": 1028
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.015983764082193375,
      "learning_rate": 0.01571785268414482,
      "loss": 0.208,
      "step": 1029
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.01997104473412037,
      "learning_rate": 0.015713691219309198,
      "loss": 0.2747,
      "step": 1030
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.017408501356840134,
      "learning_rate": 0.015709529754473574,
      "loss": 0.1409,
      "step": 1031
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.023084178566932678,
      "learning_rate": 0.015705368289637953,
      "loss": 0.4807,
      "step": 1032
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.013663158752024174,
      "learning_rate": 0.01570120682480233,
      "loss": 0.0727,
      "step": 1033
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.023880543187260628,
      "learning_rate": 0.01569704535996671,
      "loss": 0.4978,
      "step": 1034
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.015810102224349976,
      "learning_rate": 0.01569288389513109,
      "loss": 0.1748,
      "step": 1035
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.016030754894018173,
      "learning_rate": 0.015688722430295465,
      "loss": 0.0733,
      "step": 1036
    },
    {
      "epoch": 1.29,
      "grad_norm": 0.02104434370994568,
      "learning_rate": 0.01568456096545984,
      "loss": 0.2396,
      "step": 1037
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.023493843153119087,
      "learning_rate": 0.01568039950062422,
      "loss": 0.3794,
      "step": 1038
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.020526286214590073,
      "learning_rate": 0.015676238035788596,
      "loss": 0.2996,
      "step": 1039
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.01850873976945877,
      "learning_rate": 0.015672076570952976,
      "loss": 0.2571,
      "step": 1040
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.015737837180495262,
      "learning_rate": 0.015667915106117355,
      "loss": 0.1031,
      "step": 1041
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.022054312750697136,
      "learning_rate": 0.01566375364128173,
      "loss": 0.0681,
      "step": 1042
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.044910941272974014,
      "learning_rate": 0.015659592176446108,
      "loss": 0.2883,
      "step": 1043
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.012114339508116245,
      "learning_rate": 0.015655430711610487,
      "loss": 0.0986,
      "step": 1044
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.019139524549245834,
      "learning_rate": 0.015651269246774863,
      "loss": 0.1636,
      "step": 1045
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.015495910309255123,
      "learning_rate": 0.015647107781939243,
      "loss": 0.1262,
      "step": 1046
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.023599015548825264,
      "learning_rate": 0.015642946317103622,
      "loss": 0.2786,
      "step": 1047
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.02580740861594677,
      "learning_rate": 0.015638784852268,
      "loss": 0.184,
      "step": 1048
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.028933294117450714,
      "learning_rate": 0.015634623387432375,
      "loss": 0.4736,
      "step": 1049
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.04469485208392143,
      "learning_rate": 0.015630461922596754,
      "loss": 0.251,
      "step": 1050
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.02135387994349003,
      "learning_rate": 0.01562630045776113,
      "loss": 0.252,
      "step": 1051
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.014715808443725109,
      "learning_rate": 0.01562213899292551,
      "loss": 0.4316,
      "step": 1052
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.017467500641942024,
      "learning_rate": 0.015617977528089888,
      "loss": 0.2715,
      "step": 1053
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.017933281138539314,
      "learning_rate": 0.015613816063254266,
      "loss": 0.0946,
      "step": 1054
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.019199233502149582,
      "learning_rate": 0.015609654598418643,
      "loss": 0.1135,
      "step": 1055
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.02765505202114582,
      "learning_rate": 0.015605493133583021,
      "loss": 0.4438,
      "step": 1056
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.02297116257250309,
      "learning_rate": 0.0156013316687474,
      "loss": 0.1573,
      "step": 1057
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.016932332888245583,
      "learning_rate": 0.015597170203911777,
      "loss": 0.2242,
      "step": 1058
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.03125903010368347,
      "learning_rate": 0.015593008739076155,
      "loss": 0.3635,
      "step": 1059
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.02059970609843731,
      "learning_rate": 0.015588847274240534,
      "loss": 0.3572,
      "step": 1060
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.022705474868416786,
      "learning_rate": 0.01558468580940491,
      "loss": 0.2847,
      "step": 1061
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.01579454354941845,
      "learning_rate": 0.01558052434456929,
      "loss": 0.1407,
      "step": 1062
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.0002887519949581474,
      "learning_rate": 0.015576362879733668,
      "loss": 0.0004,
      "step": 1063
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.019719930365681648,
      "learning_rate": 0.015572201414898044,
      "loss": 0.3406,
      "step": 1064
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.021523548290133476,
      "learning_rate": 0.015568039950062423,
      "loss": 0.2588,
      "step": 1065
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.02310948260128498,
      "learning_rate": 0.015563878485226801,
      "loss": 0.1113,
      "step": 1066
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.012330864556133747,
      "learning_rate": 0.015559717020391177,
      "loss": 0.1165,
      "step": 1067
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.010723087936639786,
      "learning_rate": 0.015555555555555557,
      "loss": 0.0454,
      "step": 1068
    },
    {
      "epoch": 1.33,
      "grad_norm": 0.020736204460263252,
      "learning_rate": 0.015551394090719935,
      "loss": 0.2084,
      "step": 1069
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.02228960394859314,
      "learning_rate": 0.01554723262588431,
      "loss": 0.4268,
      "step": 1070
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.019444720819592476,
      "learning_rate": 0.01554307116104869,
      "loss": 0.2075,
      "step": 1071
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.012051189318299294,
      "learning_rate": 0.015538909696213068,
      "loss": 0.0643,
      "step": 1072
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.008103182539343834,
      "learning_rate": 0.015534748231377444,
      "loss": 0.0317,
      "step": 1073
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.01383668277412653,
      "learning_rate": 0.015530586766541824,
      "loss": 0.1681,
      "step": 1074
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.02172127552330494,
      "learning_rate": 0.015526425301706202,
      "loss": 0.3667,
      "step": 1075
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.024141397327184677,
      "learning_rate": 0.015522263836870578,
      "loss": 0.3577,
      "step": 1076
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.0225079245865345,
      "learning_rate": 0.015518102372034957,
      "loss": 0.4368,
      "step": 1077
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.018179163336753845,
      "learning_rate": 0.015513940907199335,
      "loss": 0.1593,
      "step": 1078
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.018162671476602554,
      "learning_rate": 0.015509779442363711,
      "loss": 0.4954,
      "step": 1079
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.019146595150232315,
      "learning_rate": 0.01550561797752809,
      "loss": 0.2988,
      "step": 1080
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.014852888882160187,
      "learning_rate": 0.015501456512692469,
      "loss": 0.0483,
      "step": 1081
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.019325925037264824,
      "learning_rate": 0.015497295047856845,
      "loss": 0.2666,
      "step": 1082
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.014204607345163822,
      "learning_rate": 0.015493133583021224,
      "loss": 0.1718,
      "step": 1083
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.01749173365533352,
      "learning_rate": 0.015488972118185602,
      "loss": 0.2151,
      "step": 1084
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.021085506305098534,
      "learning_rate": 0.015484810653349978,
      "loss": 0.1967,
      "step": 1085
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.017188740894198418,
      "learning_rate": 0.015480649188514358,
      "loss": 0.1759,
      "step": 1086
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.02077721245586872,
      "learning_rate": 0.015476487723678736,
      "loss": 0.141,
      "step": 1087
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.028460413217544556,
      "learning_rate": 0.015472326258843112,
      "loss": 0.1639,
      "step": 1088
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.016213104128837585,
      "learning_rate": 0.015468164794007491,
      "loss": 0.2114,
      "step": 1089
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.024326445534825325,
      "learning_rate": 0.015464003329171869,
      "loss": 0.2467,
      "step": 1090
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.026028770953416824,
      "learning_rate": 0.015459841864336245,
      "loss": 0.6675,
      "step": 1091
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.016207613050937653,
      "learning_rate": 0.015455680399500625,
      "loss": 0.2339,
      "step": 1092
    },
    {
      "epoch": 1.36,
      "grad_norm": 0.019064977765083313,
      "learning_rate": 0.015451518934665003,
      "loss": 0.23,
      "step": 1093
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.015190163627266884,
      "learning_rate": 0.01544735746982938,
      "loss": 0.1476,
      "step": 1094
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.01640726625919342,
      "learning_rate": 0.015443196004993758,
      "loss": 0.2102,
      "step": 1095
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.02259083464741707,
      "learning_rate": 0.015439034540158136,
      "loss": 0.2588,
      "step": 1096
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.020244941115379333,
      "learning_rate": 0.015434873075322514,
      "loss": 0.2026,
      "step": 1097
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.0171633493155241,
      "learning_rate": 0.015430711610486892,
      "loss": 0.1781,
      "step": 1098
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.002251014579087496,
      "learning_rate": 0.01542655014565127,
      "loss": 0.0016,
      "step": 1099
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.023644737899303436,
      "learning_rate": 0.015422388680815647,
      "loss": 0.3811,
      "step": 1100
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.01999722421169281,
      "learning_rate": 0.015418227215980025,
      "loss": 0.3342,
      "step": 1101
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.01838837005198002,
      "learning_rate": 0.015414065751144403,
      "loss": 0.3562,
      "step": 1102
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.01500898040831089,
      "learning_rate": 0.01540990428630878,
      "loss": 0.3335,
      "step": 1103
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.01809258759021759,
      "learning_rate": 0.015405742821473159,
      "loss": 0.1715,
      "step": 1104
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.02568463236093521,
      "learning_rate": 0.015401581356637538,
      "loss": 0.3784,
      "step": 1105
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.009081849828362465,
      "learning_rate": 0.015397419891801914,
      "loss": 0.0245,
      "step": 1106
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.023000378161668777,
      "learning_rate": 0.015393258426966292,
      "loss": 0.2214,
      "step": 1107
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.02157086692750454,
      "learning_rate": 0.015389096962130672,
      "loss": 0.2255,
      "step": 1108
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.018225355073809624,
      "learning_rate": 0.015384935497295048,
      "loss": 0.2051,
      "step": 1109
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.034307777881622314,
      "learning_rate": 0.015380774032459426,
      "loss": 0.4041,
      "step": 1110
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.01195373386144638,
      "learning_rate": 0.015376612567623805,
      "loss": 0.0962,
      "step": 1111
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.01943417638540268,
      "learning_rate": 0.015372451102788181,
      "loss": 0.1947,
      "step": 1112
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.016698267310857773,
      "learning_rate": 0.015368289637952559,
      "loss": 0.1603,
      "step": 1113
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.023589296266436577,
      "learning_rate": 0.015364128173116939,
      "loss": 0.5039,
      "step": 1114
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.010582692921161652,
      "learning_rate": 0.015359966708281315,
      "loss": 0.0792,
      "step": 1115
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.01984834484755993,
      "learning_rate": 0.015355805243445693,
      "loss": 0.1484,
      "step": 1116
    },
    {
      "epoch": 1.39,
      "grad_norm": 0.009592410176992416,
      "learning_rate": 0.015351643778610072,
      "loss": 0.0854,
      "step": 1117
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.018000148236751556,
      "learning_rate": 0.015347482313774448,
      "loss": 0.1635,
      "step": 1118
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.023731786757707596,
      "learning_rate": 0.015343320848938826,
      "loss": 0.2798,
      "step": 1119
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.0160891804844141,
      "learning_rate": 0.015339159384103206,
      "loss": 0.3643,
      "step": 1120
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.02015240676701069,
      "learning_rate": 0.015334997919267582,
      "loss": 0.239,
      "step": 1121
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.014112010598182678,
      "learning_rate": 0.01533083645443196,
      "loss": 0.149,
      "step": 1122
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.026829784736037254,
      "learning_rate": 0.015326674989596339,
      "loss": 0.3792,
      "step": 1123
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.015016230754554272,
      "learning_rate": 0.015322513524760715,
      "loss": 0.2939,
      "step": 1124
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.0005421704263426363,
      "learning_rate": 0.015318352059925093,
      "loss": 0.0006,
      "step": 1125
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.025483932346105576,
      "learning_rate": 0.015314190595089473,
      "loss": 0.5596,
      "step": 1126
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.015506415627896786,
      "learning_rate": 0.015310029130253849,
      "loss": 0.2438,
      "step": 1127
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.014859020709991455,
      "learning_rate": 0.015305867665418227,
      "loss": 0.3511,
      "step": 1128
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.017689988017082214,
      "learning_rate": 0.015301706200582606,
      "loss": 0.1792,
      "step": 1129
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.00029732234543189406,
      "learning_rate": 0.015297544735746984,
      "loss": 0.0004,
      "step": 1130
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.02316972427070141,
      "learning_rate": 0.01529338327091136,
      "loss": 0.5171,
      "step": 1131
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.021559979766607285,
      "learning_rate": 0.01528922180607574,
      "loss": 0.3516,
      "step": 1132
    },
    {
      "epoch": 1.41,
      "grad_norm": 0.029548246413469315,
      "learning_rate": 0.015285060341240117,
      "loss": 0.585,
      "step": 1133
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.021956736221909523,
      "learning_rate": 0.015280898876404493,
      "loss": 0.1686,
      "step": 1134
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.018229397013783455,
      "learning_rate": 0.015276737411568873,
      "loss": 0.2147,
      "step": 1135
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.01827104762196541,
      "learning_rate": 0.01527257594673325,
      "loss": 0.1564,
      "step": 1136
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.012026326730847359,
      "learning_rate": 0.015268414481897629,
      "loss": 0.0656,
      "step": 1137
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.02198140136897564,
      "learning_rate": 0.015264253017062007,
      "loss": 0.3943,
      "step": 1138
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.02491358108818531,
      "learning_rate": 0.015260091552226384,
      "loss": 0.1785,
      "step": 1139
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.054076824337244034,
      "learning_rate": 0.015255930087390762,
      "loss": 0.4741,
      "step": 1140
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.02986018918454647,
      "learning_rate": 0.01525176862255514,
      "loss": 0.191,
      "step": 1141
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.017277082428336143,
      "learning_rate": 0.015247607157719518,
      "loss": 0.1667,
      "step": 1142
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.030948398634791374,
      "learning_rate": 0.015243445692883896,
      "loss": 0.2549,
      "step": 1143
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.02342919073998928,
      "learning_rate": 0.015239284228048273,
      "loss": 0.0709,
      "step": 1144
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.023284493014216423,
      "learning_rate": 0.015235122763212651,
      "loss": 0.2993,
      "step": 1145
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.018761197105050087,
      "learning_rate": 0.01523096129837703,
      "loss": 0.1833,
      "step": 1146
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.01601211167871952,
      "learning_rate": 0.015226799833541407,
      "loss": 0.1031,
      "step": 1147
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.023344509303569794,
      "learning_rate": 0.015222638368705787,
      "loss": 0.4573,
      "step": 1148
    },
    {
      "epoch": 1.43,
      "grad_norm": 0.016253603622317314,
      "learning_rate": 0.015218476903870163,
      "loss": 0.1138,
      "step": 1149
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.015798864886164665,
      "learning_rate": 0.01521431543903454,
      "loss": 0.0936,
      "step": 1150
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.006655659060925245,
      "learning_rate": 0.01521015397419892,
      "loss": 0.0236,
      "step": 1151
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.015202687121927738,
      "learning_rate": 0.015205992509363296,
      "loss": 0.2206,
      "step": 1152
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.013044016435742378,
      "learning_rate": 0.015201831044527674,
      "loss": 0.11,
      "step": 1153
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.01914636231958866,
      "learning_rate": 0.015197669579692053,
      "loss": 0.2402,
      "step": 1154
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.014704947359859943,
      "learning_rate": 0.01519350811485643,
      "loss": 0.0462,
      "step": 1155
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.01785224862396717,
      "learning_rate": 0.015189346650020807,
      "loss": 0.2172,
      "step": 1156
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.033810075372457504,
      "learning_rate": 0.015185185185185187,
      "loss": 0.6226,
      "step": 1157
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.014413599856197834,
      "learning_rate": 0.015181023720349563,
      "loss": 0.1069,
      "step": 1158
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.02114710584282875,
      "learning_rate": 0.015176862255513941,
      "loss": 0.269,
      "step": 1159
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.025646749883890152,
      "learning_rate": 0.01517270079067832,
      "loss": 0.0264,
      "step": 1160
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.025219501927495003,
      "learning_rate": 0.015168539325842697,
      "loss": 0.2483,
      "step": 1161
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.019140703603625298,
      "learning_rate": 0.015164377861007074,
      "loss": 0.1626,
      "step": 1162
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.010834680870175362,
      "learning_rate": 0.015160216396171454,
      "loss": 0.0604,
      "step": 1163
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.0206084493547678,
      "learning_rate": 0.01515605493133583,
      "loss": 0.1979,
      "step": 1164
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.015438029542565346,
      "learning_rate": 0.015151893466500208,
      "loss": 0.0132,
      "step": 1165
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.014139370061457157,
      "learning_rate": 0.015147732001664587,
      "loss": 0.0623,
      "step": 1166
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.008177504874765873,
      "learning_rate": 0.015143570536828964,
      "loss": 0.0326,
      "step": 1167
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.018727127462625504,
      "learning_rate": 0.015139409071993341,
      "loss": 0.1654,
      "step": 1168
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.008063145913183689,
      "learning_rate": 0.015135247607157721,
      "loss": 0.036,
      "step": 1169
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.010104950517416,
      "learning_rate": 0.015131086142322097,
      "loss": 0.0698,
      "step": 1170
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.03561998903751373,
      "learning_rate": 0.015126924677486475,
      "loss": 0.3223,
      "step": 1171
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.018008321523666382,
      "learning_rate": 0.015122763212650854,
      "loss": 0.1576,
      "step": 1172
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.027542300522327423,
      "learning_rate": 0.01511860174781523,
      "loss": 0.3191,
      "step": 1173
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.01982768438756466,
      "learning_rate": 0.015114440282979608,
      "loss": 0.2742,
      "step": 1174
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.025857718661427498,
      "learning_rate": 0.015110278818143988,
      "loss": 0.3403,
      "step": 1175
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.016640814021229744,
      "learning_rate": 0.015106117353308364,
      "loss": 0.0975,
      "step": 1176
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.024543974548578262,
      "learning_rate": 0.015101955888472742,
      "loss": 0.1842,
      "step": 1177
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.01926487311720848,
      "learning_rate": 0.015097794423637121,
      "loss": 0.2856,
      "step": 1178
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.03251056373119354,
      "learning_rate": 0.015093632958801497,
      "loss": 0.8428,
      "step": 1179
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.07430959492921829,
      "learning_rate": 0.015089471493965877,
      "loss": 0.2742,
      "step": 1180
    },
    {
      "epoch": 1.47,
      "grad_norm": 0.01837875507771969,
      "learning_rate": 0.015085310029130255,
      "loss": 0.1376,
      "step": 1181
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.018673159182071686,
      "learning_rate": 0.015081148564294631,
      "loss": 0.3821,
      "step": 1182
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.014461681246757507,
      "learning_rate": 0.01507698709945901,
      "loss": 0.1954,
      "step": 1183
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.023188557475805283,
      "learning_rate": 0.015072825634623388,
      "loss": 0.5127,
      "step": 1184
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.013737804256379604,
      "learning_rate": 0.015068664169787764,
      "loss": 0.0552,
      "step": 1185
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.007570568937808275,
      "learning_rate": 0.015064502704952144,
      "loss": 0.028,
      "step": 1186
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.01930762454867363,
      "learning_rate": 0.015060341240116522,
      "loss": 0.1311,
      "step": 1187
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.01913153938949108,
      "learning_rate": 0.015056179775280898,
      "loss": 0.1185,
      "step": 1188
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.035232484340667725,
      "learning_rate": 0.015052018310445277,
      "loss": 0.3857,
      "step": 1189
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.020691903308033943,
      "learning_rate": 0.015047856845609655,
      "loss": 0.3433,
      "step": 1190
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.029478132724761963,
      "learning_rate": 0.015043695380774031,
      "loss": 0.3596,
      "step": 1191
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.009989960119128227,
      "learning_rate": 0.015039533915938411,
      "loss": 0.0442,
      "step": 1192
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.021565720438957214,
      "learning_rate": 0.015035372451102789,
      "loss": 0.1177,
      "step": 1193
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.02209567278623581,
      "learning_rate": 0.015031210986267165,
      "loss": 0.0751,
      "step": 1194
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.029796341434121132,
      "learning_rate": 0.015027049521431544,
      "loss": 0.4351,
      "step": 1195
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.022179480642080307,
      "learning_rate": 0.015022888056595922,
      "loss": 0.3831,
      "step": 1196
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.012408039532601833,
      "learning_rate": 0.015018726591760298,
      "loss": 0.081,
      "step": 1197
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.041155047714710236,
      "learning_rate": 0.015014565126924678,
      "loss": 0.2688,
      "step": 1198
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.011901192367076874,
      "learning_rate": 0.015010403662089056,
      "loss": 0.0905,
      "step": 1199
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.03587112948298454,
      "learning_rate": 0.015006242197253432,
      "loss": 0.3569,
      "step": 1200
    },
    {
      "epoch": 1.5,
      "eval_loss": 0.262939453125,
      "eval_runtime": 183.0744,
      "eval_samples_per_second": 1.098,
      "eval_steps_per_second": 0.552,
      "step": 1200
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.02689652144908905,
      "learning_rate": 0.015002080732417811,
      "loss": 0.4983,
      "step": 1201
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.004521407186985016,
      "learning_rate": 0.01499791926758219,
      "loss": 0.0021,
      "step": 1202
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.008102629333734512,
      "learning_rate": 0.014993757802746569,
      "loss": 0.02,
      "step": 1203
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.02285955473780632,
      "learning_rate": 0.014989596337910945,
      "loss": 0.3142,
      "step": 1204
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.006106268614530563,
      "learning_rate": 0.014985434873075323,
      "loss": 0.0276,
      "step": 1205
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.015486959367990494,
      "learning_rate": 0.014981273408239702,
      "loss": 0.1589,
      "step": 1206
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.02652638964354992,
      "learning_rate": 0.014977111943404078,
      "loss": 0.3564,
      "step": 1207
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.0476614311337471,
      "learning_rate": 0.014972950478568456,
      "loss": 0.3833,
      "step": 1208
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.014729066751897335,
      "learning_rate": 0.014968789013732836,
      "loss": 0.0376,
      "step": 1209
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.015644939616322517,
      "learning_rate": 0.014964627548897212,
      "loss": 0.1006,
      "step": 1210
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.01606246456503868,
      "learning_rate": 0.01496046608406159,
      "loss": 0.2074,
      "step": 1211
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.014171753078699112,
      "learning_rate": 0.01495630461922597,
      "loss": 0.0427,
      "step": 1212
    },
    {
      "epoch": 1.51,
      "grad_norm": 0.025123046711087227,
      "learning_rate": 0.014952143154390345,
      "loss": 0.334,
      "step": 1213
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.02304987423121929,
      "learning_rate": 0.014947981689554723,
      "loss": 0.3376,
      "step": 1214
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.014634921215474606,
      "learning_rate": 0.014943820224719103,
      "loss": 0.1454,
      "step": 1215
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.005565658211708069,
      "learning_rate": 0.014939658759883479,
      "loss": 0.0037,
      "step": 1216
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.014487390406429768,
      "learning_rate": 0.014935497295047857,
      "loss": 0.0923,
      "step": 1217
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.011326639913022518,
      "learning_rate": 0.014931335830212236,
      "loss": 0.134,
      "step": 1218
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.06571507453918457,
      "learning_rate": 0.014927174365376612,
      "loss": 0.3564,
      "step": 1219
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.02080959640443325,
      "learning_rate": 0.01492301290054099,
      "loss": 0.2401,
      "step": 1220
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.01263508666306734,
      "learning_rate": 0.01491885143570537,
      "loss": 0.0681,
      "step": 1221
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.018080338835716248,
      "learning_rate": 0.014914689970869746,
      "loss": 0.1833,
      "step": 1222
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.03379977121949196,
      "learning_rate": 0.014910528506034124,
      "loss": 0.1562,
      "step": 1223
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.009423679672181606,
      "learning_rate": 0.014906367041198503,
      "loss": 0.0558,
      "step": 1224
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.020535599440336227,
      "learning_rate": 0.01490220557636288,
      "loss": 0.3567,
      "step": 1225
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.028150491416454315,
      "learning_rate": 0.014898044111527259,
      "loss": 0.3809,
      "step": 1226
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.025025155395269394,
      "learning_rate": 0.014893882646691637,
      "loss": 0.2991,
      "step": 1227
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.014737282879650593,
      "learning_rate": 0.014889721181856013,
      "loss": 0.2094,
      "step": 1228
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.009373722597956657,
      "learning_rate": 0.014885559717020392,
      "loss": 0.0489,
      "step": 1229
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.010164734907448292,
      "learning_rate": 0.01488139825218477,
      "loss": 0.015,
      "step": 1230
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.02192559652030468,
      "learning_rate": 0.014877236787349146,
      "loss": 0.3835,
      "step": 1231
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.017824064940214157,
      "learning_rate": 0.014873075322513526,
      "loss": 0.1919,
      "step": 1232
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.016246765851974487,
      "learning_rate": 0.014868913857677904,
      "loss": 0.1741,
      "step": 1233
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.018382525071501732,
      "learning_rate": 0.01486475239284228,
      "loss": 0.3159,
      "step": 1234
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.029316797852516174,
      "learning_rate": 0.01486059092800666,
      "loss": 0.5205,
      "step": 1235
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.01960177719593048,
      "learning_rate": 0.014856429463171037,
      "loss": 0.2083,
      "step": 1236
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.022220298647880554,
      "learning_rate": 0.014852267998335413,
      "loss": 0.3396,
      "step": 1237
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.01176958717405796,
      "learning_rate": 0.014848106533499793,
      "loss": 0.1098,
      "step": 1238
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.01710779219865799,
      "learning_rate": 0.01484394506866417,
      "loss": 0.2,
      "step": 1239
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.027880975976586342,
      "learning_rate": 0.014839783603828547,
      "loss": 0.8281,
      "step": 1240
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.018767990171909332,
      "learning_rate": 0.014835622138992926,
      "loss": 0.2834,
      "step": 1241
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.012422457337379456,
      "learning_rate": 0.014831460674157304,
      "loss": 0.3337,
      "step": 1242
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.0187063068151474,
      "learning_rate": 0.01482729920932168,
      "loss": 0.2279,
      "step": 1243
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.018549783155322075,
      "learning_rate": 0.01482313774448606,
      "loss": 0.28,
      "step": 1244
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.01833917200565338,
      "learning_rate": 0.014818976279650438,
      "loss": 0.21,
      "step": 1245
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.027794407680630684,
      "learning_rate": 0.014814814814814814,
      "loss": 0.3555,
      "step": 1246
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.010096295736730099,
      "learning_rate": 0.014810653349979193,
      "loss": 0.1011,
      "step": 1247
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.014131364412605762,
      "learning_rate": 0.014806491885143571,
      "loss": 0.0641,
      "step": 1248
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.025988265872001648,
      "learning_rate": 0.014802330420307947,
      "loss": 0.2234,
      "step": 1249
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.01913675107061863,
      "learning_rate": 0.014798168955472327,
      "loss": 0.1346,
      "step": 1250
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.014575025998055935,
      "learning_rate": 0.014794007490636705,
      "loss": 0.0981,
      "step": 1251
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.01834517903625965,
      "learning_rate": 0.01478984602580108,
      "loss": 0.2886,
      "step": 1252
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.019583135843276978,
      "learning_rate": 0.01478568456096546,
      "loss": 0.2949,
      "step": 1253
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.02295992150902748,
      "learning_rate": 0.014781523096129838,
      "loss": 0.2228,
      "step": 1254
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.02698848955333233,
      "learning_rate": 0.014777361631294214,
      "loss": 0.5312,
      "step": 1255
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.01704975962638855,
      "learning_rate": 0.014773200166458594,
      "loss": 0.1948,
      "step": 1256
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.011658884584903717,
      "learning_rate": 0.014769038701622971,
      "loss": 0.0909,
      "step": 1257
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.025210224092006683,
      "learning_rate": 0.01476487723678735,
      "loss": 0.4639,
      "step": 1258
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.016480304300785065,
      "learning_rate": 0.014760715771951727,
      "loss": 0.2162,
      "step": 1259
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.017549768090248108,
      "learning_rate": 0.014756554307116105,
      "loss": 0.179,
      "step": 1260
    },
    {
      "epoch": 1.57,
      "grad_norm": 0.010009512305259705,
      "learning_rate": 0.014752392842280483,
      "loss": 0.0526,
      "step": 1261
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.015168179757893085,
      "learning_rate": 0.01474823137744486,
      "loss": 0.2261,
      "step": 1262
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.014308580197393894,
      "learning_rate": 0.014744069912609238,
      "loss": 0.1192,
      "step": 1263
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.019218329340219498,
      "learning_rate": 0.014739908447773616,
      "loss": 0.0826,
      "step": 1264
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.034390877932310104,
      "learning_rate": 0.014735746982937994,
      "loss": 0.6899,
      "step": 1265
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.022100407630205154,
      "learning_rate": 0.014731585518102372,
      "loss": 0.3491,
      "step": 1266
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.028489554300904274,
      "learning_rate": 0.01472742405326675,
      "loss": 0.1444,
      "step": 1267
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.025911176577210426,
      "learning_rate": 0.014723262588431128,
      "loss": 0.3628,
      "step": 1268
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.02298724092543125,
      "learning_rate": 0.014719101123595507,
      "loss": 0.0673,
      "step": 1269
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.01847619004547596,
      "learning_rate": 0.014714939658759883,
      "loss": 0.4316,
      "step": 1270
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.017383543774485588,
      "learning_rate": 0.014710778193924261,
      "loss": 0.2588,
      "step": 1271
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.017542459070682526,
      "learning_rate": 0.01470661672908864,
      "loss": 0.15,
      "step": 1272
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.01959545910358429,
      "learning_rate": 0.014702455264253017,
      "loss": 0.1798,
      "step": 1273
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.019086269661784172,
      "learning_rate": 0.014698293799417395,
      "loss": 0.2642,
      "step": 1274
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.02074088715016842,
      "learning_rate": 0.014694132334581774,
      "loss": 0.2119,
      "step": 1275
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.013050183653831482,
      "learning_rate": 0.014689970869746152,
      "loss": 0.1073,
      "step": 1276
    },
    {
      "epoch": 1.59,
      "grad_norm": 0.029848532751202583,
      "learning_rate": 0.014685809404910528,
      "loss": 0.2302,
      "step": 1277
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.014812586829066277,
      "learning_rate": 0.014681647940074908,
      "loss": 0.1295,
      "step": 1278
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.018400678411126137,
      "learning_rate": 0.014677486475239285,
      "loss": 0.1599,
      "step": 1279
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.013574771583080292,
      "learning_rate": 0.014673325010403662,
      "loss": 0.1953,
      "step": 1280
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.0073502687737345695,
      "learning_rate": 0.014669163545568041,
      "loss": 0.0317,
      "step": 1281
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.01186017319560051,
      "learning_rate": 0.014665002080732419,
      "loss": 0.0259,
      "step": 1282
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.021411679685115814,
      "learning_rate": 0.014660840615896795,
      "loss": 0.0804,
      "step": 1283
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.018703674897551537,
      "learning_rate": 0.014656679151061175,
      "loss": 0.1367,
      "step": 1284
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.013119488954544067,
      "learning_rate": 0.014652517686225552,
      "loss": 0.2534,
      "step": 1285
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.035264965146780014,
      "learning_rate": 0.014648356221389928,
      "loss": 0.2292,
      "step": 1286
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.02243852987885475,
      "learning_rate": 0.014644194756554308,
      "loss": 0.3123,
      "step": 1287
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.03754565119743347,
      "learning_rate": 0.014640033291718686,
      "loss": 1.1035,
      "step": 1288
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.015481263399124146,
      "learning_rate": 0.014635871826883062,
      "loss": 0.4878,
      "step": 1289
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.015926038846373558,
      "learning_rate": 0.014631710362047442,
      "loss": 0.1475,
      "step": 1290
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.02342096157371998,
      "learning_rate": 0.01462754889721182,
      "loss": 0.2212,
      "step": 1291
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.017114590853452682,
      "learning_rate": 0.014623387432376195,
      "loss": 0.1638,
      "step": 1292
    },
    {
      "epoch": 1.61,
      "grad_norm": 0.012445701286196709,
      "learning_rate": 0.014619225967540575,
      "loss": 0.0558,
      "step": 1293
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.026921991258859634,
      "learning_rate": 0.014615064502704953,
      "loss": 0.5972,
      "step": 1294
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.018161725252866745,
      "learning_rate": 0.014610903037869329,
      "loss": 0.2722,
      "step": 1295
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.01985139586031437,
      "learning_rate": 0.014606741573033709,
      "loss": 0.2174,
      "step": 1296
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.020754551514983177,
      "learning_rate": 0.014602580108198086,
      "loss": 0.3433,
      "step": 1297
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.01693241111934185,
      "learning_rate": 0.014598418643362462,
      "loss": 0.0758,
      "step": 1298
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.013273599557578564,
      "learning_rate": 0.014594257178526842,
      "loss": 0.1368,
      "step": 1299
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.028018536046147346,
      "learning_rate": 0.01459009571369122,
      "loss": 0.2196,
      "step": 1300
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.015074766241014004,
      "learning_rate": 0.014585934248855598,
      "loss": 0.1137,
      "step": 1301
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.022739581763744354,
      "learning_rate": 0.014581772784019975,
      "loss": 0.3552,
      "step": 1302
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.034288566559553146,
      "learning_rate": 0.014577611319184353,
      "loss": 0.1115,
      "step": 1303
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.027745721861720085,
      "learning_rate": 0.014573449854348731,
      "loss": 0.5386,
      "step": 1304
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.022506052628159523,
      "learning_rate": 0.014569288389513109,
      "loss": 0.478,
      "step": 1305
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.024695830419659615,
      "learning_rate": 0.014565126924677487,
      "loss": 0.324,
      "step": 1306
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.02637370489537716,
      "learning_rate": 0.014560965459841865,
      "loss": 0.3882,
      "step": 1307
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.019992567598819733,
      "learning_rate": 0.014556803995006242,
      "loss": 0.3813,
      "step": 1308
    },
    {
      "epoch": 1.63,
      "grad_norm": 0.01717136986553669,
      "learning_rate": 0.01455264253017062,
      "loss": 0.153,
      "step": 1309
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.01960982382297516,
      "learning_rate": 0.014548481065334998,
      "loss": 0.4602,
      "step": 1310
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.016627389937639236,
      "learning_rate": 0.014544319600499376,
      "loss": 0.2322,
      "step": 1311
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.008769506588578224,
      "learning_rate": 0.014540158135663755,
      "loss": 0.0365,
      "step": 1312
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.029798436909914017,
      "learning_rate": 0.014535996670828132,
      "loss": 0.6934,
      "step": 1313
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.015637395903468132,
      "learning_rate": 0.01453183520599251,
      "loss": 0.1306,
      "step": 1314
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.017693694680929184,
      "learning_rate": 0.014527673741156889,
      "loss": 0.2228,
      "step": 1315
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.0280127115547657,
      "learning_rate": 0.014523512276321265,
      "loss": 0.5562,
      "step": 1316
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.019847366958856583,
      "learning_rate": 0.014519350811485643,
      "loss": 0.2783,
      "step": 1317
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.023070378229022026,
      "learning_rate": 0.014515189346650022,
      "loss": 0.4016,
      "step": 1318
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.02012251503765583,
      "learning_rate": 0.014511027881814399,
      "loss": 0.2477,
      "step": 1319
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.02138359285891056,
      "learning_rate": 0.014506866416978776,
      "loss": 0.0745,
      "step": 1320
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.015071318484842777,
      "learning_rate": 0.014502704952143156,
      "loss": 0.3311,
      "step": 1321
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.021886644884943962,
      "learning_rate": 0.014498543487307532,
      "loss": 0.2004,
      "step": 1322
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.019193286076188087,
      "learning_rate": 0.01449438202247191,
      "loss": 0.1064,
      "step": 1323
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.017603464424610138,
      "learning_rate": 0.01449022055763629,
      "loss": 0.2197,
      "step": 1324
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.029079299420118332,
      "learning_rate": 0.014486059092800666,
      "loss": 0.4321,
      "step": 1325
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.009835940785706043,
      "learning_rate": 0.014481897627965043,
      "loss": 0.0257,
      "step": 1326
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.01926361583173275,
      "learning_rate": 0.014477736163129423,
      "loss": 0.1555,
      "step": 1327
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.01583191193640232,
      "learning_rate": 0.014473574698293799,
      "loss": 0.1516,
      "step": 1328
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.019557448104023933,
      "learning_rate": 0.014469413233458177,
      "loss": 0.2129,
      "step": 1329
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.015506278723478317,
      "learning_rate": 0.014465251768622556,
      "loss": 0.2209,
      "step": 1330
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.03219875693321228,
      "learning_rate": 0.014461090303786932,
      "loss": 0.0999,
      "step": 1331
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.01541612483561039,
      "learning_rate": 0.01445692883895131,
      "loss": 0.1244,
      "step": 1332
    },
    {
      "epoch": 1.66,
      "grad_norm": 0.022204864770174026,
      "learning_rate": 0.01445276737411569,
      "loss": 0.3271,
      "step": 1333
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.01713612489402294,
      "learning_rate": 0.014448605909280066,
      "loss": 0.1743,
      "step": 1334
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.02419763058423996,
      "learning_rate": 0.014444444444444444,
      "loss": 0.4541,
      "step": 1335
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.00032571720657870173,
      "learning_rate": 0.014440282979608823,
      "loss": 0.0004,
      "step": 1336
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.018572412431240082,
      "learning_rate": 0.0144361215147732,
      "loss": 0.2213,
      "step": 1337
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.016445094719529152,
      "learning_rate": 0.014431960049937577,
      "loss": 0.2042,
      "step": 1338
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.017806435003876686,
      "learning_rate": 0.014427798585101957,
      "loss": 0.3235,
      "step": 1339
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.015416833572089672,
      "learning_rate": 0.014423637120266333,
      "loss": 0.2715,
      "step": 1340
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.02536718361079693,
      "learning_rate": 0.01441947565543071,
      "loss": 0.1826,
      "step": 1341
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.02097124606370926,
      "learning_rate": 0.01441531419059509,
      "loss": 0.3977,
      "step": 1342
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.02159344032406807,
      "learning_rate": 0.014411152725759466,
      "loss": 0.1628,
      "step": 1343
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.02138870395720005,
      "learning_rate": 0.014406991260923846,
      "loss": 0.4041,
      "step": 1344
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.028393961489200592,
      "learning_rate": 0.014402829796088224,
      "loss": 0.3564,
      "step": 1345
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.02029264159500599,
      "learning_rate": 0.0143986683312526,
      "loss": 0.345,
      "step": 1346
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.024640792980790138,
      "learning_rate": 0.01439450686641698,
      "loss": 0.3521,
      "step": 1347
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.016772327944636345,
      "learning_rate": 0.014390345401581357,
      "loss": 0.1578,
      "step": 1348
    },
    {
      "epoch": 1.68,
      "grad_norm": 0.01774672605097294,
      "learning_rate": 0.014386183936745735,
      "loss": 0.1373,
      "step": 1349
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.021757176145911217,
      "learning_rate": 0.014382022471910113,
      "loss": 0.1776,
      "step": 1350
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.022427218034863472,
      "learning_rate": 0.01437786100707449,
      "loss": 0.1674,
      "step": 1351
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.024645455181598663,
      "learning_rate": 0.014373699542238869,
      "loss": 0.4194,
      "step": 1352
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.016755057498812675,
      "learning_rate": 0.014369538077403246,
      "loss": 0.269,
      "step": 1353
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.02398335002362728,
      "learning_rate": 0.014365376612567624,
      "loss": 0.3391,
      "step": 1354
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.02188781090080738,
      "learning_rate": 0.014361215147732004,
      "loss": 0.1846,
      "step": 1355
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.026626283302903175,
      "learning_rate": 0.01435705368289638,
      "loss": 0.3413,
      "step": 1356
    },
    {
      "epoch": 1.69,
      "grad_norm": 0.007642584387212992,
      "learning_rate": 0.014352892218060758,
      "loss": 0.0321,
      "step": 1357
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.01641274429857731,
      "learning_rate": 0.014348730753225137,
      "loss": 0.1602,
      "step": 1358
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.01177943404763937,
      "learning_rate": 0.014344569288389513,
      "loss": 0.0375,
      "step": 1359
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.017289074137806892,
      "learning_rate": 0.014340407823553891,
      "loss": 0.2212,
      "step": 1360
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.02059534192085266,
      "learning_rate": 0.01433624635871827,
      "loss": 0.0945,
      "step": 1361
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.01597408764064312,
      "learning_rate": 0.014332084893882647,
      "loss": 0.1144,
      "step": 1362
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.011774923652410507,
      "learning_rate": 0.014327923429047025,
      "loss": 0.0533,
      "step": 1363
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.0320376418530941,
      "learning_rate": 0.014323761964211404,
      "loss": 0.2842,
      "step": 1364
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.0030445631127804518,
      "learning_rate": 0.01431960049937578,
      "loss": 0.0031,
      "step": 1365
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.02051527611911297,
      "learning_rate": 0.014315439034540158,
      "loss": 0.2717,
      "step": 1366
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.036784928292036057,
      "learning_rate": 0.014311277569704538,
      "loss": 0.2264,
      "step": 1367
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.014145884662866592,
      "learning_rate": 0.014307116104868914,
      "loss": 0.0837,
      "step": 1368
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.016393741592764854,
      "learning_rate": 0.014302954640033292,
      "loss": 0.1398,
      "step": 1369
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.018714239820837975,
      "learning_rate": 0.014298793175197671,
      "loss": 0.3721,
      "step": 1370
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.02131592109799385,
      "learning_rate": 0.014294631710362047,
      "loss": 0.2935,
      "step": 1371
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.02309492789208889,
      "learning_rate": 0.014290470245526425,
      "loss": 0.21,
      "step": 1372
    },
    {
      "epoch": 1.71,
      "grad_norm": 0.026801444590091705,
      "learning_rate": 0.014286308780690805,
      "loss": 0.2778,
      "step": 1373
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.023400824517011642,
      "learning_rate": 0.01428214731585518,
      "loss": 0.3831,
      "step": 1374
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.014906673692166805,
      "learning_rate": 0.014277985851019559,
      "loss": 0.2432,
      "step": 1375
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.022430352866649628,
      "learning_rate": 0.014273824386183938,
      "loss": 0.1935,
      "step": 1376
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.019792569801211357,
      "learning_rate": 0.014269662921348314,
      "loss": 0.1677,
      "step": 1377
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.00937770027667284,
      "learning_rate": 0.014265501456512692,
      "loss": 0.0387,
      "step": 1378
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.013458197005093098,
      "learning_rate": 0.014261339991677072,
      "loss": 0.1322,
      "step": 1379
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.0007534924079664052,
      "learning_rate": 0.014257178526841448,
      "loss": 0.001,
      "step": 1380
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.016316169872879982,
      "learning_rate": 0.014253017062005826,
      "loss": 0.101,
      "step": 1381
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.01418232824653387,
      "learning_rate": 0.014248855597170205,
      "loss": 0.1289,
      "step": 1382
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.0020648986101150513,
      "learning_rate": 0.014244694132334581,
      "loss": 0.0013,
      "step": 1383
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.01654881052672863,
      "learning_rate": 0.014240532667498959,
      "loss": 0.1458,
      "step": 1384
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.021646007895469666,
      "learning_rate": 0.014236371202663339,
      "loss": 0.2888,
      "step": 1385
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.055870696902275085,
      "learning_rate": 0.014232209737827715,
      "loss": 0.8643,
      "step": 1386
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.02059507742524147,
      "learning_rate": 0.014228048272992094,
      "loss": 0.2094,
      "step": 1387
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.014149104245007038,
      "learning_rate": 0.014223886808156472,
      "loss": 0.0392,
      "step": 1388
    },
    {
      "epoch": 1.73,
      "grad_norm": 0.020512647926807404,
      "learning_rate": 0.014219725343320848,
      "loss": 0.0986,
      "step": 1389
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.015520925633609295,
      "learning_rate": 0.014215563878485228,
      "loss": 0.1333,
      "step": 1390
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.02369903400540352,
      "learning_rate": 0.014211402413649606,
      "loss": 0.3264,
      "step": 1391
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.018396614119410515,
      "learning_rate": 0.014207240948813982,
      "loss": 0.17,
      "step": 1392
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.005517400335520506,
      "learning_rate": 0.014203079483978361,
      "loss": 0.0175,
      "step": 1393
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.017674177885055542,
      "learning_rate": 0.014198918019142739,
      "loss": 0.2925,
      "step": 1394
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.02093116194009781,
      "learning_rate": 0.014194756554307115,
      "loss": 0.2559,
      "step": 1395
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.017458520829677582,
      "learning_rate": 0.014190595089471495,
      "loss": 0.1471,
      "step": 1396
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.01619010791182518,
      "learning_rate": 0.014186433624635873,
      "loss": 0.1913,
      "step": 1397
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.020769834518432617,
      "learning_rate": 0.014182272159800249,
      "loss": 0.3037,
      "step": 1398
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.027286134660243988,
      "learning_rate": 0.014178110694964628,
      "loss": 0.4504,
      "step": 1399
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.024402379989624023,
      "learning_rate": 0.014173949230129006,
      "loss": 0.4822,
      "step": 1400
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.019336512312293053,
      "learning_rate": 0.014169787765293382,
      "loss": 0.1427,
      "step": 1401
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.015416628681123257,
      "learning_rate": 0.014165626300457762,
      "loss": 0.2786,
      "step": 1402
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.018515951931476593,
      "learning_rate": 0.01416146483562214,
      "loss": 0.259,
      "step": 1403
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.013942866586148739,
      "learning_rate": 0.014157303370786516,
      "loss": 0.1501,
      "step": 1404
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.012686737813055515,
      "learning_rate": 0.014153141905950895,
      "loss": 0.1641,
      "step": 1405
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.015417816117405891,
      "learning_rate": 0.014148980441115273,
      "loss": 0.0716,
      "step": 1406
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.014001313596963882,
      "learning_rate": 0.014144818976279649,
      "loss": 0.1852,
      "step": 1407
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.016791053116321564,
      "learning_rate": 0.014140657511444029,
      "loss": 0.1738,
      "step": 1408
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.0012489767977967858,
      "learning_rate": 0.014136496046608407,
      "loss": 0.0022,
      "step": 1409
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.020686307922005653,
      "learning_rate": 0.014132334581772783,
      "loss": 0.1094,
      "step": 1410
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.01566472090780735,
      "learning_rate": 0.014128173116937162,
      "loss": 0.0838,
      "step": 1411
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.024927310645580292,
      "learning_rate": 0.01412401165210154,
      "loss": 0.1459,
      "step": 1412
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.02150912955403328,
      "learning_rate": 0.014119850187265916,
      "loss": 0.1733,
      "step": 1413
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.014271970838308334,
      "learning_rate": 0.014115688722430296,
      "loss": 0.1306,
      "step": 1414
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.01386289857327938,
      "learning_rate": 0.014111527257594673,
      "loss": 0.0351,
      "step": 1415
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.03796965628862381,
      "learning_rate": 0.01410736579275905,
      "loss": 0.3135,
      "step": 1416
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.017932679504156113,
      "learning_rate": 0.014103204327923429,
      "loss": 0.1917,
      "step": 1417
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.030670197680592537,
      "learning_rate": 0.014099042863087807,
      "loss": 0.4041,
      "step": 1418
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.017793040722608566,
      "learning_rate": 0.014094881398252185,
      "loss": 0.2169,
      "step": 1419
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.015402521938085556,
      "learning_rate": 0.014090719933416563,
      "loss": 0.0597,
      "step": 1420
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.020960144698619843,
      "learning_rate": 0.01408655846858094,
      "loss": 0.3179,
      "step": 1421
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.020554885268211365,
      "learning_rate": 0.01408239700374532,
      "loss": 0.1812,
      "step": 1422
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.01604786328971386,
      "learning_rate": 0.014078235538909696,
      "loss": 0.129,
      "step": 1423
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.0004181724216323346,
      "learning_rate": 0.014074074074074074,
      "loss": 0.0006,
      "step": 1424
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.02337593026459217,
      "learning_rate": 0.014069912609238453,
      "loss": 0.2957,
      "step": 1425
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.04740644618868828,
      "learning_rate": 0.01406575114440283,
      "loss": 0.2076,
      "step": 1426
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.01808987930417061,
      "learning_rate": 0.014061589679567207,
      "loss": 0.2023,
      "step": 1427
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.035245489329099655,
      "learning_rate": 0.014057428214731587,
      "loss": 0.4993,
      "step": 1428
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.01764986850321293,
      "learning_rate": 0.014053266749895963,
      "loss": 0.1997,
      "step": 1429
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.03306521847844124,
      "learning_rate": 0.014049105285060343,
      "loss": 0.3669,
      "step": 1430
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.021353241056203842,
      "learning_rate": 0.01404494382022472,
      "loss": 0.2961,
      "step": 1431
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.025218380615115166,
      "learning_rate": 0.014040782355389097,
      "loss": 0.2878,
      "step": 1432
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.01481439545750618,
      "learning_rate": 0.014036620890553476,
      "loss": 0.1694,
      "step": 1433
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.015485993586480618,
      "learning_rate": 0.014032459425717854,
      "loss": 0.1014,
      "step": 1434
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.009433499537408352,
      "learning_rate": 0.01402829796088223,
      "loss": 0.0399,
      "step": 1435
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.016402481123805046,
      "learning_rate": 0.01402413649604661,
      "loss": 0.1014,
      "step": 1436
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.02093060128390789,
      "learning_rate": 0.014019975031210987,
      "loss": 0.3584,
      "step": 1437
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.017392264679074287,
      "learning_rate": 0.014015813566375364,
      "loss": 0.1449,
      "step": 1438
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.0166440699249506,
      "learning_rate": 0.014011652101539743,
      "loss": 0.2472,
      "step": 1439
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.012438010424375534,
      "learning_rate": 0.014007490636704121,
      "loss": 0.0868,
      "step": 1440
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.02194707840681076,
      "learning_rate": 0.014003329171868497,
      "loss": 0.3672,
      "step": 1441
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.015828276053071022,
      "learning_rate": 0.013999167707032877,
      "loss": 0.1791,
      "step": 1442
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.026127304881811142,
      "learning_rate": 0.013995006242197254,
      "loss": 0.2229,
      "step": 1443
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.03021225333213806,
      "learning_rate": 0.01399084477736163,
      "loss": 0.5078,
      "step": 1444
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.027687614783644676,
      "learning_rate": 0.01398668331252601,
      "loss": 0.4944,
      "step": 1445
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.02552678808569908,
      "learning_rate": 0.013982521847690388,
      "loss": 0.3657,
      "step": 1446
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.01932724565267563,
      "learning_rate": 0.013978360382854764,
      "loss": 0.4089,
      "step": 1447
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.01600070483982563,
      "learning_rate": 0.013974198918019144,
      "loss": 0.0834,
      "step": 1448
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.01791650988161564,
      "learning_rate": 0.013970037453183521,
      "loss": 0.1436,
      "step": 1449
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.01735217683017254,
      "learning_rate": 0.013965875988347897,
      "loss": 0.311,
      "step": 1450
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.021060368046164513,
      "learning_rate": 0.013961714523512277,
      "loss": 0.4624,
      "step": 1451
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.02310653030872345,
      "learning_rate": 0.013957553058676655,
      "loss": 0.1981,
      "step": 1452
    },
    {
      "epoch": 1.81,
      "grad_norm": 0.011255250312387943,
      "learning_rate": 0.013953391593841031,
      "loss": 0.0776,
      "step": 1453
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.0183134526014328,
      "learning_rate": 0.01394923012900541,
      "loss": 0.1495,
      "step": 1454
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.039122916758060455,
      "learning_rate": 0.013945068664169788,
      "loss": 0.4363,
      "step": 1455
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.03267544507980347,
      "learning_rate": 0.013940907199334164,
      "loss": 0.1599,
      "step": 1456
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.014447005465626717,
      "learning_rate": 0.013936745734498544,
      "loss": 0.2357,
      "step": 1457
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.026220016181468964,
      "learning_rate": 0.013932584269662922,
      "loss": 0.3113,
      "step": 1458
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.021332180127501488,
      "learning_rate": 0.013928422804827298,
      "loss": 0.4189,
      "step": 1459
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.022160692140460014,
      "learning_rate": 0.013924261339991677,
      "loss": 0.1917,
      "step": 1460
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.018558040261268616,
      "learning_rate": 0.013920099875156055,
      "loss": 0.0836,
      "step": 1461
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.02270011231303215,
      "learning_rate": 0.013915938410320433,
      "loss": 0.2104,
      "step": 1462
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.018085598945617676,
      "learning_rate": 0.013911776945484811,
      "loss": 0.3501,
      "step": 1463
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.0162225142121315,
      "learning_rate": 0.013907615480649189,
      "loss": 0.2053,
      "step": 1464
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.03380531072616577,
      "learning_rate": 0.013903454015813567,
      "loss": 0.3596,
      "step": 1465
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.024515025317668915,
      "learning_rate": 0.013899292550977944,
      "loss": 0.3176,
      "step": 1466
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.016738541424274445,
      "learning_rate": 0.013895131086142322,
      "loss": 0.2124,
      "step": 1467
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.07320991158485413,
      "learning_rate": 0.0138909696213067,
      "loss": 0.1891,
      "step": 1468
    },
    {
      "epoch": 1.83,
      "grad_norm": 0.03906169533729553,
      "learning_rate": 0.013886808156471078,
      "loss": 0.3323,
      "step": 1469
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.02159789577126503,
      "learning_rate": 0.013882646691635456,
      "loss": 0.1887,
      "step": 1470
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.015183995477855206,
      "learning_rate": 0.013878485226799834,
      "loss": 0.0684,
      "step": 1471
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.016216358169913292,
      "learning_rate": 0.013874323761964211,
      "loss": 0.1088,
      "step": 1472
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.018191451206803322,
      "learning_rate": 0.013870162297128591,
      "loss": 0.1843,
      "step": 1473
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.02661026082932949,
      "learning_rate": 0.013866000832292967,
      "loss": 0.2686,
      "step": 1474
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.020341236144304276,
      "learning_rate": 0.013861839367457345,
      "loss": 0.1282,
      "step": 1475
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.0009772197809070349,
      "learning_rate": 0.013857677902621724,
      "loss": 0.0012,
      "step": 1476
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.018385665491223335,
      "learning_rate": 0.0138535164377861,
      "loss": 0.2412,
      "step": 1477
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.01054059062153101,
      "learning_rate": 0.013849354972950478,
      "loss": 0.0426,
      "step": 1478
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.0234739538282156,
      "learning_rate": 0.013845193508114858,
      "loss": 0.2435,
      "step": 1479
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.011829260736703873,
      "learning_rate": 0.013841032043279234,
      "loss": 0.0867,
      "step": 1480
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.009333239868283272,
      "learning_rate": 0.013836870578443612,
      "loss": 0.0369,
      "step": 1481
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.016677506268024445,
      "learning_rate": 0.013832709113607991,
      "loss": 0.2181,
      "step": 1482
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.034937627613544464,
      "learning_rate": 0.013828547648772367,
      "loss": 0.3247,
      "step": 1483
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.029054462909698486,
      "learning_rate": 0.013824386183936745,
      "loss": 0.1885,
      "step": 1484
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.023464389145374298,
      "learning_rate": 0.013820224719101125,
      "loss": 0.2864,
      "step": 1485
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.017824359238147736,
      "learning_rate": 0.013816063254265501,
      "loss": 0.1014,
      "step": 1486
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.034080225974321365,
      "learning_rate": 0.013811901789429879,
      "loss": 0.4673,
      "step": 1487
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.020294196903705597,
      "learning_rate": 0.013807740324594258,
      "loss": 0.2617,
      "step": 1488
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.016916709020733833,
      "learning_rate": 0.013803578859758634,
      "loss": 0.0597,
      "step": 1489
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.019215894863009453,
      "learning_rate": 0.013799417394923012,
      "loss": 0.181,
      "step": 1490
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.02869659848511219,
      "learning_rate": 0.013795255930087392,
      "loss": 0.2944,
      "step": 1491
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.011239245533943176,
      "learning_rate": 0.013791094465251768,
      "loss": 0.0978,
      "step": 1492
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.022165564820170403,
      "learning_rate": 0.013786933000416146,
      "loss": 0.2957,
      "step": 1493
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.023557430133223534,
      "learning_rate": 0.013782771535580525,
      "loss": 0.3359,
      "step": 1494
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.031115056946873665,
      "learning_rate": 0.013778610070744903,
      "loss": 0.562,
      "step": 1495
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.0164656825363636,
      "learning_rate": 0.01377444860590928,
      "loss": 0.2074,
      "step": 1496
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.03633042424917221,
      "learning_rate": 0.013770287141073659,
      "loss": 0.3157,
      "step": 1497
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.0232245996594429,
      "learning_rate": 0.013766125676238037,
      "loss": 0.3623,
      "step": 1498
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.01717163436114788,
      "learning_rate": 0.013761964211402413,
      "loss": 0.0588,
      "step": 1499
    },
    {
      "epoch": 1.87,
      "grad_norm": 0.02366768568754196,
      "learning_rate": 0.013757802746566792,
      "loss": 0.3667,
      "step": 1500
    },
    {
      "epoch": 1.87,
      "eval_loss": 0.255859375,
      "eval_runtime": 183.1825,
      "eval_samples_per_second": 1.097,
      "eval_steps_per_second": 0.551,
      "step": 1500
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 4806,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 300,
  "total_flos": 1.7263729655886643e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
