{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.7490636704119851,
  "eval_steps": 300,
  "global_step": 600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0,
      "grad_norm": 0.013946587219834328,
      "learning_rate": 0.01999583853516438,
      "loss": 1.1357,
      "step": 1
    },
    {
      "epoch": 0.0,
      "grad_norm": 0.03152783587574959,
      "learning_rate": 0.019991677070328756,
      "loss": 1.3057,
      "step": 2
    },
    {
      "epoch": 0.0,
      "grad_norm": 0.02430560812354088,
      "learning_rate": 0.019987515605493136,
      "loss": 1.5762,
      "step": 3
    },
    {
      "epoch": 0.0,
      "grad_norm": 0.03687096759676933,
      "learning_rate": 0.01998335414065751,
      "loss": 0.9321,
      "step": 4
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.026205476373434067,
      "learning_rate": 0.019979192675821888,
      "loss": 0.9751,
      "step": 5
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.04537992179393768,
      "learning_rate": 0.019975031210986267,
      "loss": 0.7012,
      "step": 6
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.02319790981709957,
      "learning_rate": 0.019970869746150647,
      "loss": 1.1514,
      "step": 7
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.027198510244488716,
      "learning_rate": 0.019966708281315023,
      "loss": 1.1768,
      "step": 8
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.026688028126955032,
      "learning_rate": 0.019962546816479403,
      "loss": 1.0107,
      "step": 9
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.027742978185415268,
      "learning_rate": 0.01995838535164378,
      "loss": 0.52,
      "step": 10
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.017290690913796425,
      "learning_rate": 0.019954223886808155,
      "loss": 0.5259,
      "step": 11
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.024143831804394722,
      "learning_rate": 0.019950062421972534,
      "loss": 0.895,
      "step": 12
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.03602828457951546,
      "learning_rate": 0.019945900957136914,
      "loss": 1.042,
      "step": 13
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.03868493810296059,
      "learning_rate": 0.01994173949230129,
      "loss": 0.9253,
      "step": 14
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.019538432359695435,
      "learning_rate": 0.01993757802746567,
      "loss": 0.8286,
      "step": 15
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.028750460594892502,
      "learning_rate": 0.019933416562630046,
      "loss": 0.5039,
      "step": 16
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.022473318502306938,
      "learning_rate": 0.019929255097794422,
      "loss": 0.8252,
      "step": 17
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.01876644417643547,
      "learning_rate": 0.0199250936329588,
      "loss": 0.5903,
      "step": 18
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.029065033420920372,
      "learning_rate": 0.01992093216812318,
      "loss": 0.6187,
      "step": 19
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.014426208101212978,
      "learning_rate": 0.019916770703287557,
      "loss": 0.2732,
      "step": 20
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.01835629716515541,
      "learning_rate": 0.019912609238451937,
      "loss": 0.5063,
      "step": 21
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.021680880337953568,
      "learning_rate": 0.019908447773616313,
      "loss": 0.5586,
      "step": 22
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.018381770700216293,
      "learning_rate": 0.01990428630878069,
      "loss": 0.4036,
      "step": 23
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.0139843188226223,
      "learning_rate": 0.01990012484394507,
      "loss": 0.2484,
      "step": 24
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.022173702716827393,
      "learning_rate": 0.019895963379109448,
      "loss": 0.7319,
      "step": 25
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.019779937341809273,
      "learning_rate": 0.019891801914273824,
      "loss": 0.25,
      "step": 26
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.015408563427627087,
      "learning_rate": 0.019887640449438203,
      "loss": 0.6279,
      "step": 27
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.017980150878429413,
      "learning_rate": 0.01988347898460258,
      "loss": 0.7153,
      "step": 28
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.01553407683968544,
      "learning_rate": 0.019879317519766956,
      "loss": 0.6553,
      "step": 29
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.017622757703065872,
      "learning_rate": 0.019875156054931335,
      "loss": 0.54,
      "step": 30
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.019046414643526077,
      "learning_rate": 0.019870994590095715,
      "loss": 0.7466,
      "step": 31
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.018101511523127556,
      "learning_rate": 0.01986683312526009,
      "loss": 0.6934,
      "step": 32
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.01025469321757555,
      "learning_rate": 0.01986267166042447,
      "loss": 0.5674,
      "step": 33
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.02104567363858223,
      "learning_rate": 0.019858510195588847,
      "loss": 0.8809,
      "step": 34
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.011890831403434277,
      "learning_rate": 0.019854348730753226,
      "loss": 0.2898,
      "step": 35
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.022102177143096924,
      "learning_rate": 0.019850187265917602,
      "loss": 0.48,
      "step": 36
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.014674047939479351,
      "learning_rate": 0.019846025801081982,
      "loss": 0.3022,
      "step": 37
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.019139140844345093,
      "learning_rate": 0.01984186433624636,
      "loss": 0.3066,
      "step": 38
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.016503287479281425,
      "learning_rate": 0.019837702871410737,
      "loss": 0.5918,
      "step": 39
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.019555386155843735,
      "learning_rate": 0.019833541406575114,
      "loss": 0.4966,
      "step": 40
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.015926335006952286,
      "learning_rate": 0.019829379941739493,
      "loss": 0.4143,
      "step": 41
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.016720635816454887,
      "learning_rate": 0.01982521847690387,
      "loss": 0.6763,
      "step": 42
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.010436618700623512,
      "learning_rate": 0.01982105701206825,
      "loss": 0.7246,
      "step": 43
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.016686901450157166,
      "learning_rate": 0.01981689554723263,
      "loss": 0.321,
      "step": 44
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.013563442043960094,
      "learning_rate": 0.019812734082397004,
      "loss": 0.5669,
      "step": 45
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.01583418808877468,
      "learning_rate": 0.019808572617561384,
      "loss": 0.4619,
      "step": 46
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.012616423889994621,
      "learning_rate": 0.01980441115272576,
      "loss": 0.6016,
      "step": 47
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.019653743132948875,
      "learning_rate": 0.019800249687890136,
      "loss": 0.262,
      "step": 48
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.01714310795068741,
      "learning_rate": 0.019796088223054516,
      "loss": 0.4636,
      "step": 49
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.012529288418591022,
      "learning_rate": 0.019791926758218895,
      "loss": 0.0888,
      "step": 50
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.025462329387664795,
      "learning_rate": 0.01978776529338327,
      "loss": 1.1182,
      "step": 51
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.013185207732021809,
      "learning_rate": 0.01978360382854765,
      "loss": 0.6724,
      "step": 52
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.025705890730023384,
      "learning_rate": 0.019779442363712027,
      "loss": 1.2998,
      "step": 53
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.018231689929962158,
      "learning_rate": 0.019775280898876403,
      "loss": 0.4458,
      "step": 54
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.013859216123819351,
      "learning_rate": 0.019771119434040783,
      "loss": 0.623,
      "step": 55
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.020070232450962067,
      "learning_rate": 0.019766957969205162,
      "loss": 0.73,
      "step": 56
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.012890408746898174,
      "learning_rate": 0.01976279650436954,
      "loss": 0.4128,
      "step": 57
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.014063001610338688,
      "learning_rate": 0.019758635039533918,
      "loss": 0.4192,
      "step": 58
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.016345465555787086,
      "learning_rate": 0.019754473574698294,
      "loss": 0.5889,
      "step": 59
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.0205059964209795,
      "learning_rate": 0.01975031210986267,
      "loss": 0.4709,
      "step": 60
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.014671096578240395,
      "learning_rate": 0.01974615064502705,
      "loss": 0.3325,
      "step": 61
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.011892193928360939,
      "learning_rate": 0.01974198918019143,
      "loss": 0.4302,
      "step": 62
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.015515914186835289,
      "learning_rate": 0.019737827715355805,
      "loss": 0.9307,
      "step": 63
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.01472906768321991,
      "learning_rate": 0.019733666250520185,
      "loss": 0.3135,
      "step": 64
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.013207866810262203,
      "learning_rate": 0.01972950478568456,
      "loss": 0.4983,
      "step": 65
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.006755692884325981,
      "learning_rate": 0.019725343320848937,
      "loss": 0.4126,
      "step": 66
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.013154924847185612,
      "learning_rate": 0.019721181856013317,
      "loss": 0.5415,
      "step": 67
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.012332231737673283,
      "learning_rate": 0.019717020391177696,
      "loss": 0.5249,
      "step": 68
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.01766934059560299,
      "learning_rate": 0.019712858926342072,
      "loss": 0.4358,
      "step": 69
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.018197795376181602,
      "learning_rate": 0.019708697461506452,
      "loss": 0.6753,
      "step": 70
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.016164090484380722,
      "learning_rate": 0.019704535996670828,
      "loss": 0.4712,
      "step": 71
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.029932130128145218,
      "learning_rate": 0.019700374531835204,
      "loss": 0.5342,
      "step": 72
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.01345884520560503,
      "learning_rate": 0.019696213066999584,
      "loss": 0.5029,
      "step": 73
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.01129131205379963,
      "learning_rate": 0.019692051602163963,
      "loss": 0.5376,
      "step": 74
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.016407852992415428,
      "learning_rate": 0.01968789013732834,
      "loss": 0.791,
      "step": 75
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.019868925213813782,
      "learning_rate": 0.01968372867249272,
      "loss": 0.5015,
      "step": 76
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.0146525539457798,
      "learning_rate": 0.019679567207657095,
      "loss": 0.541,
      "step": 77
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.010244067758321762,
      "learning_rate": 0.019675405742821474,
      "loss": 0.2642,
      "step": 78
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.015067693777382374,
      "learning_rate": 0.01967124427798585,
      "loss": 0.2817,
      "step": 79
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.010389878414571285,
      "learning_rate": 0.01966708281315023,
      "loss": 0.6113,
      "step": 80
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.011934892274439335,
      "learning_rate": 0.019662921348314606,
      "loss": 0.0456,
      "step": 81
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.01124374195933342,
      "learning_rate": 0.019658759883478986,
      "loss": 0.2357,
      "step": 82
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.017671743407845497,
      "learning_rate": 0.019654598418643362,
      "loss": 0.5024,
      "step": 83
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.02609691210091114,
      "learning_rate": 0.01965043695380774,
      "loss": 0.5361,
      "step": 84
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.01812068186700344,
      "learning_rate": 0.019646275488972118,
      "loss": 0.4084,
      "step": 85
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.014044742099940777,
      "learning_rate": 0.019642114024136497,
      "loss": 0.2783,
      "step": 86
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.013514760881662369,
      "learning_rate": 0.019637952559300873,
      "loss": 0.5366,
      "step": 87
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.01608353666961193,
      "learning_rate": 0.019633791094465253,
      "loss": 0.4722,
      "step": 88
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.014681047759950161,
      "learning_rate": 0.019629629629629632,
      "loss": 0.3625,
      "step": 89
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.011236661113798618,
      "learning_rate": 0.01962546816479401,
      "loss": 0.2866,
      "step": 90
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.014246581122279167,
      "learning_rate": 0.019621306699958384,
      "loss": 0.4597,
      "step": 91
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.015461236238479614,
      "learning_rate": 0.019617145235122764,
      "loss": 0.251,
      "step": 92
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.013209463097155094,
      "learning_rate": 0.01961298377028714,
      "loss": 0.4397,
      "step": 93
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.017247267067432404,
      "learning_rate": 0.01960882230545152,
      "loss": 0.1571,
      "step": 94
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.0060082110576331615,
      "learning_rate": 0.0196046608406159,
      "loss": 0.0955,
      "step": 95
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.04800603538751602,
      "learning_rate": 0.019600499375780275,
      "loss": 0.5498,
      "step": 96
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.010818454436957836,
      "learning_rate": 0.01959633791094465,
      "loss": 0.3552,
      "step": 97
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.013875529170036316,
      "learning_rate": 0.01959217644610903,
      "loss": 0.2754,
      "step": 98
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.009726175107061863,
      "learning_rate": 0.019588014981273407,
      "loss": 0.5693,
      "step": 99
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.010561893694102764,
      "learning_rate": 0.019583853516437787,
      "loss": 0.269,
      "step": 100
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.018042325973510742,
      "learning_rate": 0.019579692051602166,
      "loss": 0.2888,
      "step": 101
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.011434352956712246,
      "learning_rate": 0.019575530586766542,
      "loss": 0.3032,
      "step": 102
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.01294125895947218,
      "learning_rate": 0.01957136912193092,
      "loss": 0.3904,
      "step": 103
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.00973493605852127,
      "learning_rate": 0.019567207657095298,
      "loss": 0.1273,
      "step": 104
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.011769156903028488,
      "learning_rate": 0.019563046192259674,
      "loss": 0.3728,
      "step": 105
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.013946526683866978,
      "learning_rate": 0.019558884727424054,
      "loss": 0.4209,
      "step": 106
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.024769112467765808,
      "learning_rate": 0.019554723262588433,
      "loss": 0.5957,
      "step": 107
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.013953709043562412,
      "learning_rate": 0.01955056179775281,
      "loss": 0.896,
      "step": 108
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.016479069367051125,
      "learning_rate": 0.019546400332917185,
      "loss": 0.2915,
      "step": 109
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.012262118980288506,
      "learning_rate": 0.019542238868081565,
      "loss": 0.3962,
      "step": 110
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.009211440570652485,
      "learning_rate": 0.019538077403245944,
      "loss": 0.2974,
      "step": 111
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.01734967529773712,
      "learning_rate": 0.01953391593841032,
      "loss": 0.583,
      "step": 112
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.02681916393339634,
      "learning_rate": 0.0195297544735747,
      "loss": 0.5518,
      "step": 113
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.019412348046898842,
      "learning_rate": 0.019525593008739076,
      "loss": 0.1301,
      "step": 114
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.0728633925318718,
      "learning_rate": 0.019521431543903452,
      "loss": 0.3469,
      "step": 115
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.010789559222757816,
      "learning_rate": 0.019517270079067832,
      "loss": 0.2566,
      "step": 116
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.01410316675901413,
      "learning_rate": 0.01951310861423221,
      "loss": 0.6997,
      "step": 117
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.02386266365647316,
      "learning_rate": 0.019508947149396588,
      "loss": 0.7905,
      "step": 118
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.01500307023525238,
      "learning_rate": 0.019504785684560967,
      "loss": 0.311,
      "step": 119
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.01638227328658104,
      "learning_rate": 0.019500624219725343,
      "loss": 0.377,
      "step": 120
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.018486998975276947,
      "learning_rate": 0.019496462754889723,
      "loss": 0.4883,
      "step": 121
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.004287502728402615,
      "learning_rate": 0.0194923012900541,
      "loss": 0.0809,
      "step": 122
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.012090643867850304,
      "learning_rate": 0.01948813982521848,
      "loss": 0.3147,
      "step": 123
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.012809454463422298,
      "learning_rate": 0.019483978360382855,
      "loss": 0.2346,
      "step": 124
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.03159857168793678,
      "learning_rate": 0.019479816895547234,
      "loss": 0.0633,
      "step": 125
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.03378335013985634,
      "learning_rate": 0.01947565543071161,
      "loss": 0.4414,
      "step": 126
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.00972415879368782,
      "learning_rate": 0.01947149396587599,
      "loss": 0.1332,
      "step": 127
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.014316284097731113,
      "learning_rate": 0.019467332501040366,
      "loss": 0.2642,
      "step": 128
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.01945355348289013,
      "learning_rate": 0.019463171036204745,
      "loss": 0.2568,
      "step": 129
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.014685138128697872,
      "learning_rate": 0.01945900957136912,
      "loss": 0.2495,
      "step": 130
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.010893230326473713,
      "learning_rate": 0.0194548481065335,
      "loss": 0.1844,
      "step": 131
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.015432030893862247,
      "learning_rate": 0.01945068664169788,
      "loss": 0.2192,
      "step": 132
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.023381555452942848,
      "learning_rate": 0.019446525176862257,
      "loss": 0.1019,
      "step": 133
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.025069458410143852,
      "learning_rate": 0.019442363712026633,
      "loss": 0.6982,
      "step": 134
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.013284072279930115,
      "learning_rate": 0.019438202247191012,
      "loss": 0.2725,
      "step": 135
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.01459498330950737,
      "learning_rate": 0.01943404078235539,
      "loss": 0.342,
      "step": 136
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.010649852454662323,
      "learning_rate": 0.019429879317519768,
      "loss": 0.1597,
      "step": 137
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.00572684733197093,
      "learning_rate": 0.019425717852684148,
      "loss": 0.0163,
      "step": 138
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.010500311851501465,
      "learning_rate": 0.019421556387848524,
      "loss": 0.3718,
      "step": 139
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.02403194084763527,
      "learning_rate": 0.0194173949230129,
      "loss": 0.3464,
      "step": 140
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.015808913856744766,
      "learning_rate": 0.01941323345817728,
      "loss": 0.0919,
      "step": 141
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.017955824732780457,
      "learning_rate": 0.019409071993341655,
      "loss": 0.3323,
      "step": 142
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.015312157571315765,
      "learning_rate": 0.019404910528506035,
      "loss": 0.4712,
      "step": 143
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.013958172872662544,
      "learning_rate": 0.019400749063670415,
      "loss": 0.1681,
      "step": 144
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.01720457337796688,
      "learning_rate": 0.01939658759883479,
      "loss": 0.541,
      "step": 145
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.017649512737989426,
      "learning_rate": 0.019392426133999167,
      "loss": 0.6196,
      "step": 146
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.016069641336798668,
      "learning_rate": 0.019388264669163546,
      "loss": 0.3416,
      "step": 147
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.015727760270237923,
      "learning_rate": 0.019384103204327922,
      "loss": 0.2766,
      "step": 148
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.011026602238416672,
      "learning_rate": 0.019379941739492302,
      "loss": 0.2603,
      "step": 149
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.011536615900695324,
      "learning_rate": 0.01937578027465668,
      "loss": 0.0615,
      "step": 150
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.018519138917326927,
      "learning_rate": 0.019371618809821058,
      "loss": 0.2013,
      "step": 151
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.019297972321510315,
      "learning_rate": 0.019367457344985434,
      "loss": 0.7739,
      "step": 152
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.035224251449108124,
      "learning_rate": 0.019363295880149813,
      "loss": 0.1853,
      "step": 153
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.011118430644273758,
      "learning_rate": 0.01935913441531419,
      "loss": 0.585,
      "step": 154
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.01876828819513321,
      "learning_rate": 0.01935497295047857,
      "loss": 0.439,
      "step": 155
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.013631806708872318,
      "learning_rate": 0.01935081148564295,
      "loss": 0.2147,
      "step": 156
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.013377000577747822,
      "learning_rate": 0.019346650020807325,
      "loss": 0.0768,
      "step": 157
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.009209630079567432,
      "learning_rate": 0.0193424885559717,
      "loss": 0.12,
      "step": 158
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.010687937960028648,
      "learning_rate": 0.01933832709113608,
      "loss": 0.3562,
      "step": 159
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.01562159787863493,
      "learning_rate": 0.019334165626300456,
      "loss": 0.2192,
      "step": 160
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.005969279445707798,
      "learning_rate": 0.019330004161464836,
      "loss": 0.0499,
      "step": 161
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.01860101707279682,
      "learning_rate": 0.019325842696629215,
      "loss": 0.4082,
      "step": 162
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.013926600106060505,
      "learning_rate": 0.01932168123179359,
      "loss": 0.27,
      "step": 163
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.007220026105642319,
      "learning_rate": 0.01931751976695797,
      "loss": 0.139,
      "step": 164
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.016826435923576355,
      "learning_rate": 0.019313358302122347,
      "loss": 0.4153,
      "step": 165
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.008984547108411789,
      "learning_rate": 0.019309196837286723,
      "loss": 0.2205,
      "step": 166
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.014899051748216152,
      "learning_rate": 0.019305035372451103,
      "loss": 0.4397,
      "step": 167
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.008856400847434998,
      "learning_rate": 0.019300873907615482,
      "loss": 0.0795,
      "step": 168
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.015470389276742935,
      "learning_rate": 0.01929671244277986,
      "loss": 0.6055,
      "step": 169
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.01177644170820713,
      "learning_rate": 0.019292550977944238,
      "loss": 0.3594,
      "step": 170
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.018051499500870705,
      "learning_rate": 0.019288389513108614,
      "loss": 0.7207,
      "step": 171
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.01194706466048956,
      "learning_rate": 0.01928422804827299,
      "loss": 0.4624,
      "step": 172
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.021377161145210266,
      "learning_rate": 0.01928006658343737,
      "loss": 0.4302,
      "step": 173
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.011635289527475834,
      "learning_rate": 0.01927590511860175,
      "loss": 0.2068,
      "step": 174
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.012977547012269497,
      "learning_rate": 0.019271743653766125,
      "loss": 0.3076,
      "step": 175
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.011483551934361458,
      "learning_rate": 0.019267582188930505,
      "loss": 0.2173,
      "step": 176
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.018402379006147385,
      "learning_rate": 0.01926342072409488,
      "loss": 0.3831,
      "step": 177
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.008669276721775532,
      "learning_rate": 0.019259259259259257,
      "loss": 0.1942,
      "step": 178
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.009989948943257332,
      "learning_rate": 0.019255097794423637,
      "loss": 0.4062,
      "step": 179
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.007285960018634796,
      "learning_rate": 0.019250936329588016,
      "loss": 0.0542,
      "step": 180
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.022931115701794624,
      "learning_rate": 0.019246774864752392,
      "loss": 0.3311,
      "step": 181
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.015280869789421558,
      "learning_rate": 0.019242613399916772,
      "loss": 0.1851,
      "step": 182
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.016446208581328392,
      "learning_rate": 0.019238451935081148,
      "loss": 0.3059,
      "step": 183
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.009221578016877174,
      "learning_rate": 0.019234290470245528,
      "loss": 0.1036,
      "step": 184
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.01323636807501316,
      "learning_rate": 0.019230129005409904,
      "loss": 0.3828,
      "step": 185
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.011723767034709454,
      "learning_rate": 0.019225967540574283,
      "loss": 0.6211,
      "step": 186
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.00718157272785902,
      "learning_rate": 0.019221806075738663,
      "loss": 0.1534,
      "step": 187
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.009665646590292454,
      "learning_rate": 0.01921764461090304,
      "loss": 0.1459,
      "step": 188
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.01421988382935524,
      "learning_rate": 0.019213483146067415,
      "loss": 0.3362,
      "step": 189
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.014052601531147957,
      "learning_rate": 0.019209321681231795,
      "loss": 0.4727,
      "step": 190
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.007880319841206074,
      "learning_rate": 0.01920516021639617,
      "loss": 0.0958,
      "step": 191
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.013098879717290401,
      "learning_rate": 0.01920099875156055,
      "loss": 0.2483,
      "step": 192
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.010881153866648674,
      "learning_rate": 0.01919683728672493,
      "loss": 0.0822,
      "step": 193
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.011607570573687553,
      "learning_rate": 0.019192675821889306,
      "loss": 0.4939,
      "step": 194
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.015284847468137741,
      "learning_rate": 0.019188514357053682,
      "loss": 0.4248,
      "step": 195
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.006532551255077124,
      "learning_rate": 0.01918435289221806,
      "loss": 0.0183,
      "step": 196
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.00698343338444829,
      "learning_rate": 0.019180191427382438,
      "loss": 0.0674,
      "step": 197
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.01601043902337551,
      "learning_rate": 0.019176029962546817,
      "loss": 1.0,
      "step": 198
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.018778465688228607,
      "learning_rate": 0.019171868497711197,
      "loss": 0.5615,
      "step": 199
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.017414990812540054,
      "learning_rate": 0.019167707032875573,
      "loss": 0.4424,
      "step": 200
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.009499352425336838,
      "learning_rate": 0.01916354556803995,
      "loss": 0.0865,
      "step": 201
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.011209850199520588,
      "learning_rate": 0.01915938410320433,
      "loss": 0.3103,
      "step": 202
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.011232535354793072,
      "learning_rate": 0.019155222638368705,
      "loss": 0.3169,
      "step": 203
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.016806548461318016,
      "learning_rate": 0.019151061173533084,
      "loss": 0.6245,
      "step": 204
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.01189274899661541,
      "learning_rate": 0.019146899708697464,
      "loss": 0.1663,
      "step": 205
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.015232094563543797,
      "learning_rate": 0.01914273824386184,
      "loss": 0.6851,
      "step": 206
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.02008361555635929,
      "learning_rate": 0.01913857677902622,
      "loss": 0.4065,
      "step": 207
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.007860634475946426,
      "learning_rate": 0.019134415314190596,
      "loss": 0.114,
      "step": 208
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.009345010854303837,
      "learning_rate": 0.01913025384935497,
      "loss": 0.3083,
      "step": 209
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.01482164952903986,
      "learning_rate": 0.01912609238451935,
      "loss": 0.5288,
      "step": 210
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.013445671647787094,
      "learning_rate": 0.01912193091968373,
      "loss": 0.0577,
      "step": 211
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.01909550465643406,
      "learning_rate": 0.019117769454848107,
      "loss": 0.3801,
      "step": 212
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.013162663206458092,
      "learning_rate": 0.019113607990012486,
      "loss": 0.2639,
      "step": 213
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.01083003357052803,
      "learning_rate": 0.019109446525176862,
      "loss": 0.1499,
      "step": 214
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.00959020759910345,
      "learning_rate": 0.01910528506034124,
      "loss": 0.1101,
      "step": 215
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.010005058720707893,
      "learning_rate": 0.019101123595505618,
      "loss": 0.2026,
      "step": 216
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.02039876952767372,
      "learning_rate": 0.019096962130669998,
      "loss": 0.282,
      "step": 217
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.014765151776373386,
      "learning_rate": 0.019092800665834374,
      "loss": 0.3125,
      "step": 218
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.009571024216711521,
      "learning_rate": 0.019088639200998753,
      "loss": 0.183,
      "step": 219
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.010038415901362896,
      "learning_rate": 0.01908447773616313,
      "loss": 0.1028,
      "step": 220
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.012818839401006699,
      "learning_rate": 0.019080316271327506,
      "loss": 0.5225,
      "step": 221
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.014451993629336357,
      "learning_rate": 0.019076154806491885,
      "loss": 0.168,
      "step": 222
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.006471678148955107,
      "learning_rate": 0.019071993341656265,
      "loss": 0.0197,
      "step": 223
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.004482632502913475,
      "learning_rate": 0.01906783187682064,
      "loss": 0.0293,
      "step": 224
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.016593338921666145,
      "learning_rate": 0.01906367041198502,
      "loss": 0.5806,
      "step": 225
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.00906510278582573,
      "learning_rate": 0.019059508947149396,
      "loss": 0.1466,
      "step": 226
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.010454477742314339,
      "learning_rate": 0.019055347482313773,
      "loss": 0.1746,
      "step": 227
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.0206717811524868,
      "learning_rate": 0.019051186017478152,
      "loss": 0.6543,
      "step": 228
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.009957603178918362,
      "learning_rate": 0.01904702455264253,
      "loss": 0.0541,
      "step": 229
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.02599034458398819,
      "learning_rate": 0.019042863087806908,
      "loss": 0.4255,
      "step": 230
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.017418760806322098,
      "learning_rate": 0.019038701622971287,
      "loss": 0.4253,
      "step": 231
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.013599650003015995,
      "learning_rate": 0.019034540158135663,
      "loss": 0.3621,
      "step": 232
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.012645847164094448,
      "learning_rate": 0.01903037869330004,
      "loss": 0.1808,
      "step": 233
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.011502467095851898,
      "learning_rate": 0.01902621722846442,
      "loss": 0.4675,
      "step": 234
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.009859512560069561,
      "learning_rate": 0.0190220557636288,
      "loss": 0.0282,
      "step": 235
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.009849142283201218,
      "learning_rate": 0.019017894298793175,
      "loss": 0.0837,
      "step": 236
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.015944460406899452,
      "learning_rate": 0.019013732833957554,
      "loss": 0.2374,
      "step": 237
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.015828672796487808,
      "learning_rate": 0.01900957136912193,
      "loss": 0.5649,
      "step": 238
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.011765114963054657,
      "learning_rate": 0.01900540990428631,
      "loss": 0.4055,
      "step": 239
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.012066415511071682,
      "learning_rate": 0.019001248439450686,
      "loss": 0.3311,
      "step": 240
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.01208413578569889,
      "learning_rate": 0.018997086974615066,
      "loss": 0.3303,
      "step": 241
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.020172521471977234,
      "learning_rate": 0.01899292550977944,
      "loss": 0.6533,
      "step": 242
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.013071781024336815,
      "learning_rate": 0.01898876404494382,
      "loss": 0.2922,
      "step": 243
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.014257648959755898,
      "learning_rate": 0.018984602580108197,
      "loss": 0.5166,
      "step": 244
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.008450665511190891,
      "learning_rate": 0.018980441115272577,
      "loss": 0.1483,
      "step": 245
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01024819165468216,
      "learning_rate": 0.018976279650436953,
      "loss": 0.3325,
      "step": 246
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.012138486839830875,
      "learning_rate": 0.018972118185601333,
      "loss": 0.3181,
      "step": 247
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01706678234040737,
      "learning_rate": 0.01896795672076571,
      "loss": 0.5381,
      "step": 248
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.0165503341704607,
      "learning_rate": 0.018963795255930088,
      "loss": 0.3474,
      "step": 249
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.017817780375480652,
      "learning_rate": 0.018959633791094468,
      "loss": 0.1743,
      "step": 250
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01351422630250454,
      "learning_rate": 0.018955472326258844,
      "loss": 0.1572,
      "step": 251
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.01826195791363716,
      "learning_rate": 0.01895131086142322,
      "loss": 0.3682,
      "step": 252
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.01721639558672905,
      "learning_rate": 0.0189471493965876,
      "loss": 0.4712,
      "step": 253
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.0058861286379396915,
      "learning_rate": 0.018942987931751976,
      "loss": 0.0426,
      "step": 254
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.0133828679099679,
      "learning_rate": 0.018938826466916355,
      "loss": 0.3984,
      "step": 255
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.015871714800596237,
      "learning_rate": 0.018934665002080735,
      "loss": 0.4358,
      "step": 256
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.012308573350310326,
      "learning_rate": 0.01893050353724511,
      "loss": 0.303,
      "step": 257
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.010923982597887516,
      "learning_rate": 0.018926342072409487,
      "loss": 0.1774,
      "step": 258
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.013635121285915375,
      "learning_rate": 0.018922180607573866,
      "loss": 0.4199,
      "step": 259
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.013736642897129059,
      "learning_rate": 0.018918019142738246,
      "loss": 0.3162,
      "step": 260
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.018210401758551598,
      "learning_rate": 0.018913857677902622,
      "loss": 0.3315,
      "step": 261
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.01903851330280304,
      "learning_rate": 0.018909696213067,
      "loss": 0.6294,
      "step": 262
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.01758592203259468,
      "learning_rate": 0.018905534748231378,
      "loss": 0.853,
      "step": 263
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.014722253195941448,
      "learning_rate": 0.018901373283395754,
      "loss": 0.2148,
      "step": 264
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.010228004306554794,
      "learning_rate": 0.018897211818560133,
      "loss": 0.07,
      "step": 265
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.017041178420186043,
      "learning_rate": 0.018893050353724513,
      "loss": 0.3865,
      "step": 266
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.024548158049583435,
      "learning_rate": 0.01888888888888889,
      "loss": 0.408,
      "step": 267
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.021002428606152534,
      "learning_rate": 0.01888472742405327,
      "loss": 0.4521,
      "step": 268
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.023030998185276985,
      "learning_rate": 0.018880565959217645,
      "loss": 0.6274,
      "step": 269
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.01349740382283926,
      "learning_rate": 0.01887640449438202,
      "loss": 0.3054,
      "step": 270
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.01190619170665741,
      "learning_rate": 0.0188722430295464,
      "loss": 0.1816,
      "step": 271
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.01658334955573082,
      "learning_rate": 0.01886808156471078,
      "loss": 0.491,
      "step": 272
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.008173597976565361,
      "learning_rate": 0.018863920099875156,
      "loss": 0.0178,
      "step": 273
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.017877396196126938,
      "learning_rate": 0.018859758635039536,
      "loss": 0.4114,
      "step": 274
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.012737761251628399,
      "learning_rate": 0.01885559717020391,
      "loss": 0.2288,
      "step": 275
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.02273883856832981,
      "learning_rate": 0.018851435705368288,
      "loss": 0.4519,
      "step": 276
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.006959815509617329,
      "learning_rate": 0.018847274240532667,
      "loss": 0.0447,
      "step": 277
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.012611573562026024,
      "learning_rate": 0.018843112775697047,
      "loss": 0.1226,
      "step": 278
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.014003496617078781,
      "learning_rate": 0.018838951310861423,
      "loss": 0.4014,
      "step": 279
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.01877623423933983,
      "learning_rate": 0.018834789846025803,
      "loss": 0.2646,
      "step": 280
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.018713688477873802,
      "learning_rate": 0.01883062838119018,
      "loss": 0.5034,
      "step": 281
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.02422862872481346,
      "learning_rate": 0.018826466916354558,
      "loss": 0.8271,
      "step": 282
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.016888445243239403,
      "learning_rate": 0.018822305451518934,
      "loss": 0.541,
      "step": 283
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.025673290714621544,
      "learning_rate": 0.018818143986683314,
      "loss": 0.5269,
      "step": 284
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.013471826910972595,
      "learning_rate": 0.01881398252184769,
      "loss": 0.1483,
      "step": 285
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.016805335879325867,
      "learning_rate": 0.01880982105701207,
      "loss": 0.4895,
      "step": 286
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.02230897918343544,
      "learning_rate": 0.018805659592176446,
      "loss": 0.2627,
      "step": 287
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.017700405791401863,
      "learning_rate": 0.018801498127340825,
      "loss": 0.2869,
      "step": 288
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.026683760806918144,
      "learning_rate": 0.0187973366625052,
      "loss": 0.4714,
      "step": 289
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.017930233851075172,
      "learning_rate": 0.01879317519766958,
      "loss": 0.124,
      "step": 290
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.014429387636482716,
      "learning_rate": 0.018789013732833957,
      "loss": 0.4141,
      "step": 291
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.012393507175147533,
      "learning_rate": 0.018784852267998337,
      "loss": 0.1812,
      "step": 292
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.013746536336839199,
      "learning_rate": 0.018780690803162716,
      "loss": 0.2296,
      "step": 293
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.017592409625649452,
      "learning_rate": 0.018776529338327092,
      "loss": 0.4626,
      "step": 294
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.02273542992770672,
      "learning_rate": 0.01877236787349147,
      "loss": 0.2491,
      "step": 295
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.04013910889625549,
      "learning_rate": 0.018768206408655848,
      "loss": 0.5811,
      "step": 296
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.012396056205034256,
      "learning_rate": 0.018764044943820224,
      "loss": 0.5254,
      "step": 297
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.01785169169306755,
      "learning_rate": 0.018759883478984603,
      "loss": 0.397,
      "step": 298
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.01874547079205513,
      "learning_rate": 0.018755722014148983,
      "loss": 0.2937,
      "step": 299
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.01092853955924511,
      "learning_rate": 0.01875156054931336,
      "loss": 0.4192,
      "step": 300
    },
    {
      "epoch": 0.37,
      "eval_loss": 0.3251953125,
      "eval_runtime": 183.3307,
      "eval_samples_per_second": 1.096,
      "eval_steps_per_second": 0.551,
      "step": 300
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.012453447096049786,
      "learning_rate": 0.018747399084477735,
      "loss": 0.2705,
      "step": 301
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.020620588213205338,
      "learning_rate": 0.018743237619642115,
      "loss": 0.6685,
      "step": 302
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.018006984144449234,
      "learning_rate": 0.01873907615480649,
      "loss": 0.4458,
      "step": 303
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.012783350422978401,
      "learning_rate": 0.01873491468997087,
      "loss": 0.345,
      "step": 304
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.019229650497436523,
      "learning_rate": 0.01873075322513525,
      "loss": 0.3953,
      "step": 305
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.017787378281354904,
      "learning_rate": 0.018726591760299626,
      "loss": 0.624,
      "step": 306
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.01644415408372879,
      "learning_rate": 0.018722430295464002,
      "loss": 0.2329,
      "step": 307
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.012315182946622372,
      "learning_rate": 0.018718268830628382,
      "loss": 0.1984,
      "step": 308
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.012109145522117615,
      "learning_rate": 0.018714107365792758,
      "loss": 0.2041,
      "step": 309
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.013556061312556267,
      "learning_rate": 0.018709945900957137,
      "loss": 0.4844,
      "step": 310
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.016224991530179977,
      "learning_rate": 0.018705784436121517,
      "loss": 0.5752,
      "step": 311
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.021627897396683693,
      "learning_rate": 0.018701622971285893,
      "loss": 0.2301,
      "step": 312
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.022303931415081024,
      "learning_rate": 0.01869746150645027,
      "loss": 0.4536,
      "step": 313
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.014758111909031868,
      "learning_rate": 0.01869330004161465,
      "loss": 0.3098,
      "step": 314
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.013408167287707329,
      "learning_rate": 0.018689138576779025,
      "loss": 0.0468,
      "step": 315
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.017526132985949516,
      "learning_rate": 0.018684977111943404,
      "loss": 0.1798,
      "step": 316
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.025102650746703148,
      "learning_rate": 0.018680815647107784,
      "loss": 0.5762,
      "step": 317
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.014943995513021946,
      "learning_rate": 0.01867665418227216,
      "loss": 0.4561,
      "step": 318
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.016996843740344048,
      "learning_rate": 0.018672492717436536,
      "loss": 0.2896,
      "step": 319
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.011097901500761509,
      "learning_rate": 0.018668331252600916,
      "loss": 0.1134,
      "step": 320
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.013591892085969448,
      "learning_rate": 0.018664169787765292,
      "loss": 0.1129,
      "step": 321
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.015822574496269226,
      "learning_rate": 0.01866000832292967,
      "loss": 0.2207,
      "step": 322
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.015380003489553928,
      "learning_rate": 0.01865584685809405,
      "loss": 0.16,
      "step": 323
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.008600973524153233,
      "learning_rate": 0.018651685393258427,
      "loss": 0.0347,
      "step": 324
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.014758262783288956,
      "learning_rate": 0.018647523928422807,
      "loss": 0.2202,
      "step": 325
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.016456298530101776,
      "learning_rate": 0.018643362463587183,
      "loss": 0.2969,
      "step": 326
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.014062338508665562,
      "learning_rate": 0.01863920099875156,
      "loss": 0.2715,
      "step": 327
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.014881882816553116,
      "learning_rate": 0.01863503953391594,
      "loss": 0.022,
      "step": 328
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.0236114040017128,
      "learning_rate": 0.018630878069080318,
      "loss": 0.2834,
      "step": 329
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.023272477090358734,
      "learning_rate": 0.018626716604244694,
      "loss": 0.5132,
      "step": 330
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.012915327213704586,
      "learning_rate": 0.018622555139409074,
      "loss": 0.1646,
      "step": 331
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.027405433356761932,
      "learning_rate": 0.01861839367457345,
      "loss": 0.4375,
      "step": 332
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.019116155803203583,
      "learning_rate": 0.01861423220973783,
      "loss": 0.2905,
      "step": 333
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.015979696065187454,
      "learning_rate": 0.018610070744902205,
      "loss": 0.2188,
      "step": 334
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.016301333904266357,
      "learning_rate": 0.018605909280066585,
      "loss": 0.2358,
      "step": 335
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.011888011358678341,
      "learning_rate": 0.018601747815230964,
      "loss": 0.246,
      "step": 336
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.023288020864129066,
      "learning_rate": 0.01859758635039534,
      "loss": 0.541,
      "step": 337
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.005208710674196482,
      "learning_rate": 0.018593424885559717,
      "loss": 0.0097,
      "step": 338
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.011656765826046467,
      "learning_rate": 0.018589263420724096,
      "loss": 0.0943,
      "step": 339
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.026298977434635162,
      "learning_rate": 0.018585101955888472,
      "loss": 1.1572,
      "step": 340
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.015261673368513584,
      "learning_rate": 0.018580940491052852,
      "loss": 0.0914,
      "step": 341
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.018320735543966293,
      "learning_rate": 0.01857677902621723,
      "loss": 0.4023,
      "step": 342
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.020216815173625946,
      "learning_rate": 0.018572617561381607,
      "loss": 0.26,
      "step": 343
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.016558609902858734,
      "learning_rate": 0.018568456096545984,
      "loss": 0.301,
      "step": 344
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.013275112956762314,
      "learning_rate": 0.018564294631710363,
      "loss": 0.1666,
      "step": 345
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.023631185293197632,
      "learning_rate": 0.01856013316687474,
      "loss": 0.365,
      "step": 346
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.023109402507543564,
      "learning_rate": 0.01855597170203912,
      "loss": 0.4705,
      "step": 347
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.024755585938692093,
      "learning_rate": 0.0185518102372035,
      "loss": 0.6177,
      "step": 348
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.038343414664268494,
      "learning_rate": 0.018547648772367874,
      "loss": 0.4514,
      "step": 349
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.026825403794646263,
      "learning_rate": 0.01854348730753225,
      "loss": 0.5698,
      "step": 350
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.01578792929649353,
      "learning_rate": 0.01853932584269663,
      "loss": 0.2073,
      "step": 351
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.010524454526603222,
      "learning_rate": 0.018535164377861006,
      "loss": 0.2559,
      "step": 352
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.010751763358712196,
      "learning_rate": 0.018531002913025386,
      "loss": 0.1526,
      "step": 353
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.015715204179286957,
      "learning_rate": 0.018526841448189765,
      "loss": 0.4263,
      "step": 354
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.01730133220553398,
      "learning_rate": 0.01852267998335414,
      "loss": 0.3289,
      "step": 355
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.016789477318525314,
      "learning_rate": 0.018518518518518517,
      "loss": 0.209,
      "step": 356
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.01814727671444416,
      "learning_rate": 0.018514357053682897,
      "loss": 0.2617,
      "step": 357
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.02244481071829796,
      "learning_rate": 0.018510195588847273,
      "loss": 0.5991,
      "step": 358
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.01240987703204155,
      "learning_rate": 0.018506034124011653,
      "loss": 0.187,
      "step": 359
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.01305888220667839,
      "learning_rate": 0.018501872659176032,
      "loss": 0.1776,
      "step": 360
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.015885083004832268,
      "learning_rate": 0.01849771119434041,
      "loss": 0.2406,
      "step": 361
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.018674463033676147,
      "learning_rate": 0.018493549729504784,
      "loss": 0.4717,
      "step": 362
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.011706807650625706,
      "learning_rate": 0.018489388264669164,
      "loss": 0.1733,
      "step": 363
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.012210343964397907,
      "learning_rate": 0.01848522679983354,
      "loss": 0.0668,
      "step": 364
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.023025935515761375,
      "learning_rate": 0.01848106533499792,
      "loss": 0.4929,
      "step": 365
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.019617287442088127,
      "learning_rate": 0.0184769038701623,
      "loss": 0.3542,
      "step": 366
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.019019676372408867,
      "learning_rate": 0.018472742405326675,
      "loss": 0.1937,
      "step": 367
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.019696107134222984,
      "learning_rate": 0.018468580940491055,
      "loss": 0.5415,
      "step": 368
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.01248240377753973,
      "learning_rate": 0.01846441947565543,
      "loss": 0.3943,
      "step": 369
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.01081449817866087,
      "learning_rate": 0.018460258010819807,
      "loss": 0.1473,
      "step": 370
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.02130720019340515,
      "learning_rate": 0.018456096545984187,
      "loss": 0.2279,
      "step": 371
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.023912722244858742,
      "learning_rate": 0.018451935081148566,
      "loss": 0.623,
      "step": 372
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.013592472299933434,
      "learning_rate": 0.018447773616312942,
      "loss": 0.1094,
      "step": 373
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.01946699246764183,
      "learning_rate": 0.018443612151477322,
      "loss": 0.3538,
      "step": 374
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.01677008904516697,
      "learning_rate": 0.018439450686641698,
      "loss": 0.5879,
      "step": 375
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.014828789979219437,
      "learning_rate": 0.018435289221806074,
      "loss": 0.4316,
      "step": 376
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.020289145410060883,
      "learning_rate": 0.018431127756970454,
      "loss": 0.4058,
      "step": 377
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.02077432908117771,
      "learning_rate": 0.018426966292134833,
      "loss": 0.1469,
      "step": 378
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.010249602608382702,
      "learning_rate": 0.01842280482729921,
      "loss": 0.015,
      "step": 379
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.016949528828263283,
      "learning_rate": 0.01841864336246359,
      "loss": 0.2949,
      "step": 380
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.01481599546968937,
      "learning_rate": 0.018414481897627965,
      "loss": 0.1646,
      "step": 381
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.030077632516622543,
      "learning_rate": 0.01841032043279234,
      "loss": 0.4893,
      "step": 382
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.013509195297956467,
      "learning_rate": 0.01840615896795672,
      "loss": 0.1575,
      "step": 383
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.015224408358335495,
      "learning_rate": 0.0184019975031211,
      "loss": 0.4365,
      "step": 384
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.019919302314519882,
      "learning_rate": 0.018397836038285476,
      "loss": 0.2722,
      "step": 385
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.011082062497735023,
      "learning_rate": 0.018393674573449856,
      "loss": 0.259,
      "step": 386
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.028045201674103737,
      "learning_rate": 0.018389513108614232,
      "loss": 0.2676,
      "step": 387
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.027130700647830963,
      "learning_rate": 0.018385351643778608,
      "loss": 0.3445,
      "step": 388
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.013158266432583332,
      "learning_rate": 0.018381190178942988,
      "loss": 0.1779,
      "step": 389
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.021059006452560425,
      "learning_rate": 0.018377028714107367,
      "loss": 0.0562,
      "step": 390
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.009331466630101204,
      "learning_rate": 0.018372867249271743,
      "loss": 0.1804,
      "step": 391
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.012068395502865314,
      "learning_rate": 0.018368705784436123,
      "loss": 0.2776,
      "step": 392
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.026677824556827545,
      "learning_rate": 0.0183645443196005,
      "loss": 0.3213,
      "step": 393
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.029340950772166252,
      "learning_rate": 0.018360382854764875,
      "loss": 0.3765,
      "step": 394
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.015520433895289898,
      "learning_rate": 0.018356221389929255,
      "loss": 0.2681,
      "step": 395
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.02110999822616577,
      "learning_rate": 0.018352059925093634,
      "loss": 0.2029,
      "step": 396
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.01838153973221779,
      "learning_rate": 0.01834789846025801,
      "loss": 0.0622,
      "step": 397
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.01988046057522297,
      "learning_rate": 0.01834373699542239,
      "loss": 0.3376,
      "step": 398
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.020841067656874657,
      "learning_rate": 0.018339575530586766,
      "loss": 0.4333,
      "step": 399
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.009150462225079536,
      "learning_rate": 0.018335414065751145,
      "loss": 0.0217,
      "step": 400
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.028743427246809006,
      "learning_rate": 0.01833125260091552,
      "loss": 0.0804,
      "step": 401
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.013679815456271172,
      "learning_rate": 0.0183270911360799,
      "loss": 0.1171,
      "step": 402
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.019026050344109535,
      "learning_rate": 0.01832292967124428,
      "loss": 0.2389,
      "step": 403
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.025706058368086815,
      "learning_rate": 0.018318768206408657,
      "loss": 0.6992,
      "step": 404
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.0252694021910429,
      "learning_rate": 0.018314606741573033,
      "loss": 0.4209,
      "step": 405
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.024600857868790627,
      "learning_rate": 0.018310445276737412,
      "loss": 0.6948,
      "step": 406
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.022102979943156242,
      "learning_rate": 0.01830628381190179,
      "loss": 0.3425,
      "step": 407
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.0011589693604037166,
      "learning_rate": 0.018302122347066168,
      "loss": 0.0023,
      "step": 408
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.01646098494529724,
      "learning_rate": 0.018297960882230548,
      "loss": 0.2659,
      "step": 409
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.01534626167267561,
      "learning_rate": 0.018293799417394924,
      "loss": 0.3,
      "step": 410
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.022580618038773537,
      "learning_rate": 0.018289637952559303,
      "loss": 0.4175,
      "step": 411
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.017710313200950623,
      "learning_rate": 0.01828547648772368,
      "loss": 0.4795,
      "step": 412
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.015363357961177826,
      "learning_rate": 0.018281315022888055,
      "loss": 0.1886,
      "step": 413
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.021407626569271088,
      "learning_rate": 0.018277153558052435,
      "loss": 0.1099,
      "step": 414
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.01610243320465088,
      "learning_rate": 0.018272992093216815,
      "loss": 0.4968,
      "step": 415
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.03274524584412575,
      "learning_rate": 0.01826883062838119,
      "loss": 0.7466,
      "step": 416
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.028339523822069168,
      "learning_rate": 0.01826466916354557,
      "loss": 0.7749,
      "step": 417
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.013688563369214535,
      "learning_rate": 0.018260507698709946,
      "loss": 0.0635,
      "step": 418
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.021380651742219925,
      "learning_rate": 0.018256346233874322,
      "loss": 0.4131,
      "step": 419
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.026248306035995483,
      "learning_rate": 0.018252184769038702,
      "loss": 0.2974,
      "step": 420
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.02018003910779953,
      "learning_rate": 0.01824802330420308,
      "loss": 0.2115,
      "step": 421
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.024361971765756607,
      "learning_rate": 0.018243861839367458,
      "loss": 0.6206,
      "step": 422
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.015359626151621342,
      "learning_rate": 0.018239700374531837,
      "loss": 0.1203,
      "step": 423
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.016143817454576492,
      "learning_rate": 0.018235538909696213,
      "loss": 0.2698,
      "step": 424
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.015773026272654533,
      "learning_rate": 0.01823137744486059,
      "loss": 0.1071,
      "step": 425
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.011392384767532349,
      "learning_rate": 0.01822721598002497,
      "loss": 0.1879,
      "step": 426
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.012423066422343254,
      "learning_rate": 0.01822305451518935,
      "loss": 0.0672,
      "step": 427
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.029530595988035202,
      "learning_rate": 0.018218893050353725,
      "loss": 0.3613,
      "step": 428
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.024436235427856445,
      "learning_rate": 0.018214731585518104,
      "loss": 0.3896,
      "step": 429
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.016348615288734436,
      "learning_rate": 0.01821057012068248,
      "loss": 0.3174,
      "step": 430
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.015601023100316525,
      "learning_rate": 0.018206408655846856,
      "loss": 0.2079,
      "step": 431
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.014723938889801502,
      "learning_rate": 0.018202247191011236,
      "loss": 0.2329,
      "step": 432
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.024064524099230766,
      "learning_rate": 0.018198085726175615,
      "loss": 0.4507,
      "step": 433
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.025292137637734413,
      "learning_rate": 0.01819392426133999,
      "loss": 0.6035,
      "step": 434
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.022207777947187424,
      "learning_rate": 0.01818976279650437,
      "loss": 0.7056,
      "step": 435
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.022870400920510292,
      "learning_rate": 0.018185601331668747,
      "loss": 0.2622,
      "step": 436
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.020909421145915985,
      "learning_rate": 0.018181439866833123,
      "loss": 0.2377,
      "step": 437
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.021142246201634407,
      "learning_rate": 0.018177278401997503,
      "loss": 0.2996,
      "step": 438
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.01641058176755905,
      "learning_rate": 0.018173116937161882,
      "loss": 0.3174,
      "step": 439
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.021069413051009178,
      "learning_rate": 0.01816895547232626,
      "loss": 0.2654,
      "step": 440
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.025653362274169922,
      "learning_rate": 0.018164794007490638,
      "loss": 0.4221,
      "step": 441
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.01986721344292164,
      "learning_rate": 0.018160632542655014,
      "loss": 0.3657,
      "step": 442
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.021333815529942513,
      "learning_rate": 0.018156471077819394,
      "loss": 0.467,
      "step": 443
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.018084486946463585,
      "learning_rate": 0.01815230961298377,
      "loss": 0.2644,
      "step": 444
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.028349634259939194,
      "learning_rate": 0.01814814814814815,
      "loss": 0.5391,
      "step": 445
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.022226015105843544,
      "learning_rate": 0.018143986683312525,
      "loss": 0.5645,
      "step": 446
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.015429804101586342,
      "learning_rate": 0.018139825218476905,
      "loss": 0.0782,
      "step": 447
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.024509834125638008,
      "learning_rate": 0.01813566375364128,
      "loss": 0.4612,
      "step": 448
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.011498290114104748,
      "learning_rate": 0.01813150228880566,
      "loss": 0.1084,
      "step": 449
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.016364967450499535,
      "learning_rate": 0.018127340823970037,
      "loss": 0.3008,
      "step": 450
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.0184890478849411,
      "learning_rate": 0.018123179359134416,
      "loss": 0.2186,
      "step": 451
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.01742672361433506,
      "learning_rate": 0.018119017894298792,
      "loss": 0.243,
      "step": 452
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.01943567395210266,
      "learning_rate": 0.018114856429463172,
      "loss": 0.2913,
      "step": 453
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.024606555700302124,
      "learning_rate": 0.01811069496462755,
      "loss": 0.0767,
      "step": 454
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.008739195764064789,
      "learning_rate": 0.018106533499791928,
      "loss": 0.0287,
      "step": 455
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.01797635853290558,
      "learning_rate": 0.018102372034956304,
      "loss": 0.1814,
      "step": 456
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.015862181782722473,
      "learning_rate": 0.018098210570120683,
      "loss": 0.4111,
      "step": 457
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.022920627146959305,
      "learning_rate": 0.01809404910528506,
      "loss": 0.375,
      "step": 458
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.01702135242521763,
      "learning_rate": 0.01808988764044944,
      "loss": 0.3601,
      "step": 459
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.016336023807525635,
      "learning_rate": 0.01808572617561382,
      "loss": 0.1782,
      "step": 460
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.015450933948159218,
      "learning_rate": 0.018081564710778195,
      "loss": 0.2888,
      "step": 461
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.02359917387366295,
      "learning_rate": 0.01807740324594257,
      "loss": 0.313,
      "step": 462
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.011471382342278957,
      "learning_rate": 0.01807324178110695,
      "loss": 0.1418,
      "step": 463
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.019025051966309547,
      "learning_rate": 0.018069080316271326,
      "loss": 0.3232,
      "step": 464
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.0016236061928793788,
      "learning_rate": 0.018064918851435706,
      "loss": 0.0021,
      "step": 465
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.018086018040776253,
      "learning_rate": 0.018060757386600085,
      "loss": 0.3213,
      "step": 466
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.014555457048118114,
      "learning_rate": 0.01805659592176446,
      "loss": 0.144,
      "step": 467
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.01344548910856247,
      "learning_rate": 0.018052434456928838,
      "loss": 0.1176,
      "step": 468
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.014173297211527824,
      "learning_rate": 0.018048272992093217,
      "loss": 0.0638,
      "step": 469
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.021349281072616577,
      "learning_rate": 0.018044111527257593,
      "loss": 0.1447,
      "step": 470
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.02288207970559597,
      "learning_rate": 0.018039950062421973,
      "loss": 0.3296,
      "step": 471
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.0221555233001709,
      "learning_rate": 0.018035788597586352,
      "loss": 0.2883,
      "step": 472
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.020956367254257202,
      "learning_rate": 0.01803162713275073,
      "loss": 0.2776,
      "step": 473
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.03914099186658859,
      "learning_rate": 0.018027465667915105,
      "loss": 0.2311,
      "step": 474
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.016052138060331345,
      "learning_rate": 0.018023304203079484,
      "loss": 0.2822,
      "step": 475
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.02345450036227703,
      "learning_rate": 0.018019142738243864,
      "loss": 0.4175,
      "step": 476
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.015222625806927681,
      "learning_rate": 0.01801498127340824,
      "loss": 0.2379,
      "step": 477
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.01661856472492218,
      "learning_rate": 0.01801081980857262,
      "loss": 0.142,
      "step": 478
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.027985790744423866,
      "learning_rate": 0.018006658343736996,
      "loss": 0.2676,
      "step": 479
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.005155233666300774,
      "learning_rate": 0.01800249687890137,
      "loss": 0.0064,
      "step": 480
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.01854134164750576,
      "learning_rate": 0.01799833541406575,
      "loss": 0.3274,
      "step": 481
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.021399671211838722,
      "learning_rate": 0.01799417394923013,
      "loss": 0.3025,
      "step": 482
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.013370412401854992,
      "learning_rate": 0.017990012484394507,
      "loss": 0.22,
      "step": 483
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.02801896072924137,
      "learning_rate": 0.017985851019558886,
      "loss": 0.5527,
      "step": 484
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.016469566151499748,
      "learning_rate": 0.017981689554723262,
      "loss": 0.3855,
      "step": 485
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.018887661397457123,
      "learning_rate": 0.017977528089887642,
      "loss": 0.1444,
      "step": 486
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.016790995374321938,
      "learning_rate": 0.017973366625052018,
      "loss": 0.1682,
      "step": 487
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.01708918623626232,
      "learning_rate": 0.017969205160216398,
      "loss": 0.2532,
      "step": 488
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.013119114562869072,
      "learning_rate": 0.017965043695380774,
      "loss": 0.2593,
      "step": 489
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.01730186864733696,
      "learning_rate": 0.017960882230545153,
      "loss": 0.4368,
      "step": 490
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.0007973984465934336,
      "learning_rate": 0.01795672076570953,
      "loss": 0.0015,
      "step": 491
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.016564833000302315,
      "learning_rate": 0.01795255930087391,
      "loss": 0.1823,
      "step": 492
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.017495324835181236,
      "learning_rate": 0.017948397836038285,
      "loss": 0.2197,
      "step": 493
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.013706967234611511,
      "learning_rate": 0.017944236371202665,
      "loss": 0.0506,
      "step": 494
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.02236577868461609,
      "learning_rate": 0.01794007490636704,
      "loss": 0.0427,
      "step": 495
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.016671421006321907,
      "learning_rate": 0.01793591344153142,
      "loss": 0.4666,
      "step": 496
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.021448083221912384,
      "learning_rate": 0.0179317519766958,
      "loss": 0.2856,
      "step": 497
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.022695811465382576,
      "learning_rate": 0.017927590511860176,
      "loss": 0.4028,
      "step": 498
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.013902803882956505,
      "learning_rate": 0.017923429047024552,
      "loss": 0.386,
      "step": 499
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.013315980322659016,
      "learning_rate": 0.01791926758218893,
      "loss": 0.0199,
      "step": 500
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.01322450116276741,
      "learning_rate": 0.017915106117353308,
      "loss": 0.0723,
      "step": 501
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.026917418465018272,
      "learning_rate": 0.017910944652517687,
      "loss": 0.4121,
      "step": 502
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.017656033858656883,
      "learning_rate": 0.017906783187682067,
      "loss": 0.166,
      "step": 503
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.016564955934882164,
      "learning_rate": 0.017902621722846443,
      "loss": 0.4065,
      "step": 504
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.008763215504586697,
      "learning_rate": 0.01789846025801082,
      "loss": 0.0299,
      "step": 505
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.0215713270008564,
      "learning_rate": 0.0178942987931752,
      "loss": 0.4661,
      "step": 506
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.011343676596879959,
      "learning_rate": 0.017890137328339575,
      "loss": 0.1272,
      "step": 507
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.021843140944838524,
      "learning_rate": 0.017885975863503954,
      "loss": 0.3967,
      "step": 508
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.005304041784256697,
      "learning_rate": 0.017881814398668334,
      "loss": 0.0106,
      "step": 509
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.014229088090360165,
      "learning_rate": 0.01787765293383271,
      "loss": 0.1688,
      "step": 510
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.02197921648621559,
      "learning_rate": 0.017873491468997086,
      "loss": 0.3103,
      "step": 511
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.0247966218739748,
      "learning_rate": 0.017869330004161466,
      "loss": 0.2664,
      "step": 512
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.017200419679284096,
      "learning_rate": 0.01786516853932584,
      "loss": 0.1678,
      "step": 513
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.01293894462287426,
      "learning_rate": 0.01786100707449022,
      "loss": 0.136,
      "step": 514
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.015838829800486565,
      "learning_rate": 0.0178568456096546,
      "loss": 0.2961,
      "step": 515
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.014234489761292934,
      "learning_rate": 0.017852684144818977,
      "loss": 0.4495,
      "step": 516
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.022854570299386978,
      "learning_rate": 0.017848522679983353,
      "loss": 0.1954,
      "step": 517
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.03496082127094269,
      "learning_rate": 0.017844361215147733,
      "loss": 0.8726,
      "step": 518
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.007512242998927832,
      "learning_rate": 0.01784019975031211,
      "loss": 0.0353,
      "step": 519
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.012978347018361092,
      "learning_rate": 0.017836038285476488,
      "loss": 0.47,
      "step": 520
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.0132742365822196,
      "learning_rate": 0.017831876820640868,
      "loss": 0.0959,
      "step": 521
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.023542677983641624,
      "learning_rate": 0.017827715355805244,
      "loss": 0.217,
      "step": 522
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.01400215644389391,
      "learning_rate": 0.01782355389096962,
      "loss": 0.0919,
      "step": 523
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.010237505659461021,
      "learning_rate": 0.017819392426134,
      "loss": 0.0629,
      "step": 524
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.015069641172885895,
      "learning_rate": 0.017815230961298376,
      "loss": 0.1357,
      "step": 525
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.011609483510255814,
      "learning_rate": 0.017811069496462755,
      "loss": 0.0459,
      "step": 526
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.01928263157606125,
      "learning_rate": 0.017806908031627135,
      "loss": 0.3618,
      "step": 527
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.015118389390408993,
      "learning_rate": 0.01780274656679151,
      "loss": 0.1812,
      "step": 528
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.030556663870811462,
      "learning_rate": 0.01779858510195589,
      "loss": 0.4568,
      "step": 529
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.02257608063519001,
      "learning_rate": 0.017794423637120266,
      "loss": 0.2603,
      "step": 530
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.02450978383421898,
      "learning_rate": 0.017790262172284643,
      "loss": 0.3921,
      "step": 531
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.018064459785819054,
      "learning_rate": 0.017786100707449022,
      "loss": 0.1653,
      "step": 532
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.014579844661056995,
      "learning_rate": 0.0177819392426134,
      "loss": 0.5142,
      "step": 533
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.018851060420274734,
      "learning_rate": 0.017777777777777778,
      "loss": 0.2979,
      "step": 534
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.01819692738354206,
      "learning_rate": 0.017773616312942157,
      "loss": 0.4919,
      "step": 535
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.012211748398840427,
      "learning_rate": 0.017769454848106533,
      "loss": 0.3062,
      "step": 536
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.019866351038217545,
      "learning_rate": 0.01776529338327091,
      "loss": 0.5698,
      "step": 537
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.01991969533264637,
      "learning_rate": 0.01776113191843529,
      "loss": 0.4614,
      "step": 538
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.01758577860891819,
      "learning_rate": 0.01775697045359967,
      "loss": 0.1968,
      "step": 539
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.011264219880104065,
      "learning_rate": 0.017752808988764045,
      "loss": 0.0826,
      "step": 540
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.018839716911315918,
      "learning_rate": 0.017748647523928424,
      "loss": 0.4075,
      "step": 541
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.012899573892354965,
      "learning_rate": 0.0177444860590928,
      "loss": 0.2368,
      "step": 542
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.02240430936217308,
      "learning_rate": 0.017740324594257176,
      "loss": 0.2886,
      "step": 543
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.01477088499814272,
      "learning_rate": 0.017736163129421556,
      "loss": 0.0632,
      "step": 544
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.0271300058811903,
      "learning_rate": 0.017732001664585936,
      "loss": 0.98,
      "step": 545
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.03747643902897835,
      "learning_rate": 0.01772784019975031,
      "loss": 0.2114,
      "step": 546
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.013209293596446514,
      "learning_rate": 0.01772367873491469,
      "loss": 0.1293,
      "step": 547
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.023283572867512703,
      "learning_rate": 0.017719517270079067,
      "loss": 0.1272,
      "step": 548
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.02200518362224102,
      "learning_rate": 0.017715355805243447,
      "loss": 0.6167,
      "step": 549
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.018459653481841087,
      "learning_rate": 0.017711194340407823,
      "loss": 0.1938,
      "step": 550
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.01839401386678219,
      "learning_rate": 0.017707032875572203,
      "loss": 0.4368,
      "step": 551
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.031117385253310204,
      "learning_rate": 0.017702871410736582,
      "loss": 0.5078,
      "step": 552
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.016220442950725555,
      "learning_rate": 0.017698709945900958,
      "loss": 0.2343,
      "step": 553
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.028234465047717094,
      "learning_rate": 0.017694548481065334,
      "loss": 0.4602,
      "step": 554
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.021623658016324043,
      "learning_rate": 0.017690387016229714,
      "loss": 0.241,
      "step": 555
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.018278444185853004,
      "learning_rate": 0.01768622555139409,
      "loss": 0.2126,
      "step": 556
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.018106509000062943,
      "learning_rate": 0.01768206408655847,
      "loss": 0.4719,
      "step": 557
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.012071166187524796,
      "learning_rate": 0.01767790262172285,
      "loss": 0.1615,
      "step": 558
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.026161981746554375,
      "learning_rate": 0.017673741156887225,
      "loss": 0.6118,
      "step": 559
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.01915688067674637,
      "learning_rate": 0.0176695796920516,
      "loss": 0.0596,
      "step": 560
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.01590966060757637,
      "learning_rate": 0.01766541822721598,
      "loss": 0.1901,
      "step": 561
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.009362030774354935,
      "learning_rate": 0.017661256762380357,
      "loss": 0.0426,
      "step": 562
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.015719180926680565,
      "learning_rate": 0.017657095297544737,
      "loss": 0.2035,
      "step": 563
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.014808082953095436,
      "learning_rate": 0.017652933832709116,
      "loss": 0.1589,
      "step": 564
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.022726433351635933,
      "learning_rate": 0.017648772367873492,
      "loss": 0.2559,
      "step": 565
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.02343447506427765,
      "learning_rate": 0.017644610903037868,
      "loss": 0.4946,
      "step": 566
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.016504652798175812,
      "learning_rate": 0.017640449438202248,
      "loss": 0.1726,
      "step": 567
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.016712628304958344,
      "learning_rate": 0.017636287973366624,
      "loss": 0.2517,
      "step": 568
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.023331880569458008,
      "learning_rate": 0.017632126508531003,
      "loss": 0.3794,
      "step": 569
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.005479931831359863,
      "learning_rate": 0.017627965043695383,
      "loss": 0.0262,
      "step": 570
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.015947291627526283,
      "learning_rate": 0.01762380357885976,
      "loss": 0.2195,
      "step": 571
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.0175675917416811,
      "learning_rate": 0.01761964211402414,
      "loss": 0.1621,
      "step": 572
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.02074829861521721,
      "learning_rate": 0.017615480649188515,
      "loss": 0.27,
      "step": 573
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.0031139347702264786,
      "learning_rate": 0.01761131918435289,
      "loss": 0.005,
      "step": 574
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.013330874964594841,
      "learning_rate": 0.01760715771951727,
      "loss": 0.2313,
      "step": 575
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.018731823191046715,
      "learning_rate": 0.01760299625468165,
      "loss": 0.3057,
      "step": 576
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.03177861496806145,
      "learning_rate": 0.017598834789846026,
      "loss": 0.3982,
      "step": 577
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.022089118137955666,
      "learning_rate": 0.017594673325010406,
      "loss": 0.3254,
      "step": 578
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.010733268223702908,
      "learning_rate": 0.017590511860174782,
      "loss": 0.0645,
      "step": 579
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.022312788292765617,
      "learning_rate": 0.017586350395339158,
      "loss": 0.4197,
      "step": 580
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.020953146740794182,
      "learning_rate": 0.017582188930503537,
      "loss": 0.4355,
      "step": 581
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.014730071648955345,
      "learning_rate": 0.017578027465667917,
      "loss": 0.3032,
      "step": 582
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.008113733492791653,
      "learning_rate": 0.017573866000832293,
      "loss": 0.0445,
      "step": 583
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.015508532524108887,
      "learning_rate": 0.017569704535996673,
      "loss": 0.1904,
      "step": 584
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.028984874486923218,
      "learning_rate": 0.01756554307116105,
      "loss": 0.522,
      "step": 585
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.017001159489154816,
      "learning_rate": 0.017561381606325425,
      "loss": 0.5171,
      "step": 586
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.006894498132169247,
      "learning_rate": 0.017557220141489804,
      "loss": 0.0688,
      "step": 587
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.020242296159267426,
      "learning_rate": 0.017553058676654184,
      "loss": 0.3547,
      "step": 588
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.02047143317759037,
      "learning_rate": 0.01754889721181856,
      "loss": 0.1675,
      "step": 589
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.026067951694130898,
      "learning_rate": 0.01754473574698294,
      "loss": 0.1515,
      "step": 590
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.030283140018582344,
      "learning_rate": 0.017540574282147316,
      "loss": 0.5273,
      "step": 591
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.018824715167284012,
      "learning_rate": 0.017536412817311692,
      "loss": 0.4636,
      "step": 592
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.013927336782217026,
      "learning_rate": 0.01753225135247607,
      "loss": 0.332,
      "step": 593
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.014410550706088543,
      "learning_rate": 0.01752808988764045,
      "loss": 0.1453,
      "step": 594
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.0069348462857306,
      "learning_rate": 0.017523928422804827,
      "loss": 0.0132,
      "step": 595
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.014951486140489578,
      "learning_rate": 0.017519766957969207,
      "loss": 0.2013,
      "step": 596
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.023131584748625755,
      "learning_rate": 0.017515605493133583,
      "loss": 0.5049,
      "step": 597
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.016293618828058243,
      "learning_rate": 0.01751144402829796,
      "loss": 0.303,
      "step": 598
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.011231700889766216,
      "learning_rate": 0.01750728256346234,
      "loss": 0.1244,
      "step": 599
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.02238783799111843,
      "learning_rate": 0.017503121098626718,
      "loss": 0.4756,
      "step": 600
    },
    {
      "epoch": 0.75,
      "eval_loss": 0.2890625,
      "eval_runtime": 183.0178,
      "eval_samples_per_second": 1.098,
      "eval_steps_per_second": 0.552,
      "step": 600
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 4806,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 300,
  "total_flos": 6.90779446050816e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
